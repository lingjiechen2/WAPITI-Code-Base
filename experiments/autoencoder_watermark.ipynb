{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/remote-home/miintern1/anaconda3/envs/watermark/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import copy\n",
    "os.environ['http_proxy'] = \"http://10.176.52.116:7890\"\n",
    "os.environ['https_proxy'] = \"http://10.176.52.116:7890\"\n",
    "os.environ['all_proxy'] = \"socks5://10.176.52.116:7891\"\n",
    "import sys\n",
    "sys.path.append(('../'))\n",
    "sys.path.append(('../../'))\n",
    "# sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "import json\n",
    "from typing import Dict\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n",
    "from task_vector import TaskVector\n",
    "\n",
    "from watermarks.kgw.watermark_processor import WatermarkDetector\n",
    "from watermarks.aar.aar_watermark import AarWatermarkDetector\n",
    "from watermarks.watermark_types import WatermarkType\n",
    "import logging\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/remote-home/miintern1/anaconda3/envs/watermark/lib/python3.11/site-packages/torch/cuda/__init__.py:118: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m watermark_residuals \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/remote-home/miintern1/watermark-learnability/data/c4/watermark_residuals.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m vanilla_residuals \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/remote-home/miintern1/watermark-learnability/data/c4/vanilla_residuals.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/watermark/lib/python3.11/site-packages/torch/serialization.py:1025\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1023\u001b[0m             \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1024\u001b[0m                 \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1025\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1026\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1027\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1028\u001b[0m \u001b[43m                     \u001b[49m\u001b[43moverall_storage\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moverall_storage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1029\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1030\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mmap:\n\u001b[1;32m   1031\u001b[0m     f_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(f, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/watermark/lib/python3.11/site-packages/torch/serialization.py:1446\u001b[0m, in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1444\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(data_file, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1445\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1446\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1448\u001b[0m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_validate_loaded_sparse_tensors()\n\u001b[1;32m   1449\u001b[0m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_log_api_usage_metadata(\n\u001b[1;32m   1450\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch.load.metadata\u001b[39m\u001b[38;5;124m\"\u001b[39m, {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mserialization_id\u001b[39m\u001b[38;5;124m\"\u001b[39m: zip_file\u001b[38;5;241m.\u001b[39mserialization_id()}\n\u001b[1;32m   1451\u001b[0m )\n",
      "File \u001b[0;32m~/anaconda3/envs/watermark/lib/python3.11/site-packages/torch/serialization.py:1416\u001b[0m, in \u001b[0;36m_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m   1414\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1415\u001b[0m     nbytes \u001b[38;5;241m=\u001b[39m numel \u001b[38;5;241m*\u001b[39m torch\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_element_size(dtype)\n\u001b[0;32m-> 1416\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m \u001b[43mload_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_maybe_decode_ascii\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1418\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m typed_storage\n",
      "File \u001b[0;32m~/anaconda3/envs/watermark/lib/python3.11/site-packages/torch/serialization.py:1390\u001b[0m, in \u001b[0;36m_load.<locals>.load_tensor\u001b[0;34m(dtype, numel, key, location)\u001b[0m\n\u001b[1;32m   1385\u001b[0m         storage\u001b[38;5;241m.\u001b[39mbyteswap(dtype)\n\u001b[1;32m   1387\u001b[0m \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m   1388\u001b[0m \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m   1389\u001b[0m typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[0;32m-> 1390\u001b[0m     wrap_storage\u001b[38;5;241m=\u001b[39m\u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   1391\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m   1392\u001b[0m     _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m   1394\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typed_storage\u001b[38;5;241m.\u001b[39m_data_ptr() \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   1395\u001b[0m     loaded_storages[key] \u001b[38;5;241m=\u001b[39m typed_storage\n",
      "File \u001b[0;32m~/anaconda3/envs/watermark/lib/python3.11/site-packages/torch/serialization.py:390\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 390\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    391\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    392\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/anaconda3/envs/watermark/lib/python3.11/site-packages/torch/serialization.py:265\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_cuda_deserialize\u001b[39m(obj, location):\n\u001b[1;32m    264\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m location\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m--> 265\u001b[0m         device \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_cuda_device\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_torch_load_uninitialized\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    267\u001b[0m             \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice(device):\n",
      "File \u001b[0;32m~/anaconda3/envs/watermark/lib/python3.11/site-packages/torch/serialization.py:249\u001b[0m, in \u001b[0;36mvalidate_cuda_device\u001b[0;34m(location)\u001b[0m\n\u001b[1;32m    246\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39m_utils\u001b[38;5;241m.\u001b[39m_get_device_index(location, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    248\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[0;32m--> 249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAttempting to deserialize object on a CUDA \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    250\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice but torch.cuda.is_available() is False. \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    251\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIf you are running on a CPU-only machine, \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    252\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mplease use torch.load with map_location=torch.device(\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;130;01m\\'\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    253\u001b[0m                        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mto map your storages to the CPU.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    254\u001b[0m device_count \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice_count()\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m device_count:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Attempting to deserialize object on a CUDA device but torch.cuda.is_available() is False. If you are running on a CPU-only machine, please use torch.load with map_location=torch.device('cpu') to map your storages to the CPU."
     ]
    }
   ],
   "source": [
    "watermark_residuals = torch.load(\"/remote-home/miintern1/watermark-learnability/data/c4/watermark_residuals.pt\", map_location='cpu')\n",
    "vanilla_residuals = torch.load(\"/remote-home/miintern1/watermark-learnability/data/c4/vanilla_residuals.pt\", map_location='cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 250\n",
    "min_length = 250\n",
    "num_samples = 512\n",
    "batch_size = 16\n",
    "device = 'cpu'\n",
    "dataset = load_dataset(\"allenai/c4\", \"realnewslike\", split=\"validation\", streaming=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'timestamp', 'url'],\n",
       "    num_rows: 13863\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|██████████| 13863/13863 [00:20<00:00, 663.39 examples/s]\n",
      "Map: 100%|██████████| 10498/10498 [00:06<00:00, 1686.28 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def filter_length(example):\n",
    "        return len(tokenizer(example['text'], truncation=True, max_length=max_length)[\"input_ids\"]) >= min_length\n",
    "\n",
    "def encode(examples):\n",
    "    trunc_tokens = tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    # Examples are truncated to max_length, which comprises the possible generation prompt and the text to be generated\n",
    "    examples[\"text\"] = tokenizer.batch_decode(trunc_tokens[\"input_ids\"], skip_special_tokens=True)\n",
    "    prompt = tokenizer(\n",
    "        examples[\"text\"], truncation=True, padding=True, max_length=50, return_tensors=\"pt\",\n",
    "    ).to(device)\n",
    "    examples[\"prompt_text\"] = tokenizer.batch_decode(prompt[\"input_ids\"], skip_special_tokens=True)\n",
    "    examples[\"input_ids\"] = prompt[\"input_ids\"]\n",
    "    examples[\"attention_mask\"] = prompt[\"attention_mask\"]\n",
    "    examples[\"text_completion\"] = tokenizer.batch_decode(\n",
    "        trunc_tokens[\"input_ids\"][:, 50:], skip_special_tokens=True\n",
    "    )\n",
    "    return examples\n",
    "\n",
    "dataset = dataset.filter(filter_length)\n",
    "# Set how many samples will be skipped\n",
    "dataset = dataset.map(encode, batched=True)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = []\n",
    "human_text = []\n",
    "prompt_text = []\n",
    "full_human_text = []\n",
    "for batch in dataloader:\n",
    "    if len(human_text) >= num_samples:\n",
    "        break\n",
    "    if (type(batch[\"input_ids\"]) == list):\n",
    "        batch[\"input_ids\"] = torch.stack(batch[\"input_ids\"], dim=1).to(device)\n",
    "    if (type(batch[\"attention_mask\"]) == list):\n",
    "        batch[\"attention_mask\"] = torch.stack(batch[\"attention_mask\"], dim=1).to(device)\n",
    "    prompts.append(batch)\n",
    "    human_text.extend(batch[\"text_completion\"])\n",
    "    prompt_text.extend(batch[\"prompt_text\"])\n",
    "    full_human_text.extend(batch[\"text\"])\n",
    "human_text = human_text[:num_samples]\n",
    "prompt_text = prompt_text[:num_samples]\n",
    "full_human_text = full_human_text[:num_samples]\n",
    "raw_input = {\n",
    "    \"prompts\": prompts,\n",
    "    \"human_text\": human_text,\n",
    "    \"prompt_text\": prompt_text,\n",
    "    \"full_human_text\": full_human_text\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10498/10498 [00:10<00:00, 981.26it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10498=\n"
     ]
    }
   ],
   "source": [
    "block_size = 512\n",
    "batch_size = 8\n",
    "# Initialize lists to hold the tokenized and grouped data\n",
    "all_input_ids = []\n",
    "all_attention_masks = []\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# Tokenize and concatenate texts\n",
    "for example in tqdm(dataset):\n",
    "    text = example[\"text\"]\n",
    "    # Tokenize the text\n",
    "    \n",
    "    tokenized_text = tokenizer(text, return_tensors='pt', truncation=True, padding='max_length', max_length=block_size)\n",
    "    \n",
    "    # Add the tokenized text to the lists\n",
    "    all_input_ids.append(tokenized_text['input_ids'].squeeze().tolist())\n",
    "    all_attention_masks.append(tokenized_text['attention_mask'].squeeze().tolist())\n",
    "print(f\"{len(all_input_ids)}=\")\n",
    "\n",
    "# Flatten the lists\n",
    "from itertools import chain\n",
    "all_input_ids = list(chain(*all_input_ids))\n",
    "all_attention_masks = list(chain(*all_attention_masks))\n",
    "\n",
    "# Ensure the total length is a multiple of block_size\n",
    "total_length = (len(all_input_ids) // block_size) * block_size\n",
    "all_input_ids = all_input_ids[:total_length]\n",
    "all_attention_masks = all_attention_masks[:total_length]\n",
    "\n",
    "# Split the tokenized texts into chunks of block_size\n",
    "grouped_input_ids = [all_input_ids[i:i + block_size] for i in range(0, total_length, block_size)]\n",
    "grouped_attention_masks = [all_attention_masks[i:i + block_size] for i in range(0, total_length, block_size)]\n",
    "\n",
    "# Create batches of size batch_size\n",
    "batched_input_ids = [grouped_input_ids[i:i + batch_size] for i in range(0, len(grouped_input_ids), batch_size)]\n",
    "batched_attention_masks = [grouped_attention_masks[i:i + batch_size] for i in range(0, len(grouped_attention_masks), batch_size)]\n",
    "\n",
    "\n",
    "batched_input_ids = [torch.tensor(batch) for batch in batched_input_ids]\n",
    "batched_attention_masks = [torch.tensor(batch) for batch in batched_attention_masks]\n",
    "\n",
    "# Example: Inspect the first batch\n",
    "print(\"First batch input IDs:\", batched_input_ids[0].shape)\n",
    "print(\"First batch attention masks:\", batched_attention_masks[0].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "batch_size=16\n",
    "block_size=512\n",
    "def group_texts(examples):\n",
    "        # Concatenate all texts.\n",
    "        concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "        total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "        # We drop the small remainder, and if the total_length < block_size  we exclude this batch and return an empty dict.\n",
    "        # We could add padding if the model supported it instead of this drop, you can customize this part to your needs.\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "        # Split by chunks of max_len.\n",
    "        result = {\n",
    "            k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "            for k, t in concatenated_examples.items()\n",
    "        }\n",
    "        result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(prompts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use OpenWebText Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = load_dataset(\n",
    "            'Skylion007/openwebtext',\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [01:26<00:00, 43.01s/it]\n"
     ]
    }
   ],
   "source": [
    "hf_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\",device_map = 'cpu')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [02:56<00:00, 58.91s/it]\n"
     ]
    }
   ],
   "source": [
    "watermark_config = {\n",
    "     \"cygu/llama-2-7b-logit-watermark-distill-kgw-k0-gamma0.25-delta1\":{\"type\": \"kgw\", \"k\": 0, \"gamma\": 0.25, \"delta\": 1.0, \"seeding_scheme\": \"simple_0\", \"kgw_device\": \"cpu\"},\n",
    "     \"cygu/llama-2-7b-logit-watermark-distill-kgw-k0-gamma0.25-delta2\":{\"type\": \"kgw\", \"k\": 0, \"gamma\": 0.25, \"delta\": 2.0, \"seeding_scheme\": \"simple_0\", \"kgw_device\": \"cpu\"},\n",
    "     \"cygu/llama-2-7b-logit-watermark-distill-kgw-k1-gamma0.25-delta1\":{\"type\": \"kgw\", \"k\": 1, \"gamma\": 0.25, \"delta\": 1.0, \"seeding_scheme\": \"simple_1\", \"kgw_device\": \"cpu\"},\n",
    "     \"cygu/llama-2-7b-logit-watermark-distill-kgw-k1-gamma0.25-delta2\":{\"type\": \"kgw\", \"k\": 1, \"gamma\": 0.25, \"delta\": 2.0, \"seeding_scheme\": \"simple_1\", \"kgw_device\": \"cpu\"},\n",
    "     \"cygu/llama-2-7b-logit-watermark-distill-kgw-k2-gamma0.25-delta2\":{\"type\": \"kgw\", \"k\": 2, \"gamma\": 0.25, \"delta\": 2.0, \"seeding_scheme\": \"simple_2\", \"kgw_device\": \"cpu\"},\n",
    "}\n",
    "watermark_name, watermark_config = watermark_config.popitem()\n",
    "watermarked_model = AutoModelForCausalLM.from_pretrained(watermark_name)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 6, 4096])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['model.layers.0', 'model.layers.1', 'model.layers.2', 'model.layers.3', 'model.layers.4', 'model.layers.5', 'model.layers.6', 'model.layers.7', 'model.layers.8', 'model.layers.9', 'model.layers.10', 'model.layers.11', 'model.layers.12', 'model.layers.13', 'model.layers.14', 'model.layers.15', 'model.layers.16', 'model.layers.17', 'model.layers.18', 'model.layers.19', 'model.layers.20', 'model.layers.21', 'model.layers.22', 'model.layers.23', 'model.layers.24', 'model.layers.25', 'model.layers.26', 'model.layers.27', 'model.layers.28', 'model.layers.29', 'model.layers.30', 'model.layers.31'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def hook_fn(module, input, output):\n",
    "    print(f\"Hook called for {module}\")\n",
    "    return output\n",
    "\n",
    "\n",
    "# Register the hook to the desired layer. For example, to capture the output of the first transformer block:\n",
    "layer_outputs = {}\n",
    "\n",
    "def get_layer_output_hook(layer_name):\n",
    "    def hook_fn(module, input, output):\n",
    "        layer_outputs[layer_name] = output\n",
    "    return hook_fn\n",
    "\n",
    "\n",
    "layer_name = \"model.layers.31\"  # This might need to be adjusted based on your model architecture\n",
    "layer = dict([*hf_model.named_modules()])[layer_name]\n",
    "layer.register_forward_hook(get_layer_output_hook(layer_name))\n",
    "\n",
    "# for i in range(0,1):  # Change the range to match the number of layers you want to capture\n",
    "#     layer_name = f\"model.layers.{i}\"  # Adjust the layer name/path as needed\n",
    "#     layer = dict([*hf_model.named_modules()])[layer_name]\n",
    "#     print(layer)\n",
    "#     layer.register_forward_hook(get_layer_output_hook(layer_name))\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "# Example input\n",
    "input_text = \"This is a test.\"\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "\n",
    "# Move inputs to the device if necessary\n",
    "inputs = {k: v.to(hf_model.device) for k, v in inputs.items()}\n",
    "\n",
    "# Run inference\n",
    "with torch.no_grad():\n",
    "    outputs = hf_model(**inputs)\n",
    "\n",
    "# Retrieve the intermediate embeddings\n",
    "intermediate_embeddings = layer_outputs[layer_name]\n",
    "print(intermediate_embeddings[0].shape)\n",
    "layer_outputs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Genrate watermarked logit and original logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['text', 'timestamp', 'url', 'prompt_text', 'input_ids', 'attention_mask', 'text_completion'])\n",
      "torch.Size([16, 50])\n"
     ]
    }
   ],
   "source": [
    "batch = prompts[0]\n",
    "print(batch.keys())\n",
    "DO_SAMPLE = True\n",
    "temperature=1.0\n",
    "top_p=0.9\n",
    "top_k=0\n",
    "print(batch['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_output = hf_model.generate(\n",
    "                            input_ids=batch[\"input_ids\"],\n",
    "                            attention_mask=batch[\"attention_mask\"],\n",
    "                            do_sample=DO_SAMPLE,\n",
    "                            min_new_tokens=200,\n",
    "                            max_new_tokens=200,\n",
    "                            temperature=temperature,\n",
    "                            top_p=top_p,\n",
    "                            top_k=top_k,\n",
    "                            pad_token_id=tokenizer.eos_token_id,\n",
    "                        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'batch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m vanilla_output_logit  \u001b[38;5;241m=\u001b[39m hf_model(input_ids\u001b[38;5;241m=\u001b[39m\u001b[43mbatch\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m], attention_mask\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'batch' is not defined"
     ]
    }
   ],
   "source": [
    "vanilla_output_logit  = hf_model(input_ids=batch[\"input_ids\"], attention_mask=batch[\"attention_mask\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 50])\n",
      "'re'\n"
     ]
    }
   ],
   "source": [
    "# Convert logits to token IDs by taking the argmax\n",
    "from pprint import pprint\n",
    "predicted_token_ids = torch.argmax(vanilla_output_logit['logits'], dim=-1)\n",
    "print(predicted_token_ids.shape)\n",
    "# Decode token IDs to text\n",
    "predicted_text = tokenizer.decode(predicted_token_ids[1][-1], skip_special_tokens=True)\n",
    "pprint(predicted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "expand(torch.FloatTensor{[8, 8, 1, 512, 512]}, size=[8, 1, 512, 512]): the number of sizes provided (4) must be greater or equal to the number of dimensions in the tensor (5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m attention_mask\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 6\u001b[0m     vanilla_output_logit \u001b[38;5;241m=\u001b[39m \u001b[43mhf_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/watermark/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/watermark/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/watermark/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:1174\u001b[0m, in \u001b[0;36mLlamaForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1171\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> 1174\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1175\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1177\u001b[0m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1178\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1179\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1180\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1181\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1182\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1183\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1185\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1187\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpretraining_tp \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/watermark/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/watermark/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/watermark/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:950\u001b[0m, in \u001b[0;36mLlamaModel.forward\u001b[0;34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m position_ids \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    948\u001b[0m     position_ids \u001b[38;5;241m=\u001b[39m cache_position\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 950\u001b[0m causal_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_update_causal_mask\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    951\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_attentions\u001b[49m\n\u001b[1;32m    952\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    954\u001b[0m \u001b[38;5;66;03m# embed positions\u001b[39;00m\n\u001b[1;32m    955\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m inputs_embeds\n",
      "File \u001b[0;32m~/anaconda3/envs/watermark/lib/python3.11/site-packages/transformers/models/llama/modeling_llama.py:1079\u001b[0m, in \u001b[0;36mLlamaModel._update_causal_mask\u001b[0;34m(self, attention_mask, input_tensor, cache_position, past_key_values, output_attentions)\u001b[0m\n\u001b[1;32m   1077\u001b[0m         padding_mask \u001b[38;5;241m=\u001b[39m causal_mask[:, :, :, :mask_length] \u001b[38;5;241m+\u001b[39m attention_mask[:, \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;28;01mNone\u001b[39;00m, :]\n\u001b[1;32m   1078\u001b[0m         padding_mask \u001b[38;5;241m=\u001b[39m padding_mask \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1079\u001b[0m         \u001b[43mcausal_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43mmask_length\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m=\u001b[39m causal_mask[:, :, :, :mask_length]\u001b[38;5;241m.\u001b[39mmasked_fill(\n\u001b[1;32m   1080\u001b[0m             padding_mask, min_dtype\n\u001b[1;32m   1081\u001b[0m         )\n\u001b[1;32m   1082\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m   1083\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_attn_implementation \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msdpa\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1084\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1089\u001b[0m     \u001b[38;5;66;03m# using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\u001b[39;00m\n\u001b[1;32m   1090\u001b[0m     \u001b[38;5;66;03m# Details: https://github.com/pytorch/pytorch/issues/110213\u001b[39;00m\n\u001b[1;32m   1091\u001b[0m     causal_mask \u001b[38;5;241m=\u001b[39m AttentionMaskConverter\u001b[38;5;241m.\u001b[39m_unmask_unattended(causal_mask, min_dtype)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: expand(torch.FloatTensor{[8, 8, 1, 512, 512]}, size=[8, 1, 512, 512]): the number of sizes provided (4) must be greater or equal to the number of dimensions in the tensor (5)"
     ]
    }
   ],
   "source": [
    "for input_ids, attention_mask in zip(batched_input_ids, batched_attention_masks):\n",
    "    input_ids = input_ids.to(device)\n",
    "    attention_mask = attention_mask.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        vanilla_output_logit = hf_model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "    break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 512, 32000])\n"
     ]
    }
   ],
   "source": [
    "print(vanilla_output_logit.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mdecode(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m1\u001b[39m], skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer.decode(batch['input_ids'][1], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Belying expectations, Prasar Bharti has earned only Rs 58.19 crore (Rs 581.9 million) as revenue during the Commonwealth Games last month.\\nThe gross revenue earned by PB on account of telecasting/broadcasting of advertisements on Doordarshan channel and All India Radio during coverage of the Commonwealth Games is Rs 58.17 crore, Minister of State for Information and Broadcasting S Jagathrakshakan informed the Lok Sabha on Tuesday.\\nWhile AIR earned Rs 2.18 crore (Rs 21.8 million), Doordarshan garnered Rs 55.99 crore (Rs 559.9 million) as revenue, he said. Prasar Bharati had earlier said it knew in advance that the recently concluded Commonwealth Games, for which Doordarshan was the official broadcaster, would not bring in huge advertising revenues.\\nas a result of luke warm response from advertisers.\\nNotably, DD'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['text'][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted_token_ids[0].shape\n",
    "batch['input_ids'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "final_result_folder = \"/remote-home/miintern1/watermark-learnability/data/c4\"\n",
    "watermarked_logits = torch.load(final_result_folder + \"watermark_logits.pt\")\n",
    "vanilla_logits = torch.load(final_result_folder + \"vanilla_logits.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"/remote-home/miintern1/watermark-learnability/data/c4/watermark_vector_evaluation.json\", 'r') as f:\n",
    "    watermark_vector_evaluation = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.  0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.  1.3 1.6 1.9 2.2 2.5 2.8 3.1\n",
      " 3.4 3.7]\n",
      "dict_keys(['watermarked_output', 'full_watermarked_output', 'vanilla_output', 'vanilla_scores', 'watermarked_scores', 'median_seq_rep_3', 'mean_seq_rep_3', 'list_seq_rep_3', 'total_rep_3', 'mean_perplexity', 'median_perplexity', 'perplexities'])\n"
     ]
    }
   ],
   "source": [
    "coefficient_list =concatenated_array = np.concatenate((np.arange(0.0, 1.0, 0.1) , np.arange(1.0, 4.0, 0.3)))\n",
    "print(coefficient_list)\n",
    "print(watermark_vector_evaluation['cygu/llama-2-7b-logit-watermark-distill-kgw-k1-gamma0.25-delta2']['0.4'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Watermark Score = [0.016355227883506743, 1.218366051701328e-08, 0.026491784932231744, 0.17282432636257503, 0.012308539746637977, 1.3979689297990424e-06, 0.0007538752495526407, 0.0020763850313310736, 0.4296066659083557, 0.17282432636257503, 7.772294375042377e-05, 0.0033414074125787197, 0.4945272214607492, 0.0020763850313310736, 0.00014143029167768795, 0.16221293030657816, 3.1168611747939005e-07, 0.03768993880655806, 7.772294375042377e-05, 0.00526769149311491, 1.4280990971319214e-07, 1.4280990971319214e-07, 0.00813539260549211, 0.0033414074125787197, 0.00813539260549211, 2.8726776896510243e-06, 0.012308539746637977, 7.772294375042377e-05, 0.00526769149311491, 0.0014671279285978405, 0.0033414074125787197, 7.491718234060429e-19, 1.1417761815694096e-05, 0.0033414074125787197, 0.0006430814108669477, 0.026491784932231744, 4.004543805359876e-07, 0.00016911265119370079, 0.0020763850313310736, 0.0007538752495526407, 0.00044048696298454207, 4.185168928687397e-05, 6.413665629600846e-08, 5.153782031004316e-09, 7.772294375042377e-05, 0.07176952533150473, 0.0012640422276867533, 1.3979689297990424e-06, 0.0012640422276867533, 0.00014143029167768795, 1.1417761815694096e-05, 0.07176952533150473, 0.0020763850313310736, 2.208264238251801e-05, 0.018243682347378828, 0.00014143029167768795, 8.687637967599595e-10, 5.7852274903586825e-06, 0.00014143029167768795, 0.012308539746637977, 0.018243682347378828, 4.004543805359876e-07, 0.012308539746637977, 5.0819502902761e-05, 0.012308539746637977, 0.05253966459642522, 0.026491784932231744, 3.1168611747939005e-07, 2.8234179538166053e-08, 4.185168928687397e-05, 7.772294375042377e-05, 1.3979689297990424e-06, 1.850289616547429e-07, 3.1168611747939005e-07, 0.026491784932231744, 0.00044048696298454207, 0.00025215972974174425, 0.00044048696298454207, 0.05253966459642522, 0.0002992415380524399, 0.07784753488527126, 2.208264238251801e-05, 2.8726776896510243e-06, 0.020305707993592128, 0.0033414074125787197, 0.0020763850313310736, 2.8726776896510243e-06, 4.185168928687397e-05, 2.8726776896510243e-06, 4.8905302519953945e-08, 0.03768993880655806, 0.00813539260549211, 4.185168928687397e-05, 0.19520011678884325, 0.00014143029167768795, 0.00044048696298454207, 4.185168928687397e-05, 0.00044048696298454207, 4.185168928687397e-05, 0.0020763850313310736, 0.018243682347378828, 2.8726776896510243e-06, 0.00526769149311491, 0.018243682347378828, 3.6008151398279957e-06, 0.0016985061031091282, 0.00813539260549211, 0.0033414074125787197, 4.185168928687397e-05, 4.185168928687397e-05, 0.005984616771209155, 1.3979689297990424e-06, 3.4621625478619577e-10, 1.1417761815694096e-05, 1.1417761815694096e-05, 0.0020763850313310736, 5.180739749914511e-11, 4.185168928687397e-05, 0.00044048696298454207, 1.4280990971319214e-07, 3.436341394431526e-05, 0.30763174579810193, 0.00526769149311491, 0.020305707993592128, 3.1168611747939005e-07, 4.185168928687397e-05, 0.20465851453056633, 0.009178679390778589, 4.185168928687397e-05, 0.0033414074125787197, 0.00044048696298454207, 1.1417761815694096e-05, 5.153782031004316e-09, 7.772294375042377e-05, 2.8234179538166053e-08, 2.8234179538166053e-08, 0.00044048696298454207, 0.0005188340346265688, 0.00044048696298454207, 0.00813539260549211, 0.012308539746637977, 0.00025215972974174425, 0.00526769149311491, 0.05253966459642522, 0.00014143029167768795, 0.00044048696298454207, 6.66759169705751e-07, 0.32242573363325705, 1.4280990971319214e-07, 0.09608156354657002, 0.03768993880655806, 1.4280990971319214e-07, 0.018243682347378828, 0.005984616771209155, 0.00526769149311491, 9.807077168101148e-05, 1.3979689297990424e-06, 0.018243682347378828, 2.8726776896510243e-06, 5.180739749914511e-11, 0.0005188340346265688, 0.012308539746637977, 0.07176952533150473, 0.00025215972974174425, 0.029291323400813283, 0.0020763850313310736, 0.0012640422276867533, 0.00014143029167768795, 0.21681740584472967, 0.00526769149311491, 0.16221293030657816, 0.00011794222905803933, 2.208264238251801e-05, 0.020305707993592128, 0.029291323400813283, 0.0020763850313310736, 2.208264238251801e-05, 1.218366051701328e-08, 0.09608156354657002, 0.0020763850313310736, 0.0023926406266429954, 1.4280990971319214e-07, 6.413665629600846e-08, 0.00014143029167768795, 1.4083981957400238e-05, 0.00014143029167768795, 3.1168611747939005e-07, 0.018243682347378828, 0.012308539746637977, 2.208264238251801e-05, 2.208264238251801e-05, 0.0020763850313310736, 0.018243682347378828, 0.026491784932231744, 1.218366051701328e-08, 0.30763174579810193, 0.07176952533150473, 0.1260836532587778, 3.1168611747939005e-07, 0.00025215972974174425, 0.30763174579810193, 0.00025215972974174425, 0.00025215972974174425, 0.0012640422276867533, 0.00813539260549211, 0.00014143029167768795, 0.05253966459642522, 0.09608156354657002, 0.012308539746637977, 0.00014143029167768795, 0.00813539260549211, 0.05253966459642522, 0.0033414074125787197, 0.0033414074125787197, 3.1168611747939005e-07, 0.0020763850313310736, 0.00044048696298454207, 0.0020763850313310736, 0.0014671279285978405, 0.0033414074125787197, 3.1168611747939005e-07, 0.00813539260549211, 0.0012640422276867533, 0.0002992415380524399, 1.1417761815694096e-05, 0.0007538752495526407, 0.00014143029167768795, 0.018243682347378828, 3.1168611747939005e-07, 7.161570374319398e-12, 0.00016911265119370079, 5.7852274903586825e-06, 0.0033414074125787197, 2.8234179538166053e-08, 6.413665629600846e-08, 0.018243682347378828, 1.3979689297990424e-06, 0.0033414074125787197, 0.00014143029167768795, 0.00813539260549211, 0.00044048696298454207, 0.00813539260549211, 0.0007538752495526407, 6.249738972469774e-13, 0.0007538752495526407, 0.016355227883506743, 0.018243682347378828, 2.208264238251801e-05, 0.00044048696298454207, 0.0012640422276867533, 0.029291323400813283, 0.00014143029167768795, 0.07176952533150473, 0.7416723095695861, 0.0020763850313310736, 0.05253966459642522, 0.25329407978722024, 0.0038229784327127665, 2.8726776896510243e-06, 9.364978073223926e-05, 0.0012640422276867533, 0.0020763850313310736, 4.185168928687397e-05, 0.20465851453056633, 0.012308539746637977, 7.772294375042377e-05, 0.03768993880655806, 0.00025215972974174425, 0.00526769149311491, 6.413665629600846e-08, 0.00025215972974174425, 2.208264238251801e-05, 1.1417761815694096e-05, 0.0414023702540248, 2.8726776896510243e-06, 0.012308539746637977, 0.0007538752495526407, 0.00813539260549211, 1.6196290272301364e-08, 0.0012640422276867533, 0.00526769149311491, 0.026491784932231744, 4.185168928687397e-05, 0.03768993880655806, 2.208264238251801e-05, 0.0007538752495526407, 5.7852274903586825e-06, 0.0020763850313310736, 2.8726776896510243e-06, 6.91125165250654e-09, 0.00813539260549211, 0.00813539260549211, 0.1260836532587778, 0.018243682347378828, 0.00014143029167768795, 0.0033414074125787197, 5.7852274903586825e-06, 0.00014143029167768795, 0.00016911265119370079, 6.66759169705751e-07, 4.185168928687397e-05, 1.1417761815694096e-05, 0.018243682347378828, 0.00813539260549211, 2.208264238251801e-05, 0.00044048696298454207, 2.208264238251801e-05, 0.009178679390778589, 0.09608156354657002, 0.0007538752495526407, 0.018243682347378828, 0.00044048696298454207, 2.208264238251801e-05, 0.21681740584472967, 0.009178679390778589, 2.8726776896510243e-06, 0.00044048696298454207, 0.00526769149311491, 0.00014143029167768795, 0.00044048696298454207, 0.0012640422276867533, 0.0038229784327127665, 0.05253966459642522, 0.026491784932231744, 2.208264238251801e-05, 0.00014143029167768795, 7.772294375042377e-05, 0.0023926406266429954, 6.66759169705751e-07, 5.7852274903586825e-06, 0.0007538752495526407, 1.1417761815694096e-05, 0.07176952533150473, 4.004543805359876e-07, 0.00016911265119370079, 0.00025215972974174425, 0.00025215972974174425, 0.0002992415380524399, 5.153782031004316e-09, 0.25329407978722024, 6.66759169705751e-07, 0.018243682347378828, 7.772294375042377e-05, 0.00016911265119370079, 0.03768993880655806, 0.0012640422276867533, 4.185168928687397e-05, 0.00813539260549211, 1.3979689297990424e-06, 4.185168928687397e-05, 0.00014143029167768795, 8.495616097290753e-07, 2.1371291940963133e-09, 0.00014143029167768795, 7.772294375042377e-05, 0.05734712978019507, 0.0007538752495526407, 0.00021189447953663386, 1.1417761815694096e-05, 0.0020763850313310736, 0.00014143029167768795, 0.00813539260549211, 0.25329407978722024, 1.4280990971319214e-07, 8.495616097290753e-07, 0.026491784932231744, 2.8234179538166053e-08, 1.4280990971319214e-07, 2.5847254272870104e-12, 0.09608156354657002, 0.026491784932231744, 1.1417761815694096e-05, 0.013792325396823218, 1.4280990971319214e-07, 6.66759169705751e-07, 4.185168928687397e-05, 0.00044048696298454207, 0.0020763850313310736, 5.7852274903586825e-06, 0.0012640422276867533, 2.208264238251801e-05, 0.00044048696298454207, 0.026491784932231744, 1.1417761815694096e-05, 2.208264238251801e-05, 0.00014143029167768795, 5.7852274903586825e-06, 1.1417761815694096e-05, 6.413665629600846e-08, 7.772294375042377e-05, 0.00813539260549211, 0.00813539260549211, 0.00044048696298454207, 0.1260836532587778, 0.00016911265119370079, 0.26687849628994026, 0.00526769149311491, 0.0023926406266429954, 0.00813539260549211, 0.00044048696298454207, 0.026491784932231744, 0.1519945489193213, 5.0819502902761e-05, 0.0007538752495526407, 0.00014143029167768795, 0.00813539260549211, 0.012308539746637977, 0.013792325396823218, 2.8726776896510243e-06, 2.8726776896510243e-06, 0.00014143029167768795, 1.4280990971319214e-07, 1.4280990971319214e-07, 0.018243682347378828, 0.0012640422276867533, 0.0020763850313310736, 1.1417761815694096e-05, 4.185168928687397e-05, 0.00025215972974174425, 4.185168928687397e-05, 5.7852274903586825e-06, 0.0033414074125787197, 0.00044048696298454207, 4.185168928687397e-05, 7.772294375042377e-05, 0.16221293030657816, 0.0002992415380524399, 0.0005188340346265688, 0.026491784932231744, 0.0012640422276867533, 0.0012640422276867533, 2.208264238251801e-05, 0.07176952533150473, 0.01095964370101508, 0.018243682347378828, 0.00813539260549211, 9.364978073223926e-05, 0.012308539746637977, 1.1417761815694096e-05, 0.00014143029167768795, 1.7666461197620142e-06, 0.0033414074125787197, 0.10358208597462179, 0.03768993880655806, 0.1260836532587778, 0.0023926406266429954, 0.00044048696298454207, 0.00044048696298454207, 0.0020763850313310736, 0.0007538752495526407, 1.0790279406238122e-13, 0.07176952533150473, 0.012308539746637977, 0.018243682347378828, 0.03768993880655806, 1.1417761815694096e-05, 0.029291323400813283, 0.07176952533150473, 0.00014143029167768795, 0.00813539260549211, 0.00025215972974174425, 1.3979689297990424e-06, 0.00526769149311491, 4.185168928687397e-05, 0.0020763850313310736, 7.267101730688786e-11, 0.026491784932231744, 0.0020763850313310736, 1.3979689297990424e-06, 0.0020763850313310736, 3.4621625478619577e-10, 5.7852274903586825e-06, 3.1168611747939005e-07, 0.0007538752495526407, 0.0002992415380524399, 0.00044048696298454207, 0.012308539746637977, 1.1417761815694096e-05, 6.413665629600846e-08, 0.0038229784327127665, 0.00025215972974174425, 0.009178679390778589, 2.1371291940963133e-09, 7.772294375042377e-05, 0.00014143029167768795, 4.185168928687397e-05, 0.00526769149311491, 0.0012640422276867533, 6.66759169705751e-07, 0.014629736495197162, 7.772294375042377e-05, 0.00014143029167768795, 1.4280990971319214e-07, 0.00526769149311491, 0.05253966459642522, 0.00526769149311491, 0.10358208597462179, 0.0033414074125787197, 0.00526769149311491, 7.267101730688786e-11, 0.00526769149311491, 0.018243682347378828, 0.0012640422276867533, 6.413665629600846e-08, 2.208264238251801e-05, 0.0038229784327127665, 0.0007538752495526407, 0.06603941531448622, 0.00044048696298454207, 0.0007538752495526407, 0.05734712978019507, 0.0012640422276867533]\n",
      "Vanilla Score = [0.4945272214607492, 0.16221293030657816, 0.4945272214607492, 0.6850125974681823, 0.012308539746637977, 0.20465851453056633, 0.7416723095695861, 0.32242573363325705, 0.7927771479355588, 0.6392026174713024, 0.4945272214607492, 0.4296066659083557, 0.9967774549158284, 0.05253966459642522, 0.5598834854396685, 0.6850125974681823, 0.837576843905729, 0.1260836532587778, 0.16221293030657816, 0.03768993880655806, 0.4296066659083557, 0.3668076971387207, 0.20465851453056633, 0.9072776115814041, 0.6239326241390097, 0.5598834854396685, 0.4945272214607492, 0.5598834854396685, 0.9671836481639393, 0.26687849628994026, 0.6850125974681823, 0.7416723095695861, 0.4296066659083557, 0.4296066659083557, 0.837576843905729, 0.8757288430545195, 0.38250743933112885, 0.44583680479645454, 0.6850125974681823, 0.7927771479355588, 0.7927771479355588, 0.9523076727949352, 0.4296066659083557, 0.4296066659083557, 0.9072776115814041, 0.6850125974681823, 0.4945272214607492, 0.6239326241390097, 0.16221293030657816, 0.3668076971387207, 0.837576843905729, 0.837576843905729, 0.8757288430545195, 0.5598834854396685, 0.837576843905729, 0.09608156354657002, 0.6239326241390097, 0.9910418021389016, 0.018243682347378828, 0.6239326241390097, 0.30763174579810193, 0.1351159725207279, 0.5598834854396685, 0.05253966459642522, 0.6850125974681823, 0.6239326241390097, 0.3668076971387207, 0.3668076971387207, 0.3668076971387207, 0.7927771479355588, 0.7416723095695861, 0.012308539746637977, 0.5108662874554788, 0.4296066659083557, 0.8757288430545195, 0.4945272214607492, 0.3668076971387207, 0.7927771479355588, 0.9072776115814041, 0.32242573363325705, 0.16221293030657816, 0.3668076971387207, 0.6392026174713024, 0.05734712978019507, 0.6239326241390097, 0.7927771479355588, 0.3668076971387207, 0.837576843905729, 0.5598834854396685, 0.5598834854396685, 0.30763174579810193, 0.4296066659083557, 0.16221293030657816, 0.10358208597462179, 0.6850125974681823, 0.6239326241390097, 0.03768993880655806, 0.9072776115814041, 0.6239326241390097, 0.9671836481639393, 0.932597005431005, 0.6850125974681823, 0.30763174579810193, 0.5598834854396685, 0.32242573363325705, 0.05734712978019507, 0.20465851453056633, 0.9994613326123495, 0.07176952533150473, 0.9857641802872973, 0.5108662874554788, 0.4945272214607492, 0.932597005431005, 0.30763174579810193, 0.9072776115814041, 0.6850125974681823, 0.9857641802872973, 0.30763174579810193, 0.20465851453056633, 0.03768993880655806, 0.7927771479355588, 0.20465851453056633, 0.20465851453056633, 0.38250743933112885, 0.9072776115814041, 0.16221293030657816, 0.9072776115814041, 0.4945272214607492, 0.3668076971387207, 0.30763174579810193, 0.4296066659083557, 0.07176952533150473, 0.09608156354657002, 0.2271022456134706, 0.026491784932231744, 0.8757288430545195, 0.03768993880655806, 0.009178679390778589, 0.17282432636257503, 0.9523076727949352, 0.25329407978722024, 0.5598834854396685, 0.1260836532587778, 0.4945272214607492, 0.9523076727949352, 0.9910418021389016, 0.16221293030657816, 0.32242573363325705, 0.20465851453056633, 0.4945272214607492, 0.5598834854396685, 0.4296066659083557, 0.3668076971387207, 0.17282432636257503, 0.4296066659083557, 0.03768993880655806, 0.30763174579810193, 0.1260836532587778, 0.7927771479355588, 0.7927771479355588, 0.17282432636257503, 0.16221293030657816, 0.05253966459642522, 0.6850125974681823, 0.9560266666371865, 0.837576843905729, 0.03768993880655806, 0.7927771479355588, 0.10358208597462179, 0.5598834854396685, 0.837576843905729, 0.6850125974681823, 0.9989868525508205, 0.5758957701145038, 0.6991775254935335, 0.30763174579810193, 0.6239326241390097, 0.1260836532587778, 0.8757288430545195, 0.05253966459642522, 0.09608156354657002, 0.9072776115814041, 0.4296066659083557, 0.837576843905729, 0.5108662874554788, 0.3668076971387207, 0.3668076971387207, 0.9523076727949352, 0.07176952533150473, 0.3668076971387207, 0.16221293030657816, 0.9780617051525234, 0.837576843905729, 0.7927771479355588, 0.07176952533150473, 0.30763174579810193, 0.6850125974681823, 0.9671836481639393, 0.9523076727949352, 0.05253966459642522, 0.3668076971387207, 0.30763174579810193, 0.9857641802872973, 0.3668076971387207, 0.16221293030657816, 0.9671836481639393, 0.837576843905729, 0.20465851453056633, 0.5598834854396685, 0.16221293030657816, 0.30763174579810193, 0.00813539260549211, 0.012308539746637977, 0.5598834854396685, 0.4296066659083557, 0.8836160351862405, 0.9072776115814041, 0.30763174579810193, 0.8471148436929272, 0.6239326241390097, 0.10358208597462179, 0.25329407978722024, 0.25329407978722024, 0.17282432636257503, 0.03768993880655806, 0.3668076971387207, 0.9671836481639393, 0.9999946432989134, 0.07176952533150473, 0.05253966459642522, 0.0002992415380524399, 0.16221293030657816, 0.8757288430545195, 0.837576843905729, 0.20465851453056633, 0.837576843905729, 0.25329407978722024, 0.8757288430545195, 0.4945272214607492, 0.20465851453056633, 0.837576843905729, 0.7416723095695861, 0.1260836532587778, 0.4945272214607492, 0.932597005431005, 0.6239326241390097, 0.25329407978722024, 0.5598834854396685, 0.018243682347378828, 0.6850125974681823, 0.5758957701145038, 0.9072776115814041, 0.7927771479355588, 0.7927771479355588, 0.8757288430545195, 0.1260836532587778, 0.6239326241390097, 0.26687849628994026, 0.012308539746637977, 0.07784753488527126, 0.5598834854396685, 0.4945272214607492, 0.16221293030657816, 0.8757288430545195, 0.6239326241390097, 0.30763174579810193, 0.25329407978722024, 0.7416723095695861, 0.3668076971387207, 0.20465851453056633, 0.6850125974681823, 0.4296066659083557, 0.6850125974681823, 0.8836160351862405, 0.09608156354657002, 0.6239326241390097, 0.25329407978722024, 0.3668076971387207, 0.1351159725207279, 0.6850125974681823, 0.4296066659083557, 0.026491784932231744, 0.5598834854396685, 0.837576843905729, 0.7416723095695861, 0.837576843905729, 0.00813539260549211, 0.8757288430545195, 0.9981614549437857, 0.44583680479645454, 0.5598834854396685, 0.012308539746637977, 0.8757288430545195, 0.932597005431005, 0.4296066659083557, 0.4945272214607492, 0.4945272214607492, 0.25329407978722024, 0.21681740584472967, 0.4296066659083557, 0.6239326241390097, 0.6850125974681823, 0.7927771479355588, 0.6239326241390097, 0.4945272214607492, 0.30763174579810193, 0.6392026174713024, 0.6392026174713024, 0.16221293030657816, 0.30763174579810193, 0.4134311739525406, 0.6850125974681823, 0.5598834854396685, 0.32242573363325705, 0.9136074600438041, 0.7927771479355588, 0.07176952533150473, 0.16221293030657816, 0.16221293030657816, 0.8757288430545195, 0.09608156354657002, 0.8039770719281015, 0.6239326241390097, 0.8757288430545195, 0.932597005431005, 0.16221293030657816, 0.6850125974681823, 0.8471148436929272, 0.5598834854396685, 0.25329407978722024, 0.1260836532587778, 0.10358208597462179, 0.9945386313411916, 0.9560266666371865, 0.8039770719281015, 0.09608156354657002, 0.9005854485605496, 0.17282432636257503, 0.5598834854396685, 0.3668076971387207, 0.05253966459642522, 0.3668076971387207, 0.03768993880655806, 0.6991775254935335, 0.7927771479355588, 0.07176952533150473, 0.6850125974681823, 0.03768993880655806, 0.6239326241390097, 0.7927771479355588, 0.6850125974681823, 0.8039770719281015, 0.07176952533150473, 0.4945272214607492, 0.07176952533150473, 0.9560266666371865, 0.25329407978722024, 0.6850125974681823, 0.9523076727949352, 0.026491784932231744, 0.932597005431005, 0.012308539746637977, 0.5598834854396685, 0.9857641802872973, 0.8471148436929272, 0.9072776115814041, 0.07176952533150473, 0.3668076971387207, 0.20465851453056633, 0.9523076727949352, 0.837576843905729, 0.07176952533150473, 0.9560266666371865, 0.837576843905729, 0.1260836532587778, 0.5598834854396685, 0.7927771479355588, 0.4945272214607492, 0.7927771479355588, 0.5598834854396685, 0.07176952533150473, 0.4945272214607492, 0.837576843905729, 0.7927771479355588, 0.6850125974681823, 0.6239326241390097, 0.20465851453056633, 0.25329407978722024, 0.9910418021389016, 0.20465851453056633, 0.8757288430545195, 0.7416723095695861, 0.8757288430545195, 0.7416723095695861, 0.17282432636257503, 0.32242573363325705, 0.0012640422276867533, 0.9375246722719879, 0.7927771479355588, 0.6239326241390097, 0.0020763850313310736, 0.4945272214607492, 0.32242573363325705, 0.4945272214607492, 0.16221293030657816, 0.9523076727949352, 0.5598834854396685, 0.9983678043455444, 0.09608156354657002, 0.7416723095695861, 0.20465851453056633, 0.00813539260549211, 0.16221293030657816, 0.4296066659083557, 0.4296066659083557, 0.5598834854396685, 0.6850125974681823, 0.32242573363325705, 0.7927771479355588, 0.4945272214607492, 0.7927771479355588, 0.7927771479355588, 0.05253966459642522, 0.6239326241390097, 0.4945272214607492, 0.30763174579810193, 0.6991775254935335, 0.5108662874554788, 0.20465851453056633, 0.6239326241390097, 0.8757288430545195, 0.4945272214607492, 0.25329407978722024, 0.09608156354657002, 0.25329407978722024, 0.20465851453056633, 0.5758957701145038, 0.837576843905729, 0.4296066659083557, 0.3668076971387207, 0.26687849628994026, 0.6239326241390097, 0.6991775254935335, 0.8757288430545195, 0.5758957701145038, 0.16221293030657816, 0.6850125974681823, 0.012308539746637977, 0.018243682347378828, 0.9072776115814041, 0.20465851453056633, 0.30763174579810193, 0.7416723095695861, 0.6239326241390097, 0.5598834854396685, 0.5598834854396685, 0.44583680479645454, 0.20465851453056633, 0.6850125974681823, 0.5598834854396685, 0.6850125974681823, 0.7416723095695861, 0.7927771479355588, 0.9780617051525234, 0.4296066659083557, 0.5108662874554788, 0.5598834854396685, 0.5598834854396685, 0.4945272214607492, 0.30763174579810193, 0.07176952533150473, 0.9671836481639393, 0.4945272214607492, 0.3668076971387207, 0.026491784932231744, 0.6850125974681823, 0.6239326241390097, 0.4296066659083557, 0.07176952533150473, 0.26687849628994026, 0.4945272214607492, 0.6991775254935335, 0.5598834854396685, 0.09608156354657002, 0.837576843905729, 0.3668076971387207, 0.16221293030657816, 0.16221293030657816, 0.4945272214607492, 0.7416723095695861, 0.837576843905729, 0.6850125974681823, 0.3668076971387207, 0.20465851453056633, 0.9523076727949352, 0.9857641802872973, 0.9799873239362169, 0.6850125974681823, 0.5598834854396685, 0.44583680479645454, 0.25329407978722024, 0.6239326241390097, 0.4945272214607492, 0.932597005431005, 0.8757288430545195, 0.07784753488527126, 0.9671836481639393, 0.6239326241390097, 0.05253966459642522, 0.6239326241390097, 0.029291323400813283, 0.3668076971387207]\n",
      "Perplexity = 5.401320188073441\n",
      "('Marketers are employed in the public sector as well as the private sector.\\n'\n",
      " 'Most people think of marketing as a strictly private sector activity, but '\n",
      " 'the reality is people with sales and marketing backgrounds are hired by '\n",
      " 'government agencies at the federal, state and local level for a variety of '\n",
      " 'purposes. Marketers in government focus on the public interest. Government '\n",
      " 'marketing campaigns typically have the goal of increasing public awareness '\n",
      " 'and understanding of issues important to society, such as improving health '\n",
      " 'care, promoting energy conservation, and\\uf020 raising environmental '\n",
      " 'awareness. The associated jobs in the public sector vary in responsibility '\n",
      " 'and task as the Government plays different roles and utilizes different '\n",
      " 'strategies in order to fulfill the public interest, The first government '\n",
      " 'marketing job was the War Advertising Council (WAC) established in 1917 '\n",
      " 'during World War I.  The WAC developed posters urging Americans to conserve '\n",
      " 'food and fuel and to work together to win the war effort. More recent '\n",
      " 'government marketing campaigns include:  Healthcare: Government agencies '\n",
      " 'have implemented various  campaigns to increase awareness about proper '\n",
      " 'vaccination practices, disease outbreak and the benefits of medical ins')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "coefficient = '1.6'\n",
    "model_name = 'cygu/llama-2-7b-logit-watermark-distill-kgw-k2-gamma0.25-delta2'\n",
    "print(f\"Watermark Score = {watermark_vector_evaluation[model_name][coefficient]['watermarked_scores']}\")\n",
    "print(f\"Vanilla Score = {watermark_vector_evaluation[model_name][coefficient]['vanilla_scores']}\")\n",
    "print(f\"Perplexity = {watermark_vector_evaluation[model_name][coefficient]['mean_perplexity']}\")\n",
    "pprint(watermark_vector_evaluation[model_name][coefficient]['full_watermarked_output'][2])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "watermark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
