{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /remote-home1/miintern1/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import argparse\n",
    "import os\n",
    "import copy\n",
    "os.environ['http_proxy'] = \"http://10.176.58.101:7890\"\n",
    "os.environ['https_proxy'] = \"http://10.176.58.101:7890\"\n",
    "os.environ['all_proxy'] = \"socks5://10.176.58.101:7891\"\n",
    "import sys\n",
    "sys.path.append(('../'))\n",
    "sys.path.append(('../../'))\n",
    "# sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "import json\n",
    "from typing import Dict\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed, LogitsProcessorList\n",
    "from transformer_lens import HookedTransformer\n",
    "from task_vector import TaskVector\n",
    "import plotly.express as px\n",
    "\n",
    "from watermarks.kgw.watermark_processor import WatermarkDetector\n",
    "from watermarks.aar.aar_watermark import AarWatermarkDetector\n",
    "from watermarks.watermark_types import WatermarkType\n",
    "from watermarks.kgw.watermark_processor import WatermarkLogitsProcessor\n",
    "\n",
    "\n",
    "from huggingface_hub import login\n",
    "# login(token=\"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\")\n",
    "login(token='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')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bcfed0b76e9c4f3c86f800b666a700ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a90ccb63def47b798ed457d6d20241b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10265e5520574ea587398fc0018de12a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer  = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-hf')\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "hf_model = AutoModelForCausalLM.from_pretrained('meta-llama/Llama-2-7b-hf')\n",
    "math_model = AutoModelForCausalLM.from_pretrained('neuralmagic/Llama-2-7b-gsm8k')\n",
    "watermarked_model = AutoModelForCausalLM.from_pretrained(\"cygu/llama-2-7b-logit-watermark-distill-kgw-k0-gamma0.25-delta1\")\n",
    "task_vector = TaskVector(hf_model, watermarked_model)\n",
    "finetune_task_vector = TaskVector(hf_model, math_model)\n",
    "watermarked_math_model = task_vector.apply_to(math_model, scaling_coef= 1.3)\n",
    "\n",
    "# del hf_model\n",
    "# del watermarked_model\n",
    "# del math_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task vector norm: 31.284238815307617\n",
      "Finetune task vector norm: 32.49064636230469\n"
     ]
    }
   ],
   "source": [
    "task_vector_norm = sum([torch.norm(v)**2 for v in task_vector.vector.values()])**0.5\n",
    "finetune_task_vector_norm = sum([torch.norm(v)**2 for v in finetune_task_vector.vector.values()])**0.5\n",
    "\n",
    "print(f\"Task vector norm: {task_vector_norm}\")\n",
    "print(f\"Finetune task vector norm: {finetune_task_vector_norm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0207)\n",
      "tensor(0.0107)\n"
     ]
    }
   ],
   "source": [
    "print(torch.norm(task_vector.vector['model.embed_tokens.weight']) / torch.norm(hf_model.state_dict()['model.embed_tokens.weight']))\n",
    "print(torch.norm(finetune_task_vector.vector['model.embed_tokens.weight']) / torch.norm(hf_model.state_dict()['model.embed_tokens.weight']))\n",
    "# print(torch.norm(finetune_task_vector.vector['model.embed_tokens.weight']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='OnAnOrange/llama-alpaca-watermarked-model', vocab_size=32000, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32000: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer= AutoTokenizer.from_pretrained('OnAnOrange/llama-alpaca-watermarked-model')\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='PKU-Alignment/alpaca-7b-reproduced-llama-2', vocab_size=32000, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '<pad>'}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t32000: AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('PKU-Alignment/alpaca-7b-reproduced-llama-2')\n",
    "\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/OnAnOrange/llama-alpaca-watermarked-model/commit/2c02e9f460d61d2ec796de8c54fe358bd463528f', commit_message='Upload tokenizer', commit_description='', oid='2c02e9f460d61d2ec796de8c54fe358bd463528f', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('PKU-Alignment/alpaca-7b-reproduced-llama-2')\n",
    "tokenizer.push_to_hub('OnAnOrange/llama-alpaca-watermarked-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /remote-home1/miintern1/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1765b47078374688830d68d5020ccbc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c319cbffbb2f431695730e174519276d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00006-of-00006.safetensors:   0%|          | 0.00/2.68G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e135647fd2b48ab977083290326c775",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00006.safetensors:   0%|          | 0.00/4.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d718c51c5215400ba7437ad5df688f0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00005-of-00006.safetensors:   0%|          | 0.00/4.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e96c23e4aeb469986be9b86bdb55a73",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 6 LFS files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdcb93a1353c4fe19b9bd40ef72b6885",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00006.safetensors:   0%|          | 0.00/4.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0936920d120945a49eddd297e0bdb407",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00006.safetensors:   0%|          | 0.00/4.84G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab30040a3e2d4ccd86e2e00242c6956d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00006.safetensors:   0%|          | 0.00/4.86G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/OnAnOrange/llama-alpaca-watermarked-model/commit/7e139da59dea17b08f444ebd929165a1b9aa4f94', commit_message='Upload LlamaForCausalLM', commit_description='', oid='7e139da59dea17b08f444ebd929165a1b9aa4f94', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "login(token='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')\n",
    "watermarked_math_model.push_to_hub('OnAnOrange/llama-alpaca-watermarked-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.embed_tokens.weight torch.Size([32000, 4096])\n",
      "model.layers.0.self_attn.q_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.0.self_attn.k_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.0.self_attn.v_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.0.self_attn.o_proj.weight torch.Size([4096, 4096])\n",
      "model.layers.0.mlp.gate_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.0.mlp.up_proj.weight torch.Size([11008, 4096])\n",
      "model.layers.0.mlp.down_proj.weight torch.Size([4096, 11008])\n",
      "model.layers.0.input_layernorm.weight torch.Size([4096])\n",
      "model.layers.0.post_attention_layernorm.weight torch.Size([4096])\n"
     ]
    }
   ],
   "source": [
    "for name, param in list(watermarked_math_model.named_parameters())[:10]:\n",
    "    print(name, param.shape)\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1331f35250d47be95b352118b988eee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/OnAnOrange/llama-gms8k-watermarked-model/commit/9b6abdb404f72dac6effe319b4ff92c1dbe91053', commit_message='Upload tokenizer', commit_description='', oid='9b6abdb404f72dac6effe319b4ff92c1dbe91053', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.push_to_hub('OnAnOrange/llama-gms8k-watermarked-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/remote-home1/miintern1/anaconda3/envs/watermark/lib/python3.12/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "008f7a31a71c45fd8a2c591ecb3b2b00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebdd3b493c1d4055a00c34734db3861f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model Llama-2-7b into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7628040b2fb049268f0bc1bc48979489",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:You are not using LayerNorm, so the writing weights can't be centered! Skipping\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained model Llama-2-7b into HookedTransformer\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "HookedTransformer(\n",
       "  (embed): Embed()\n",
       "  (hook_embed): HookPoint()\n",
       "  (blocks): ModuleList(\n",
       "    (0-31): 32 x TransformerBlock(\n",
       "      (ln1): RMSNormPre(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (ln2): RMSNormPre(\n",
       "        (hook_scale): HookPoint()\n",
       "        (hook_normalized): HookPoint()\n",
       "      )\n",
       "      (attn): Attention(\n",
       "        (hook_k): HookPoint()\n",
       "        (hook_q): HookPoint()\n",
       "        (hook_v): HookPoint()\n",
       "        (hook_z): HookPoint()\n",
       "        (hook_attn_scores): HookPoint()\n",
       "        (hook_pattern): HookPoint()\n",
       "        (hook_result): HookPoint()\n",
       "        (hook_rot_k): HookPoint()\n",
       "        (hook_rot_q): HookPoint()\n",
       "      )\n",
       "      (mlp): GatedMLP(\n",
       "        (hook_pre): HookPoint()\n",
       "        (hook_pre_linear): HookPoint()\n",
       "        (hook_post): HookPoint()\n",
       "      )\n",
       "      (hook_attn_in): HookPoint()\n",
       "      (hook_q_input): HookPoint()\n",
       "      (hook_k_input): HookPoint()\n",
       "      (hook_v_input): HookPoint()\n",
       "      (hook_mlp_in): HookPoint()\n",
       "      (hook_attn_out): HookPoint()\n",
       "      (hook_mlp_out): HookPoint()\n",
       "      (hook_resid_pre): HookPoint()\n",
       "      (hook_resid_mid): HookPoint()\n",
       "      (hook_resid_post): HookPoint()\n",
       "    )\n",
       "  )\n",
       "  (ln_final): RMSNormPre(\n",
       "    (hook_scale): HookPoint()\n",
       "    (hook_normalized): HookPoint()\n",
       "  )\n",
       "  (unembed): Unembed()\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer  = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-hf')\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    \n",
    "hf_model = AutoModelForCausalLM.from_pretrained('meta-llama/Llama-2-7b-hf')\n",
    "watermarked_model = AutoModelForCausalLM.from_pretrained(\"cygu/llama-2-7b-logit-watermark-distill-kgw-k0-gamma0.25-delta1\")\n",
    "task_vector = TaskVector(hf_model, watermarked_model)\n",
    "coefficient_watermarked_model = task_vector.apply_to(hf_model, scaling_coef= 1.3)\n",
    "coefficient_model = HookedTransformer.from_pretrained('Llama-2-7b', device=\"cpu\", hf_model=coefficient_watermarked_model)\n",
    "coefficient_model.eval()\n",
    "\n",
    "vanilla_model = HookedTransformer.from_pretrained('Llama-2-7b', device=\"cpu\")\n",
    "vanilla_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 14\u001b[0m\n\u001b[1;32m     10\u001b[0m watermark_config \u001b[38;5;241m=\u001b[39m watermark_configs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcygu/llama-2-7b-logit-watermark-distill-kgw-k0-gamma0.25-delta2\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     11\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     12\u001b[0m detector \u001b[38;5;241m=\u001b[39m WatermarkDetector(\n\u001b[1;32m     13\u001b[0m                             device\u001b[38;5;241m=\u001b[39mwatermark_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkgw_device\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[0;32m---> 14\u001b[0m                             tokenizer\u001b[38;5;241m=\u001b[39m\u001b[43mtokenizer\u001b[49m,\n\u001b[1;32m     15\u001b[0m                             vocab\u001b[38;5;241m=\u001b[39mtokenizer\u001b[38;5;241m.\u001b[39mget_vocab()\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m     16\u001b[0m                             gamma\u001b[38;5;241m=\u001b[39mwatermark_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgamma\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     17\u001b[0m                             seeding_scheme\u001b[38;5;241m=\u001b[39mwatermark_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseeding_scheme\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     18\u001b[0m                             normalizers\u001b[38;5;241m=\u001b[39m[],\n\u001b[1;32m     19\u001b[0m                         )\n\u001b[1;32m     21\u001b[0m example_text \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNeural networks are powerful tools.\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMachine learning models can learn from data.\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "# hf_instruct_model = AutoModelForCausalLM.from_pretrained('meta-llama/Llama-2-7b-chat-hf')\n",
    "\n",
    "watermark_configs = {\n",
    "     \"cygu/llama-2-7b-logit-watermark-distill-kgw-k0-gamma0.25-delta1\":{\"type\": \"kgw\", \"k\": 0, \"gamma\": 0.25, \"delta\": 1.0, \"seeding_scheme\": \"simple_0\", \"kgw_device\": \"cpu\"},\n",
    "     \"cygu/llama-2-7b-logit-watermark-distill-kgw-k0-gamma0.25-delta2\":{\"type\": \"kgw\", \"k\": 0, \"gamma\": 0.25, \"delta\": 2.0, \"seeding_scheme\": \"simple_0\", \"kgw_device\": \"cpu\"},\n",
    "     \"cygu/llama-2-7b-logit-watermark-distill-kgw-k1-gamma0.25-delta1\":{\"type\": \"kgw\", \"k\": 1, \"gamma\": 0.25, \"delta\": 1.0, \"seeding_scheme\": \"simple_1\", \"kgw_device\": \"cpu\"},\n",
    "     \"cygu/llama-2-7b-logit-watermark-distill-kgw-k1-gamma0.25-delta2\":{\"type\": \"kgw\", \"k\": 1, \"gamma\": 0.25, \"delta\": 2.0, \"seeding_scheme\": \"simple_1\", \"kgw_device\": \"cpu\"},\n",
    "     \"cygu/llama-2-7b-logit-watermark-distill-kgw-k2-gamma0.25-delta2\":{\"type\": \"kgw\", \"k\": 2, \"gamma\": 0.25, \"delta\": 2.0, \"seeding_scheme\": \"simple_2\", \"kgw_device\": \"cpu\"},\n",
    "}\n",
    "watermark_config = watermark_configs[\"cygu/llama-2-7b-logit-watermark-distill-kgw-k0-gamma0.25-delta2\"]\n",
    "device = 'cpu'\n",
    "detector = WatermarkDetector(\n",
    "                            device=watermark_config.get(\"kgw_device\", 'cpu'),\n",
    "                            tokenizer=tokenizer,\n",
    "                            vocab=tokenizer.get_vocab().values(),\n",
    "                            gamma=watermark_config[\"gamma\"],\n",
    "                            seeding_scheme=watermark_config[\"seeding_scheme\"],\n",
    "                            normalizers=[],\n",
    "                        )\n",
    "\n",
    "example_text = [\"Neural networks are powerful tools.\", \"Machine learning models can learn from data.\"]\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenized_inputs = tokenizer(\n",
    "    example_text, \n",
    "    padding=True,  # Pad to the longest sequence in the batch\n",
    "    truncation=True,  # Truncate sequences to the model's maximum length\n",
    "    return_tensors=\"pt\"  # Return as PyTorch tensors\n",
    ")\n",
    "batch = {\n",
    "    \"input_ids\": tokenized_inputs[\"input_ids\"],  # Token IDs of the input sequences\n",
    "    \"attention_mask\": tokenized_inputs[\"attention_mask\"]  # Attention masks for padding\n",
    "}\n",
    "watermark = WatermarkLogitsProcessor(\n",
    "    vocab=tokenizer.get_vocab().values(),\n",
    "    gamma=watermark_config[\"gamma\"],\n",
    "    delta=watermark_config[\"delta\"],\n",
    "    seeding_scheme=watermark_config[\"seeding_scheme\"],\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "min_new_tokens = 50\n",
    "max_new_tokens = 50\n",
    "temperature = 1.0\n",
    "top_k = 0\n",
    "top_p = 1.0\n",
    "\n",
    "\n",
    "do_sample = True\n",
    "watermarked_text_embedding = hf_model.generate(\n",
    "                input_ids=batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "                do_sample=do_sample,\n",
    "                min_new_tokens=min_new_tokens,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "                top_k=top_k,\n",
    "                logits_processor=LogitsProcessorList([watermark]),\n",
    "            )\n",
    "watermarked_text = tokenizer.batch_decode(watermarked_text_embedding, skip_special_tokens=True)\n",
    "print(watermarked_text)\n",
    "\n",
    "\n",
    "score = detector.detect(watermarked_text[0])\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'watermarked_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m watermarked_content \u001b[38;5;241m=\u001b[39m \u001b[43mwatermarked_model\u001b[49m\u001b[38;5;241m.\u001b[39mgenerate(\n\u001b[1;32m      2\u001b[0m                 input_ids\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      3\u001b[0m                 attention_mask\u001b[38;5;241m=\u001b[39mbatch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m      4\u001b[0m                 do_sample\u001b[38;5;241m=\u001b[39mdo_sample,\n\u001b[1;32m      5\u001b[0m                 min_new_tokens\u001b[38;5;241m=\u001b[39mmin_new_tokens,\n\u001b[1;32m      6\u001b[0m                 max_new_tokens\u001b[38;5;241m=\u001b[39mmax_new_tokens,\n\u001b[1;32m      7\u001b[0m                 temperature\u001b[38;5;241m=\u001b[39mtemperature,\n\u001b[1;32m      8\u001b[0m                 top_p\u001b[38;5;241m=\u001b[39mtop_p,\n\u001b[1;32m      9\u001b[0m                 top_k\u001b[38;5;241m=\u001b[39mtop_k,\n\u001b[1;32m     10\u001b[0m             )\n\u001b[1;32m     11\u001b[0m watermarked_content_text \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbatch_decode(watermarked_content, skip_special_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(watermarked_content_text)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'watermarked_model' is not defined"
     ]
    }
   ],
   "source": [
    "watermarked_content = watermarked_model.generate(\n",
    "                input_ids=batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "                do_sample=do_sample,\n",
    "                min_new_tokens=min_new_tokens,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "                temperature=temperature,\n",
    "                top_p=top_p,\n",
    "                top_k=top_k,\n",
    "            )\n",
    "watermarked_content_text = tokenizer.batch_decode(watermarked_content, skip_special_tokens=True)\n",
    "print(watermarked_content_text)\n",
    "print(detector.detect(watermarked_content_text[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "watermarked_attn_pattern.shape=torch.Size([15, 15])\n",
      "vanilla_attn_pattern.shape=torch.Size([15, 15])\n"
     ]
    }
   ],
   "source": [
    "input_text = \"Neural networks can be fully understood, let's do it!\"\n",
    "input_ids = tokenizer(input_text, return_tensors='pt')['input_ids']\n",
    "tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "\n",
    "watermarked_logits, watermarked_cache = coefficient_model.run_with_cache(input_ids, return_type='logits')\n",
    "vanilla_logits, vanilla_cache = vanilla_model.run_with_cache(input_ids, return_type='logits')\n",
    "\n",
    "layer = 0\n",
    "watermarked_attn_pattern = watermarked_cache[f'blocks.{layer}.attn.hook_pattern'][0,0]\n",
    "vanilla_attn_pattern = vanilla_cache[f'blocks.{layer}.attn.hook_pattern'][0,0]\n",
    "\n",
    "print(f'{watermarked_attn_pattern.shape=}')\n",
    "print(f'{vanilla_attn_pattern.shape=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0's different in attention scores: 2.325881299355336%\n",
      "Layer 1's different in attention scores: 10.35998210573171%\n",
      "Layer 2's different in attention scores: 2.5030622625267434%\n",
      "Layer 3's different in attention scores: 1.2060115839534544%\n",
      "Layer 4's different in attention scores: 0.845424689434199%\n",
      "Layer 5's different in attention scores: 1.7578768362398227%\n",
      "Layer 6's different in attention scores: 1.2541395319952489%\n",
      "Layer 7's different in attention scores: 0.9313278358179716%\n",
      "Layer 8's different in attention scores: 0.2857732111809033%\n",
      "Layer 9's different in attention scores: 1.9607445812169995%\n",
      "Layer 10's different in attention scores: 1.21744999542731%\n",
      "Layer 11's different in attention scores: 4.063571248603139%\n",
      "Layer 12's different in attention scores: 3.298253836325621%\n",
      "Layer 13's different in attention scores: 4.935298594891546%\n",
      "Layer 14's different in attention scores: 6.684312045749305%\n",
      "Layer 15's different in attention scores: 1.572974357800097%\n",
      "Layer 16's different in attention scores: 3.168636868521957%\n",
      "Layer 17's different in attention scores: 1.0736683816199533%\n",
      "Layer 18's different in attention scores: 2.4149343108088788%\n",
      "Layer 19's different in attention scores: 2.139649169446374%\n"
     ]
    }
   ],
   "source": [
    "for layer in range(20):\n",
    "    watermarked_attn_pattern = watermarked_cache[f'blocks.{layer}.attn.hook_pattern'][0,0]\n",
    "    vanilla_attn_pattern = vanilla_cache[f'blocks.{layer}.attn.hook_pattern'][0,0]\n",
    "    # print(f'{watermarked_attn_pattern.shape=}')\n",
    "    # print(torch.isclose(watermarked_attn_pattern, vanilla_attn_pattern, atol=1e-6).all())\n",
    "    print(f\"Layer {layer}'s different in attention scores: {torch.norm(watermarked_attn_pattern - vanilla_attn_pattern).item() * 100 / torch.norm(vanilla_attn_pattern).item()}%\")\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly.express as px\n",
    "# from plotly.subplots import make_subplots\n",
    "# import plotly.graph_objects as go\n",
    "\n",
    "# for layer in range(20):\n",
    "# # layer = 0\n",
    "#     watermarked_attn_pattern = watermarked_cache[f'blocks.{layer}.attn.hook_pattern'][0,0]\n",
    "#     vanilla_attn_pattern = vanilla_cache[f'blocks.{layer}.attn.hook_pattern'][0,0]\n",
    "\n",
    "#     # Create a subplot with 1 row and 2 columns\n",
    "#     fig = make_subplots(rows=1, cols=2, subplot_titles=(\"Watermarked Attention Pattern\", \"Vanilla Attention Pattern\"))\n",
    "\n",
    "#     # Watermarked attention pattern plot\n",
    "#     watermarked_fig = px.imshow(\n",
    "#         watermarked_attn_pattern,\n",
    "#         labels=dict(x=\"Input Tokens\", y=\"Input Tokens\", color=\"Attention Score\"),\n",
    "#         x=tokens,\n",
    "#         y=tokens,\n",
    "#         color_continuous_scale='Viridis'\n",
    "#     )\n",
    "\n",
    "#     # Vanilla attention pattern plot\n",
    "#     vanilla_fig = px.imshow(\n",
    "#         vanilla_attn_pattern,\n",
    "#         labels=dict(x=\"Input Tokens\", y=\"Input Tokens\", color=\"Attention Score\"),\n",
    "#         x=tokens,\n",
    "#         y=tokens,\n",
    "#         color_continuous_scale='Viridis'\n",
    "#     )\n",
    "\n",
    "#     # Add watermarked attention pattern to the first subplot\n",
    "#     fig.add_trace(\n",
    "#         go.Heatmap(\n",
    "#             z=watermarked_attn_pattern,\n",
    "#             x=tokens,\n",
    "#             y=tokens,\n",
    "#             colorscale='Viridis',\n",
    "#             colorbar=dict(title=\"Attention Score\")\n",
    "#         ),\n",
    "#         row=1, col=1\n",
    "#     )\n",
    "\n",
    "#     # Add vanilla attention pattern to the second subplot\n",
    "#     fig.add_trace(\n",
    "#         go.Heatmap(\n",
    "#             z=vanilla_attn_pattern,\n",
    "#             x=tokens,\n",
    "#             y=tokens,\n",
    "#             colorscale='Viridis',\n",
    "#             showscale=False  # Disable the color scale on this one to avoid duplication\n",
    "#         ),\n",
    "#         row=1, col=2\n",
    "#     )\n",
    "\n",
    "#     # Update layout\n",
    "#     fig.update_layout(\n",
    "#         title=f\"Layer{layer}: Comparison of Attention Patterns: Watermarked vs. Vanilla\",\n",
    "#         xaxis=dict(tickangle=-45),\n",
    "#         autosize=False,  # Disable autosizing\n",
    "#         width=1600,      # Double the width for two side-by-side plots\n",
    "#         height=800,      # Same height\n",
    "#         margin=dict(l=100, r=100, b=150, t=100),  # Adjust margins\n",
    "#     )\n",
    "\n",
    "#     # Show the plot\n",
    "#     fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import plotly.express as px\n",
    "# for layer in range(20):\n",
    "#     watermarked_attn_pattern = watermarked_cache[f'blocks.{layer}.attn.hook_pattern'][0,0]\n",
    "#     vanilla_attn_pattern = vanilla_cache[f'blocks.{layer}.attn.hook_pattern'][0,0]\n",
    "#     difference_attn_pattern = watermarked_attn_pattern - vanilla_attn_pattern\n",
    "#     fig = px.imshow(\n",
    "#         difference_attn_pattern,\n",
    "#         labels=dict(x=\"Input Tokens\", y=\"Input Tokens\", color=\"Attention Score\"),\n",
    "#         x=tokens,\n",
    "#         y=tokens,\n",
    "#         color_continuous_scale='Viridis'\n",
    "#     )\n",
    "\n",
    "\n",
    "#     fig.update_layout(\n",
    "#         title=\"Attention Map for First Head\",\n",
    "#         xaxis=dict(tickangle=-45),\n",
    "#         autosize=False,  # Disable autosizing\n",
    "#         width=800,       # Set the width of the plot\n",
    "#         height=800,      # Set the height of the plot\n",
    "#         margin=dict(l=100, r=100, b=150, t=100),  # Adjust margins to make room for labels\n",
    "#     )\n",
    "\n",
    "#     fig.update_layout(title=f\"Layer{layer}'s Attention score difference\", xaxis=dict(tickangle=-45))\n",
    "\n",
    "#     # Show the plot\n",
    "#     fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Watermarked sentence in watermarked model, check about the n-gram saliency\n",
    "# unwatermarked sentence in watermarked model, check about the n-gram saliency\n",
    "# unwatermarked sentences and watermarked sentences in vanilla model to chekc the n-gram saliency difference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87472758ecb5467a923a81ece5e650e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 17\u001b[0m\n\u001b[1;32m     13\u001b[0m watermark_config \u001b[38;5;241m=\u001b[39m watermark_configs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcygu/llama-2-7b-logit-watermark-distill-kgw-k0-gamma0.25-delta2\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     14\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     16\u001b[0m watermark \u001b[38;5;241m=\u001b[39m WatermarkLogitsProcessor(\n\u001b[0;32m---> 17\u001b[0m     vocab\u001b[38;5;241m=\u001b[39m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241m.\u001b[39mget_vocab()\u001b[38;5;241m.\u001b[39mvalues(),\n\u001b[1;32m     18\u001b[0m     gamma\u001b[38;5;241m=\u001b[39mwatermark_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgamma\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     19\u001b[0m     delta\u001b[38;5;241m=\u001b[39mwatermark_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelta\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     20\u001b[0m     seeding_scheme\u001b[38;5;241m=\u001b[39mwatermark_config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mseeding_scheme\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     21\u001b[0m     device\u001b[38;5;241m=\u001b[39mdevice,\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     23\u001b[0m min_new_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[1;32m     24\u001b[0m max_new_tokens \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'watermarked_text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m detector \u001b[38;5;241m=\u001b[39m WatermarkDetector(\n\u001b[1;32m      2\u001b[0m                             device\u001b[38;5;241m=\u001b[39mwatermark_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkgw_device\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m),\n\u001b[1;32m      3\u001b[0m                             tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m                             normalizers\u001b[38;5;241m=\u001b[39m[],\n\u001b[1;32m      8\u001b[0m                         )\n\u001b[0;32m----> 9\u001b[0m score \u001b[38;5;241m=\u001b[39m detector\u001b[38;5;241m.\u001b[39mdetect(\u001b[43mwatermarked_text\u001b[49m[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(score)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'watermarked_text' is not defined"
     ]
    }
   ],
   "source": [
    "detector = WatermarkDetector(\n",
    "                            device=watermark_config.get(\"kgw_device\", 'cpu'),\n",
    "                            tokenizer=tokenizer,\n",
    "                            vocab=tokenizer.get_vocab().values(),\n",
    "                            gamma=watermark_config[\"gamma\"],\n",
    "                            seeding_scheme=watermark_config[\"seeding_scheme\"],\n",
    "                            normalizers=[],\n",
    "                        )\n",
    "score = detector.detect(watermarked_text[0])\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neuron Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a53bc8985351412eb6bf1c76132e8dc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "watermarked_model = AutoModelForCausalLM.from_pretrained(\"cygu/llama-2-7b-logit-watermark-distill-kgw-k0-gamma0.25-delta1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "watermarked_model.train()\n",
    "\n",
    "# Vanilla Text\n",
    "input_text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "print(detector.detect(input_text))\n",
    "inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "outputs = watermarked_model(**inputs, labels=inputs[\"input_ids\"])\n",
    "loss = outputs.loss\n",
    "loss.backward()\n",
    "\n",
    "for name, param in watermarked_model.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        # print(f\"Gradient for {name}:\")\n",
    "        # print(param.grad)  # This is where the gradient is stored\n",
    "        break\n",
    "    else:\n",
    "        print(f\"No gradient for {name} (probably a non-learnable parameter).\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "def topk_elements_by_dot_product(model, k):\n",
    "    # Store the element-wise products and their indices across all parameters\n",
    "    elementwise_products = []\n",
    "    indices = []\n",
    "    param_names = []\n",
    "    \n",
    "    input_text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "    # print(detector.detect(input_text))\n",
    "    inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "\n",
    "\n",
    "    # Iterate over model parameters and their gradients\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.grad is not None:\n",
    "            product = param * param.grad\n",
    "            flattened_product = product.view(-1)\n",
    "            elementwise_products.append(flattened_product)\n",
    "            indices.append(torch.arange(flattened_product.numel())) \n",
    "            param_names.append([name] * flattened_product.numel())  \n",
    "\n",
    "    # Concatenate all the element-wise products and indices into one tensor\n",
    "    elementwise_products = torch.cat(elementwise_products)\n",
    "    indices = torch.cat(indices)\n",
    "    param_names = [item for sublist in param_names for item in sublist]  # Flatten param_names list\n",
    "\n",
    "    # Get the top K element-wise products and their corresponding indices\n",
    "    topk_values, topk_indices = torch.topk(elementwise_products, k)\n",
    "\n",
    "    # Map top K indices back to the original parameter names and indices\n",
    "    topk_param_names = [param_names[i] for i in topk_indices]\n",
    "    topk_param_indices = [indices[i].item() for i in topk_indices]  # Convert indices to integers\n",
    "\n",
    "    return topk_values, topk_param_names, topk_param_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5  # Specify how many top elements you want\n",
    "topk_values, topk_param_names, topk_param_indices = topk_elements_by_dot_product(watermarked_model, k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Top {k} elements by dot product:{topk_param_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the Spearman's Rank Correlation Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "291\n"
     ]
    }
   ],
   "source": [
    "# Check whether those take reacted strong to vanilla text neurons are within the watermark vector.\n",
    "\n",
    "params_list = list(watermarked_model.named_parameters())\n",
    "print(len(params_list))\n",
    "\n",
    "dot_product = dict()\n",
    "for name, param in watermarked_model.named_parameters():\n",
    "    if param.grad is not None:\n",
    "        # print(f\"Gradient for {name}:\")\n",
    "        # print(param.grad.shape)  # This is where the gradient is stored\n",
    "        dot_product[name] = param.grad * param\n",
    "        assert dot_product[name].shape == param.shape\n",
    "    else:\n",
    "        print(f\"No gradient for {name} (probably a non-learnable parameter).\")\n",
    "        break\n",
    "\n",
    "\n",
    "from scipy.stats import stats\n",
    "spearman_corr, _ =  stats.spearmanr(task_vector.vector['model.embed_tokens.weight'].cpu().numpy().flatten(), dot_product['model.embed_tokens.weight'].cpu().numpy().flatten())\n",
    "print(f'{spearman_corr=}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "watermark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
