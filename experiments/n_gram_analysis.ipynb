{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import argparse\n",
    "import os\n",
    "import copy\n",
    "os.environ['http_proxy'] = \"http://10.176.52.116:7890\"\n",
    "os.environ['https_proxy'] = \"http://10.176.52.116:7890\"\n",
    "os.environ['all_proxy'] = \"socks5://10.176.52.116:7891\"\n",
    "import sys\n",
    "sys.path.append(('../'))\n",
    "sys.path.append(('../../'))\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "import json\n",
    "from typing import Dict\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n",
    "from task_vector import TaskVector\n",
    "\n",
    "\n",
    "from watermarks.kgw.watermark_processor import WatermarkDetector\n",
    "from watermarks.aar.aar_watermark import AarWatermarkDetector\n",
    "from watermarks.watermark_types import WatermarkType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2500 [00:00<?, ?it/s]2024-08-21 22:10:56.661806: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-21 22:10:58.009216: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-21 22:10:58.430562: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-21 22:10:58.619330: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-21 22:10:59.679387: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-21 22:11:05.273013: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "  0%|          | 1/2500 [00:18<13:08:50, 18.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are in total 2350 unique unigrams and 4914 unique bigrams.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Top 10 most common unigrams:\n",
      "the: 299\n",
      "to: 151\n",
      "a: 125\n",
      "in: 114\n",
      "and: 113\n",
      "of: 109\n",
      "that: 71\n",
      "was: 58\n",
      "his: 47\n",
      "for: 46\n",
      "Top 10 most common bigrams:\n",
      "of the: 30\n",
      "in the: 24\n",
      "at the: 16\n",
      "to the: 15\n",
      "that the: 10\n",
      "on the: 10\n",
      "that he: 9\n",
      "with the: 9\n",
      "the American: 9\n",
      "for the: 8\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from itertools import chain\n",
    "from collections import defaultdict, Counter\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Parameters based on your provided setup\n",
    "model_name_or_path = \"mistralai/Mistral-7B-v0.1\"\n",
    "dataset_name = \"Skylion007/openwebtext\"\n",
    "block_size = 256\n",
    "per_device_train_batch_size = 8\n",
    "gradient_accumulation_steps = 4\n",
    "\n",
    "# Load the tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "\n",
    "# Load the dataset in streaming mode\n",
    "dataset = load_dataset(dataset_name, split='train', streaming=True)\n",
    "\n",
    "# Function to tokenize the text\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"])\n",
    "\n",
    "# Tokenize the dataset\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Function to decode and calculate n-grams\n",
    "def calculate_ngrams(sequence, n):\n",
    "    ngrams = zip(*[sequence[i:] for i in range(n)])\n",
    "    return [\" \".join(ngram) for ngram in ngrams]\n",
    "\n",
    "# Initialize a dictionary to store all n-grams\n",
    "unigram_counts = defaultdict(int)\n",
    "bigram_counts = defaultdict(int)\n",
    "\n",
    "# Function to group texts into blocks of block_size\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts together\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    \n",
    "    # Drop the remainder and split into blocks of block_size\n",
    "    total_length = (total_length // block_size) * block_size\n",
    "    result = {\n",
    "        k: [t[i:i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "\n",
    "# Apply the grouping function to the tokenized dataset\n",
    "grouped_dataset = tokenized_dataset.map(group_texts, batched=True)\n",
    "\n",
    "# Function to create batches\n",
    "def create_batches(dataset, batch_size, gradient_accumulation_steps):\n",
    "    batch = []\n",
    "    for example in dataset:\n",
    "        batch.append(example)\n",
    "        if len(batch) == batch_size * gradient_accumulation_steps:\n",
    "            yield batch\n",
    "            batch = []\n",
    "\n",
    "# Create batches\n",
    "batches = create_batches(grouped_dataset, per_device_train_batch_size, gradient_accumulation_steps)\n",
    "# print(f\"Number of batches: {len(list(batches))}\")\n",
    "\n",
    "steps = 2500\n",
    "# Iterate over the batches and calculate n-grams\n",
    "progressbar = tqdm(range(steps))\n",
    "example_number = 0\n",
    "for batch in batches:\n",
    "    for example in batch:\n",
    "        example_number += 1\n",
    "        decoded_text = tokenizer.decode(example[\"input_ids\"], skip_special_tokens=True)\n",
    "        unigrams = calculate_ngrams(decoded_text.split(), 1)\n",
    "        for unigram in unigrams:\n",
    "            unigram_counts[unigram] += 1\n",
    "        bigrams = calculate_ngrams(decoded_text.split(), 2)\n",
    "        for bigram in bigrams:\n",
    "            bigram_counts[bigram] += 1\n",
    "    progressbar.update(1)\n",
    "    steps -= 1\n",
    "    break\n",
    "    if steps == 0:\n",
    "        break\n",
    "\n",
    "# If you want to convert the defaultdict to a regular dictionary\n",
    "uningram_counts = dict(unigram_counts)\n",
    "bigram_counts = dict(bigram_counts)\n",
    "# Optionally, print out the top n-grams\n",
    "print(f\"There are in total {len(unigram_counts)} unique unigrams and {len(bigram_counts)} unique bigrams.\")\n",
    "print('-'*100)\n",
    "\n",
    "\n",
    "print(\"Top 10 most common unigrams:\")\n",
    "for ngram, count in Counter(unigram_counts).most_common(10):\n",
    "    print(f\"{ngram}: {count}\")\n",
    "\n",
    "print(\"Top 10 most common bigrams:\")\n",
    "for ngram, count in Counter(bigram_counts).most_common(10):\n",
    "    print(f\"{ngram}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38dff5ff9f654bb4b1ec679bed21af56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "teacher_model_path = \"meta-llama/Llama-2-7b-hf\"\n",
    "teacher_model = AutoModelForCausalLM.from_pretrained(teacher_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from watermarks.aar.aar_watermark import AarWatermark\n",
    "from watermarks.kgw.kgw_watermark import KGWWatermark\n",
    "from watermarks.kth.kth_watermark import KTHWatermark\n",
    "from watermarks.watermark_types import WatermarkType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'watermarks' (namespace) from ['/remote-home/miintern1/watermark-learnability/experiments/../watermarks', '/remote-home/miintern1/watermark-learnability/watermarks']>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import importlib\n",
    "import watermarks.kgw.kgw_watermark\n",
    "from watermarks.kgw.kgw_watermark import KGWWatermark\n",
    "importlib.reload(watermarks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(teacher_model_path)\n",
    "watermark_configs = {\n",
    "     \"cygu/llama-2-7b-logit-watermark-distill-kgw-k0-gamma0.25-delta1\":{\"type\": \"kgw\", \"k\": 0, \"gamma\": 0.25, \"delta\": 1.0, \"seeding_scheme\": \"simple_0\", \"kgw_device\": \"cpu\"},\n",
    "     \"cygu/llama-2-7b-logit-watermark-distill-kgw-k0-gamma0.25-delta2\":{\"type\": \"kgw\", \"k\": 0, \"gamma\": 0.25, \"delta\": 2.0, \"seeding_scheme\": \"simple_0\", \"kgw_device\": \"cpu\"},\n",
    "     \"cygu/llama-2-7b-logit-watermark-distill-kgw-k1-gamma0.25-delta1\":{\"type\": \"kgw\", \"k\": 1, \"gamma\": 0.25, \"delta\": 1.0, \"seeding_scheme\": \"simple_1\", \"kgw_device\": \"cpu\"},\n",
    "     \"cygu/llama-2-7b-logit-watermark-distill-kgw-k1-gamma0.25-delta2\":{\"type\": \"kgw\", \"k\": 1, \"gamma\": 0.25, \"delta\": 2.0, \"seeding_scheme\": \"simple_1\", \"kgw_device\": \"cpu\"},\n",
    "     \"cygu/llama-2-7b-logit-watermark-distill-kgw-k2-gamma0.25-delta2\":{\"type\": \"kgw\", \"k\": 2, \"gamma\": 0.25, \"delta\": 2.0, \"seeding_scheme\": \"simple_2\", \"kgw_device\": \"cpu\"},\n",
    "}\n",
    "watermark_config = watermark_configs[\"cygu/llama-2-7b-logit-watermark-distill-kgw-k1-gamma0.25-delta1\"]\n",
    "\n",
    "watermarker = KGWWatermark(\n",
    "            vocab=tokenizer.get_vocab().values(),\n",
    "            gamma= watermark_config[\"gamma\"],\n",
    "            delta=watermark_config[\"delta\"],\n",
    "            seeding_scheme=watermark_config[\"seeding_scheme\"],\n",
    "            tokenizer=tokenizer,\n",
    "            device=watermark_config[\"kgw_device\"],\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    1, 24948,   592,   920, 29915, 29879,   596,  2462,  2675, 29973,\n",
      "           306,  4966,   366,   526,  2599,  1532, 29889,   306,   626,  2599,\n",
      "          1532,  2086, 29889]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])}\n",
      "torch.Size([1, 23, 32000])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "text = \"Tell me how's your day going? I hope you are doing well. I am doing well too.\"\n",
    "inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "print(inputs)\n",
    "teacher_outputs = teacher_model(**inputs)\n",
    "# print(teacher_outputs)\n",
    "print(teacher_outputs.logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 23, 32000])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 23, 32000])\n",
      "tensor(20)\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 20 is out of bounds for dimension 0 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m logits \u001b[38;5;241m=\u001b[39m teacher_outputs\u001b[38;5;241m.\u001b[39mlogits\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(logits\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m----> 4\u001b[0m watermark_logits  \u001b[38;5;241m=\u001b[39m \u001b[43mwatermarker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwatermark_logits\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogits\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(watermark_logits\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(torch\u001b[38;5;241m.\u001b[39mall((watermark_logits \u001b[38;5;241m-\u001b[39m logits)\u001b[38;5;241m==\u001b[39m\u001b[38;5;241m0\u001b[39m))\n",
      "File \u001b[0;32m~/watermark-learnability/experiments/../watermarks/kgw/kgw_watermark.py:76\u001b[0m, in \u001b[0;36mKGWWatermark.watermark_logits\u001b[0;34m(self, input_ids, logits)\u001b[0m\n\u001b[1;32m     74\u001b[0m true_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnonzero(mask[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m], as_tuple\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m     75\u001b[0m \u001b[38;5;28mprint\u001b[39m(true_indices[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 76\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mlogits\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtrue_indices\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[1;32m     77\u001b[0m logits[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:, :mask\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]][mask] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelta\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28mprint\u001b[39m(logits[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mk \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m:, :mask\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]][true_indices[\u001b[38;5;241m0\u001b[39m]])\n",
      "\u001b[0;31mIndexError\u001b[0m: index 20 is out of bounds for dimension 0 with size 1"
     ]
    }
   ],
   "source": [
    "input_ids = inputs[\"input_ids\"]\n",
    "logits = teacher_outputs.logits\n",
    "print(logits.shape)\n",
    "watermark_logits  = watermarker.watermark_logits(input_ids, logits)\n",
    "print(watermark_logits.shape)\n",
    "print(torch.all((watermark_logits - logits)==0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 23])\n",
      "torch.Size([1, 23, 32000])\n",
      "tensor([False, False, False,  ..., False,  True, False])\n",
      "tensor(20)\n"
     ]
    }
   ],
   "source": [
    "hashes = torch.sum(input_ids.unfold(-1, watermarker.k, 1), dim=-1)  # (batch, seq_len - k + 1)\n",
    "print(hashes.shape)\n",
    "mask = watermarker.greenlist_masks[hashes]\n",
    "print(mask.shape)\n",
    "# logits[..., watermarker.k-1:, :mask.shape[-1]][mask]\n",
    "print(mask[0][-1])\n",
    "true_indices = torch.nonzero(mask[0][-1], as_tuple=False).squeeze()\n",
    "print(true_indices[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([736000]), torch.Size([1, 23, 32000]))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = mask.view(-1)\n",
    "test.shape, mask.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
