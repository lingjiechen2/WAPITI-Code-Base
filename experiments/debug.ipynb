{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import copy\n",
    "os.environ['http_proxy'] = \"http://10.176.52.116:7890\"\n",
    "os.environ['https_proxy'] = \"http://10.176.52.116:7890\"\n",
    "os.environ['all_proxy'] = \"socks5://10.176.52.116:7891\"\n",
    "import sys\n",
    "# sys.path.append(('../'))\n",
    "# sys.path.append(('../../'))\n",
    "sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "import json\n",
    "from typing import Dict\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n",
    "from task_vector import TaskVector\n",
    "from itertools import chain\n",
    "from watermarks.kgw.watermark_processor import WatermarkDetector\n",
    "from watermarks.aar.aar_watermark import AarWatermarkDetector\n",
    "from watermarks.watermark_types import WatermarkType\n",
    "import logging\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "['kgw-k0-gamma0.25-delta1', 'kgw-k0-gamma0.25-delta2', 'kgw-k1-gamma0.25-delta2', 'kgw-k1-gamma0.25-delta1']\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "classified_dict = defaultdict(list)\n",
    "for path in MLP_model_list:\n",
    "    classified_dict[path[83:106]].append(path)\n",
    "print(len(classified_dict['kgw-k1-gamma0.25-delta1']))\n",
    "print(list(classified_dict.keys())) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.003692150115966797,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading data files",
       "rate": null,
       "total": 21,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2f27d448613345f693e386f82f66dd90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/21 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004203081130981445,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading data",
       "rate": null,
       "total": 625172480,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9808c8b99cb44ebd988e24941228fec8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/625M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0043680667877197266,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading data",
       "rate": null,
       "total": 625264640,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbceb03448474033afcde345b8bb880b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/625M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0038089752197265625,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading data",
       "rate": null,
       "total": 624445440,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02fb5eac57dc476da1159e508fec58f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/624M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.003532886505126953,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading data",
       "rate": null,
       "total": 628961280,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6591e5d0e5524030a375def41fdc3c54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/629M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0039637088775634766,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading data",
       "rate": null,
       "total": 626708480,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f01550f499fe4410b8a9004841b72af3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/627M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.003682374954223633,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading data",
       "rate": null,
       "total": 620666880,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "241e4d0d3210434fb776a972e09f4911",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/621M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.003450632095336914,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading data",
       "rate": null,
       "total": 618752000,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "676f250d4b364430b52d5754b2b8e76f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/619M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.006064176559448242,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading data",
       "rate": null,
       "total": 619141120,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6d2cdb0f604c4656872e885008bf88e3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/619M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0035076141357421875,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading data",
       "rate": null,
       "total": 617789440,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c9c86b4b546445585e37547a9f16347",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/618M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.005236148834228516,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading data",
       "rate": null,
       "total": 619192320,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d18b175ef6f49d9b3217b0106aaeb46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/619M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0036163330078125,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading data",
       "rate": null,
       "total": 376709120,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15ac606900684a9abc18b2b030280067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/377M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.008778572082519531,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Generating train split",
       "rate": null,
       "total": 8013769,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9495e8e5af3745edb994f076e86aa714",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/8013769 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "import copy\n",
    "os.environ['http_proxy'] = \"http://10.176.52.116:7890\"\n",
    "os.environ['https_proxy'] = \"http://10.176.52.116:7890\"\n",
    "os.environ['all_proxy'] = \"socks5://10.176.52.116:7891\"\n",
    "# Load the first 1% of the OpenWebText dataset\n",
    "dataset = load_dataset(\"openwebtext\", split='train[:1%]')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.003726482391357422,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Saving the dataset (0/1 shards)",
       "rate": null,
       "total": 80138,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "608af2cf757245679f205c6521656ba8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/80138 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "save_path = \"/remote-home/share/data/openwebtext\"  # Specify the path where you want to save the dataset\n",
    "dataset.save_to_disk(save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "import copy\n",
    "os.environ['http_proxy'] = \"http://10.176.52.116:7890\"\n",
    "os.environ['https_proxy'] = \"http://10.176.52.116:7890\"\n",
    "os.environ['all_proxy'] = \"socks5://10.176.52.116:7891\"\n",
    "from torch.optim import AdamW\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, get_cosine_schedule_with_warmup,Cache\n",
    "from datasets import load_dataset, Dataset, load_from_disk\n",
    "from torch.utils.data import DataLoader\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "sys.path.append(('../'))\n",
    "sys.path.append(('../../'))\n",
    "import logging\n",
    "from task_vector import TaskVector\n",
    "import logging\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"/remote-home/share/data/openwebtext\"\n",
    "dataset = load_from_disk(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tested_model_name = \"RohitSahoo/llama-2-7b-chat-hf-math-ft-V1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(tested_model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.003949403762817383,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8e97c23712a48928012a58e0412d7dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained(\"RohitSahoo/llama-2-7b-chat-hf-math-ft-V1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5, betas=(0.9, 0.999))\n",
    "total_steps = int(2500 * (64/batch_size))\n",
    "warmup_steps = int(0.1 * total_steps)  # 10% of total steps for warmup\n",
    "\n",
    "scheduler = get_cosine_schedule_with_warmup(optimizer, num_warmup_steps=warmup_steps, num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004152536392211914,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Map",
       "rate": null,
       "total": 80138,
       "unit": " examples",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "121b530340ba4dddba86b8415b7a04ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/80138 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size = 4\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding=\"max_length\", truncation=True, max_length=512)\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "dataloader = DataLoader(tokenized_datasets, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1],\n",
       "        [1, 1, 1,  ..., 1, 1, 1]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch = next(iter(dataloader))\n",
    "batch['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:1.9419769048690796\n",
      "1:4.125180721282959\n",
      "2:2.830976724624634\n",
      "3:2.038374662399292\n",
      "4:5.609251976013184\n",
      "5:2.373664140701294\n",
      "6:4.295639991760254\n",
      "7:4.111433506011963\n",
      "8:2.560845375061035\n",
      "9:3.267369508743286\n",
      "10:2.0757975578308105\n",
      "11:2.07499361038208\n",
      "12:2.7399561405181885\n",
      "13:1.9650331735610962\n",
      "14:1.9647235870361328\n",
      "15:3.525581121444702\n",
      "16:4.107758045196533\n",
      "17:2.586552858352661\n",
      "18:1.6376299858093262\n",
      "19:2.5431292057037354\n",
      "20:2.0452449321746826\n",
      "21:3.902618408203125\n",
      "22:2.7822561264038086\n",
      "23:2.009279489517212\n",
      "24:2.370344877243042\n",
      "25:2.616163969039917\n",
      "26:3.718142032623291\n",
      "27:1.7252116203308105\n",
      "28:2.0578060150146484\n",
      "29:3.540306806564331\n",
      "30:3.650243043899536\n",
      "31:1.9725979566574097\n",
      "32:1.9312020540237427\n",
      "33:2.0513861179351807\n",
      "34:2.186934471130371\n",
      "35:2.178136110305786\n",
      "36:3.2234275341033936\n",
      "37:2.2616355419158936\n",
      "38:2.911768674850464\n",
      "39:3.1385769844055176\n",
      "40:3.766120195388794\n",
      "41:3.315037488937378\n",
      "42:2.5913455486297607\n",
      "43:2.536370038986206\n",
      "44:2.7551724910736084\n",
      "45:2.076582908630371\n",
      "46:2.4357078075408936\n",
      "47:2.1708343029022217\n",
      "48:2.2990541458129883\n",
      "49:2.647627592086792\n",
      "50:2.644127368927002\n",
      "51:2.2426817417144775\n",
      "52:1.950240135192871\n",
      "53:2.830397129058838\n",
      "54:2.1993532180786133\n",
      "55:2.3026626110076904\n",
      "56:2.2125837802886963\n",
      "57:2.112586498260498\n",
      "58:2.0724198818206787\n",
      "59:1.724618673324585\n",
      "60:2.012988567352295\n",
      "61:1.7629082202911377\n",
      "62:1.697854995727539\n",
      "63:1.665751338005066\n",
      "64:1.868345022201538\n",
      "65:2.161724328994751\n",
      "66:2.2623400688171387\n",
      "67:2.00453782081604\n",
      "68:2.1744508743286133\n",
      "69:1.9368456602096558\n"
     ]
    }
   ],
   "source": [
    "device = 'cpu'\n",
    "index = 0\n",
    "for batch in dataloader:\n",
    "    input_ids = batch['input_ids'].to(device)\n",
    "    attention_mask = batch['attention_mask'].to(device)\n",
    "    # get_gpu_info()\n",
    "    (f\"{input_ids.shape}, {attention_mask.shape}\")\n",
    "    optimizer.zero_grad()\n",
    "    # outputs = model(**inputs, labels=inputs['input_ids'])\n",
    "    # print(batch.keys())\n",
    "    # cache = Cache(model, batch['input_ids'], batch['attention_mask'])\n",
    "    outputs = model(input_ids = input_ids, attention_mask = attention_mask, labels=input_ids)#, cache=cache)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "    print(f\"{index}:{loss.item()}\")\n",
    "    index += 1\n",
    "    # break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['kgw-k1-gamma0.25-delta1', 'kgw-k0-gamma0.25-delta1', 'kgw-k0-gamma0.25-delta2', 'kgw-k1-gamma0.25-delta2']\n"
     ]
    }
   ],
   "source": [
    "watermark_types = list(set([watermark_name[83:106] for watermark_name in MLP_model_list]))\n",
    "print(watermark_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/remote-home/miintern1/watermark-learnability/data/model_weights_2/model.layers.31_kgw-k0-gamma0.25-delta1_hidden_dim_16384_num_layers_2_best_model.pt',\n",
       " '/remote-home/miintern1/watermark-learnability/data/model_weights_2/model.layers.31_kgw-k0-gamma0.25-delta2_hidden_dim_8192_num_layers_2_best_model.pt',\n",
       " '/remote-home/miintern1/watermark-learnability/data/model_weights_2/model.layers.31_kgw-k1-gamma0.25-delta2_hidden_dim_16384_num_layers_3_best_model.pt',\n",
       " '/remote-home/miintern1/watermark-learnability/data/model_weights_2/model.layers.31_kgw-k0-gamma0.25-delta1_hidden_dim_8192_num_layers_2_best_model.pt',\n",
       " '/remote-home/miintern1/watermark-learnability/data/model_weights_2/model.layers.31_kgw-k0-gamma0.25-delta1_hidden_dim_8192_num_layers_3_best_model.pt',\n",
       " '/remote-home/miintern1/watermark-learnability/data/model_weights_2/model.layers.31_kgw-k0-gamma0.25-delta1_hidden_dim_16384_num_layers_1_best_model.pt',\n",
       " '/remote-home/miintern1/watermark-learnability/data/model_weights_2/model.layers.31_kgw-k1-gamma0.25-delta2_hidden_dim_8192_num_layers_1_best_model.pt',\n",
       " '/remote-home/miintern1/watermark-learnability/data/model_weights_2/model.layers.31_kgw-k0-gamma0.25-delta1_hidden_dim_8192_num_layers_1_best_model.pt',\n",
       " '/remote-home/miintern1/watermark-learnability/data/model_weights_2/model.layers.31_kgw-k0-gamma0.25-delta2_hidden_dim_16384_num_layers_1_best_model.pt',\n",
       " '/remote-home/miintern1/watermark-learnability/data/model_weights_2/model.layers.31_kgw-k1-gamma0.25-delta2_hidden_dim_16384_num_layers_1_best_model.pt',\n",
       " '/remote-home/miintern1/watermark-learnability/data/model_weights_2/model.layers.31_kgw-k1-gamma0.25-delta2_hidden_dim_8192_num_layers_3_best_model.pt',\n",
       " '/remote-home/miintern1/watermark-learnability/data/model_weights_2/model.layers.31_kgw-k1-gamma0.25-delta2_hidden_dim_8192_num_layers_2_best_model.pt',\n",
       " '/remote-home/miintern1/watermark-learnability/data/model_weights_2/model.layers.31_kgw-k1-gamma0.25-delta2_hidden_dim_16384_num_layers_2_best_model.pt',\n",
       " '/remote-home/miintern1/watermark-learnability/data/model_weights_2/model.layers.31_kgw-k1-gamma0.25-delta1_hidden_dim_8192_num_layers_1_best_model.pt',\n",
       " '/remote-home/miintern1/watermark-learnability/data/model_weights_2/model.layers.31_kgw-k1-gamma0.25-delta1_hidden_dim_16384_num_layers_3_best_model.pt',\n",
       " '/remote-home/miintern1/watermark-learnability/data/model_weights_2/model.layers.31_kgw-k1-gamma0.25-delta1_hidden_dim_8192_num_layers_2_best_model.pt',\n",
       " '/remote-home/miintern1/watermark-learnability/data/model_weights_2/model.layers.31_kgw-k1-gamma0.25-delta1_hidden_dim_16384_num_layers_1_best_model.pt',\n",
       " '/remote-home/miintern1/watermark-learnability/data/model_weights_2/model.layers.31_kgw-k0-gamma0.25-delta1_hidden_dim_16384_num_layers_3_best_model.pt',\n",
       " '/remote-home/miintern1/watermark-learnability/data/model_weights_2/model.layers.31_kgw-k0-gamma0.25-delta2_hidden_dim_8192_num_layers_1_best_model.pt',\n",
       " '/remote-home/miintern1/watermark-learnability/data/model_weights_2/model.layers.31_kgw-k0-gamma0.25-delta2_hidden_dim_16384_num_layers_2_best_model.pt',\n",
       " '/remote-home/miintern1/watermark-learnability/data/model_weights_2/model.layers.31_kgw-k1-gamma0.25-delta1_hidden_dim_16384_num_layers_2_best_model.pt',\n",
       " '/remote-home/miintern1/watermark-learnability/data/model_weights_2/model.layers.31_kgw-k0-gamma0.25-delta2_hidden_dim_8192_num_layers_3_best_model.pt',\n",
       " '/remote-home/miintern1/watermark-learnability/data/model_weights_2/model.layers.31_kgw-k1-gamma0.25-delta1_hidden_dim_8192_num_layers_3_best_model.pt',\n",
       " '/remote-home/miintern1/watermark-learnability/data/model_weights_2/model.layers.31_kgw-k0-gamma0.25-delta2_hidden_dim_16384_num_layers_3_best_model.pt']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'hidden_dim_8192_num_layers_1'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 'model.layers.31_kgw-k1-gamma0.25-delta1_hidden_dim_8192_num_layers_3_best_model'\n",
    "b = 'hidden_dim_8192_num_layers_3'\n",
    "p = '/remote-home/miintern1/watermark-learnability/data/model_weights_2/model.layers.31_kgw-k1-gamma0.25-delta1_hidden_dim_8192_num_layers_1_best_model.pt'\n",
    "p[-42:-14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['vanilla_model', 'kgw-k0-gamma0.25-delta1', 'kgw-k0-gamma0.25-delta2', 'kgw-k1-gamma0.25-delta2', 'kgw-k1-gamma0.25-delta1'])\n",
      "dict_keys(['watermarked_model', 'hidden_dim_8192_num_layers_2', 'idden_dim_16384_num_layers_1', 'hidden_dim_8192_num_layers_1', 'idden_dim_16384_num_layers_2', 'hidden_dim_8192_num_layers_3', 'idden_dim_16384_num_layers_3'])\n",
      "dict_keys(['full_output', 'truncate_prompt_output', 'watermark_scores', 'median_seq_rep_3', 'mean_seq_rep_3', 'list_seq_rep_3', 'total_rep_3'])\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('/remote-home/miintern1/watermark-learnability/data/c4/simulation_generation.json') as f:\n",
    "    simulation_generation = json.load(f)\n",
    "    print(simulation_generation.keys())\n",
    "print(simulation_generation['kgw-k0-gamma0.25-delta2'].keys())\n",
    "print(simulation_generation['vanilla_model'].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(simulation_generation['kgw-k0-gamma0.25-delta2']['watermarked_model']['truncate_prompt_output'])\n",
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.795166233285417, 0.08362477687173493, 0.14235496852585539, 0.002617811328063631, 0.045622888154362884, 0.0007172067688006159, 0.023103250395920184, 2.39395138341366e-05, 8.026747830016304e-05, 0.09830732478526512, 0.004731966961401138, 0.0007172067688006159, 6.607832459974615e-06, 0.5873332091757439, 0.33187088109041213, 0.14235496852585539, 0.4562406987109025, 0.045622888154362884, 0.045622888154362884, 0.01085572810624082, 0.08362477687173493, 0.33187088109041213, 0.22526818027284895, 0.045622888154362884, 0.33187088109041213, 0.0007172067688006159, 0.33187088109041213, 0.4562406987109025, 0.004731966961401138, 0.055123360333705934, 0.0019130927836177925, 0.045622888154362884, 6.607832459974615e-06, 0.023103250395920184, 0.0007172067688006159, 0.004731966961401138, 0.2519188554772399, 0.9929537746788165, 0.0019130927836177925, 0.22526818027284895, 0.4562406987109025, 2.39395138341366e-05, 0.01085572810624082, 0.023103250395920184, 0.045622888154362884, 0.004731966961401138, 0.01085572810624082, 2.39395138341366e-05, 0.14235496852585539, 0.01085572810624082, 0.4562406987109025, 0.045622888154362884, 0.055123360333705934, 0.004731966961401138, 0.0007172067688006159, 0.14235496852585539, 0.14235496852585539, 0.08362477687173493, 0.08362477687173493, 0.22526818027284895, 0.08362477687173493, 0.36296333549553494, 0.023103250395920184, 0.004731966961401138, 8.026747830016304e-05, 0.8166124177620249, 0.023103250395920184, 0.023103250395920184, 0.08362477687173493, 0.01085572810624082, 0.08362477687173493, 0.14235496852585539, 0.000366240220973836, 0.00024925137169824236, 0.023103250395920184, 0.7115261138266462, 0.08362477687173493, 0.14235496852585539, 0.004731966961401138, 0.2519188554772399, 0.08362477687173493, 2.39395138341366e-05, 0.6183814353384695, 0.0010161782725049107, 8.026747830016304e-05, 6.607832459974615e-06, 0.00024925137169824236, 0.14235496852585539, 0.01085572810624082, 0.08362477687173493, 0.023103250395920184, 8.026747830016304e-05, 0.023103250395920184, 0.00012251345164968294, 0.0019130927836177925, 0.4562406987109025, 0.023103250395920184, 8.026747830016304e-05, 0.22526818027284895, 0.08362477687173493, 0.4562406987109025, 0.33187088109041213, 0.14235496852585539, 0.004731966961401138, 0.023103250395920184, 0.000366240220973836, 0.33187088109041213, 0.22526818027284895, 0.33187088109041213, 0.045622888154362884, 0.01391760867866067, 0.0019130927836177925, 2.39395138341366e-05, 0.4562406987109025, 0.023103250395920184, 0.33187088109041213, 0.004731966961401138, 0.0007172067688006159, 0.004731966961401138, 0.22526818027284895, 0.045622888154362884, 0.0007172067688006159, 0.0007172067688006159, 0.01391760867866067, 0.08362477687173493, 0.023103250395920184, 0.33187088109041213, 8.026747830016304e-05, 0.0019130927836177925, 0.045622888154362884, 0.22526818027284895, 0.22526818027284895, 0.33187088109041213, 0.045622888154362884, 0.5873332091757439, 6.607832459974615e-06, 0.01085572810624082, 0.36296333549553494, 0.00024925137169824236, 0.01085572810624082, 0.895427145713559, 0.00024925137169824236, 0.08362477687173493, 8.026747830016304e-05, 0.22526818027284895, 0.00024925137169824236, 0.004731966961401138, 0.8363160997499084, 8.644873061013288e-08, 0.045622888154362884, 1.7311108806323842e-08, 0.045622888154362884, 0.08362477687173493, 0.0007172067688006159, 0.0019130927836177925, 0.0007172067688006159, 0.33187088109041213, 0.14235496852585539, 0.045622888154362884, 0.5873332091757439, 0.09830732478526512, 0.0019130927836177925, 0.004731966961401138, 0.14235496852585539, 0.2519188554772399, 0.01085572810624082, 0.14235496852585539, 0.0019130927836177925, 0.2519188554772399, 0.045622888154362884, 0.023103250395920184, 0.7115261138266462, 0.045622888154362884, 0.22526818027284895, 0.006262907247611063, 0.4562406987109025, 0.004731966961401138, 0.08362477687173493, 6.607832459974615e-06, 0.004731966961401138, 0.14235496852585539, 0.6093249917030334, 2.39395138341366e-05, 0.045622888154362884, 0.002617811328063631, 0.023103250395920184, 0.14235496852585539, 0.5873332091757439, 0.08362477687173493, 0.5873332091757439, 0.0007172067688006159, 0.045622888154362884, 0.004731966961401138, 0.22526818027284895, 0.023103250395920184, 2.39395138341366e-05, 0.01085572810624082, 0.14235496852585539, 0.01085572810624082, 0.4562406987109025, 0.14235496852585539, 0.0007172067688006159, 0.004731966961401138, 0.14235496852585539, 0.7825386004028272, 0.22526818027284895, 0.00024925137169824236, 0.045622888154362884, 0.14235496852585539, 0.22526818027284895, 0.0007172067688006159, 0.22526818027284895, 0.01085572810624082, 0.22526818027284895, 0.01085572810624082, 0.002617811328063631, 2.39395138341366e-05, 1.6864908352125707e-06, 0.002617811328063631, 0.08362477687173493, 0.2519188554772399, 0.14235496852585539, 0.004731966961401138, 0.0010161782725049107, 0.045622888154362884, 0.01085572810624082, 2.39395138341366e-05, 0.00024925137169824236, 0.0019130927836177925, 0.023103250395920184, 0.6183814353384695, 0.09830732478526512, 0.0007172067688006159, 0.045622888154362884, 0.00024925137169824236, 0.14235496852585539, 2.39395138341366e-05, 0.01085572810624082, 0.22526818027284895, 0.0019130927836177925, 0.00024925137169824236, 0.22526818027284895, 0.004731966961401138, 0.045622888154362884, 0.14235496852585539, 0.14235496852585539, 0.004731966961401138, 0.0019130927836177925, 0.33187088109041213, 0.01085572810624082, 0.002617811328063631, 0.8166124177620249, 0.045622888154362884, 0.00024925137169824236, 0.14235496852585539, 0.01085572810624082, 0.004731966961401138, 0.7377976898104908, 0.33187088109041213, 0.7377976898104908, 0.33187088109041213, 0.33187088109041213, 0.08362477687173493, 0.08362477687173493, 0.5873332091757439, 0.4562406987109025, 0.023103250395920184, 0.00024925137169824236, 0.0019130927836177925, 0.22526818027284895, 2.39395138341366e-05, 0.22526818027284895, 0.4562406987109025, 0.055123360333705934, 0.33187088109041213, 0.023103250395920184, 0.004731966961401138, 0.22526818027284895, 0.16308327146260387, 0.023103250395920184, 0.33187088109041213, 0.023103250395920184, 0.004731966961401138, 0.0007172067688006159, 0.14235496852585539, 0.5873332091757439, 0.023103250395920184, 0.14235496852585539, 0.004731966961401138, 0.006262907247611063, 0.004731966961401138, 0.023103250395920184, 0.08362477687173493, 0.14235496852585539, 0.14235496852585539, 0.08362477687173493, 0.33187088109041213, 0.0019130927836177925, 0.002617811328063631, 0.3135140584781766, 0.08362477687173493, 3.975680287272736e-07, 0.023103250395920184, 0.5873332091757439, 0.08362477687173493, 0.14235496852585539, 0.045622888154362884, 0.01391760867866067, 0.4562406987109025, 0.004731966961401138, 5.152872091953731e-05, 0.14235496852585539, 0.01085572810624082, 1.0940752803515118e-05, 0.006262907247611063, 0.14235496852585539, 0.01085572810624082, 0.4562406987109025, 0.023103250395920184, 0.045622888154362884, 0.004731966961401138, 0.002617811328063631, 0.004731966961401138, 0.08362477687173493, 0.0019130927836177925, 0.22526818027284895, 0.023103250395920184, 0.01391760867866067, 0.004731966961401138, 0.08362477687173493, 0.14235496852585539, 6.607832459974615e-06, 0.01085572810624082, 0.16308327146260387, 0.09830732478526512, 8.026747830016304e-05, 0.42324462464832346, 0.36296333549553494, 0.01391760867866067, 0.22526818027284895, 0.023103250395920184, 0.0019130927836177925, 8.026747830016304e-05, 0.2519188554772399, 0.01085572810624082, 0.045622888154362884, 0.01085572810624082, 0.002781510353088379, 0.045622888154362884, 0.14235496852585539, 0.0019130927836177925, 3.802150495064323e-05, 0.045622888154362884, 0.01085572810624082, 0.01085572810624082, 0.2519188554772399, 0.0019130927836177925, 0.0007172067688006159, 0.00024925137169824236, 0.004731966961401138, 0.22526818027284895, 0.08362477687173493, 0.01835421114073839, 0.045622888154362884, 0.02873315983553088, 0.01085572810624082, 8.026747830016304e-05, 0.023103250395920184, 0.023103250395920184, 0.4562406987109025, 0.023103250395920184, 0.0019130927836177925, 0.055123360333705934, 0.5873332091757439, 0.01085572810624082, 0.023103250395920184, 0.023103250395920184, 0.08362477687173493, 0.045622888154362884, 0.0019130927836177925, 0.08362477687173493, 0.08362477687173493, 0.004731966961401138, 0.36296333549553494, 0.0007172067688006159, 2.39395138341366e-05, 0.14235496852585539, 8.026747830016304e-05, 0.5873332091757439, 0.0007172067688006159, 0.4562406987109025, 0.01085572810624082, 0.14235496852585539, 0.5873332091757439, 0.36296333549553494, 0.16308327146260387, 0.023103250395920184, 0.6183814353384695, 0.045622888154362884, 0.045622888154362884, 0.004731966961401138, 0.6183814353384695, 0.006262907247611063, 0.0019130927836177925, 0.0019130927836177925, 0.14235496852585539, 0.22526818027284895, 0.01391760867866067, 0.045622888154362884, 0.33187088109041213, 0.14235496852585539, 0.14235496852585539, 0.7115261138266462, 0.00024925137169824236, 0.08362477687173493, 0.00024925137169824236, 0.14235496852585539, 0.5873332091757439, 0.08362477687173493, 0.08362477687173493, 8.026747830016304e-05, 0.023103250395920184, 0.895427145713559, 0.023103250395920184, 0.9914724170581114, 0.14235496852585539, 0.01391760867866067, 0.16308327146260387, 0.0007172067688006159, 0.004731966961401138, 0.22526818027284895, 2.39395138341366e-05, 0.023103250395920184, 0.045622888154362884, 2.39395138341366e-05, 0.0007172067688006159, 0.02873315983553088, 0.0019130927836177925, 0.0019130927836177925, 0.22526818027284895, 0.16308327146260387, 0.045622888154362884, 0.006262907247611063, 0.004731966961401138, 0.055123360333705934, 0.22526818027284895, 0.01085572810624082, 0.0007172067688006159, 0.08362477687173493, 8.026747830016304e-05, 0.023103250395920184, 0.0007172067688006159, 0.5873332091757439, 0.22526818027284895, 0.01085572810624082, 0.045622888154362884, 0.22526818027284895, 0.045622888154362884, 0.14235496852585539, 0.01085572810624082, 8.026747830016304e-05, 0.045622888154362884, 0.0019130927836177925, 0.08362477687173493, 0.023103250395920184, 0.055123360333705934, 0.004731966961401138, 0.004731966961401138, 8.026747830016304e-05, 0.08362477687173493, 0.045622888154362884, 0.01085572810624082, 0.01085572810624082, 0.8166124177620249, 0.045622888154362884, 0.0019130927836177925, 0.14235496852585539, 0.023103250395920184, 0.33187088109041213, 0.0010161782725049107, 0.045622888154362884, 0.000366240220973836, 0.004731966961401138, 0.7115261138266462, 0.08362477687173493, 0.004731966961401138, 0.0007172067688006159, 0.023103250395920184, 0.0019130927836177925, 6.607832459974615e-06, 0.0019130927836177925, 0.01085572810624082, 8.026747830016304e-05, 0.0019130927836177925, 0.14235496852585539, 0.00024925137169824236, 0.006262907247611063, 0.14235496852585539, 0.8166124177620249, 0.7377976898104908, 0.00024925137169824236, 0.08362477687173493, 0.045622888154362884, 0.8166124177620249, 0.023103250395920184, 0.02873315983553088, 0.0019130927836177925, 0.33187088109041213, 0.14235496852585539, 0.5873332091757439, 0.00012251345164968294, 0.08362477687173493]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from watermarks.kgw.watermark_processor import WatermarkDetector\n",
    "from transformers import AutoTokenizer\n",
    "vanilla_model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(vanilla_model_name)\n",
    "def compute_p_value(samples, detector, type='kgw'):\n",
    "    score_list = []\n",
    "    for s in samples:\n",
    "        score = detector.detect(s)\n",
    "        score_list.append(score['p_value']) if type=='kgw' else score_list.append(score) \n",
    "    return score_list\n",
    "detector = WatermarkDetector(\n",
    "                    device='cpu',\n",
    "                    tokenizer=tokenizer,\n",
    "                    vocab=tokenizer.get_vocab().values(),\n",
    "                    gamma=0.25,\n",
    "                    seeding_scheme='simple_0',\n",
    "                    normalizers=[],\n",
    "                )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['watermarked_model', 'hidden_dim_8192_num_layers_2', 'idden_dim_16384_num_layers_1', 'hidden_dim_8192_num_layers_1', 'idden_dim_16384_num_layers_2', 'hidden_dim_8192_num_layers_3', 'idden_dim_16384_num_layers_3'])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simulation_generation['kgw-k0-gamma0.25-delta2'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class Swish(nn.Module):\n",
    "    def forward(self, x):\n",
    "        return x * torch.sigmoid(x)\n",
    "\n",
    "class TimeEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    TimeEmbedding模块将把整型t，以Transformer函数式位置编码的方式，映射成向量，\n",
    "    其shape为(batch_size, time_channel)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_channels: int):\n",
    "        \"\"\"\n",
    "        Params:\n",
    "            n_channels：即time_channel\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_channels = n_channels\n",
    "        self.lin1 = nn.Linear(self.n_channels // 4, self.n_channels)\n",
    "        self.act = Swish()\n",
    "        self.lin2 = nn.Linear(self.n_channels, self.n_channels)\n",
    "\n",
    "    def forward(self, t: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Params:\n",
    "            t: 维度（batch_size），整型时刻t\n",
    "        \"\"\"\n",
    "        # 以下转换方法和Transformer的位置编码一致\n",
    "        # 【强烈建议大家动手跑一遍，打印出每一个步骤的结果和尺寸，更方便理解】\n",
    "        half_dim = self.n_channels // 8\n",
    "        print(f\"half_dim: {half_dim}\")\n",
    "\n",
    "        emb = math.log(10_000) / (half_dim - 1)\n",
    "        print(f\"emb (log scale factor): {emb}\")\n",
    "\n",
    "        emb = torch.exp(torch.arange(half_dim, device=t.device) * -emb)\n",
    "        print(f\"emb (after exp): {emb}\")\n",
    "\n",
    "        emb = t[:, None] * emb[None, :]\n",
    "        print(f\"emb (after scaling with t): {emb}\")\n",
    "\n",
    "        emb = torch.cat((emb.sin(), emb.cos()), dim=1)\n",
    "        print(f\"emb (after sin/cos): {emb}\")\n",
    "\n",
    "        # Transform with the MLP\n",
    "        emb = self.act(self.lin1(emb))\n",
    "        print(f\"emb (after lin1 and act): {emb}\")\n",
    "\n",
    "        emb = self.lin2(emb)\n",
    "        print(f\"emb (after lin2): {emb}\")\n",
    "\n",
    "        # 输出维度(batch_size, time_channels)\n",
    "        return emb\n",
    "\n",
    "# Example test code\n",
    "if __name__ == \"__main__\":\n",
    "    batch_size = 2\n",
    "    time_channels = 32\n",
    "    time_embedding = TimeEmbedding(time_channels)\n",
    "\n",
    "    t = torch.tensor([1, 2], dtype=torch.float32)  # Example batch of time steps\n",
    "    print(f\"Input t: {t}\")\n",
    "    output = time_embedding(t)\n",
    "    print(f\"Output: {output}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantile values: [0.25, 0.5, 0.75]\n",
      "vanilla_model\n",
      "[0.22526818 0.4562407  0.71152611]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "====================================================================================================\n",
      "kgw-k0-gamma0.25-delta1\n",
      "watermarked_model\n",
      "[0.00424435 0.03717802 0.14235497]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "num_layers_1_hidden_dim_16384\n",
      "[0.04562289 0.16308327 0.4562407 ]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "idden_dim_16384_num_layers_2\n",
      "[0.04562289 0.22526818 0.4562407 ]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "hidden_dim_8192_num_layers_2\n",
      "[0.049921   0.22526818 0.4562407 ]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "hidden_dim_8192_num_layers_3\n",
      "[0.04562289 0.22526818 0.4562407 ]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "idden_dim_16384_num_layers_1\n",
      "[0.04562289 0.16308327 0.4562407 ]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "hidden_dim_8192_num_layers_1\n",
      "[0.05274824 0.14235497 0.4562407 ]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "idden_dim_16384_num_layers_3\n",
      "[0.04562289 0.22526818 0.4562407 ]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "====================================================================================================\n",
      "kgw-k0-gamma0.25-delta2\n",
      "watermarked_model\n",
      "[1.68649084e-06 8.02674783e-05 1.91309278e-03]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "hidden_dim_8192_num_layers_2\n",
      "[0.01085573 0.08362478 0.25191886]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "idden_dim_16384_num_layers_1\n",
      "[0.01391761 0.08362478 0.22526818]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "hidden_dim_8192_num_layers_1\n",
      "[0.02310325 0.08362478 0.22526818]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "idden_dim_16384_num_layers_2\n",
      "[0.01085573 0.08362478 0.22526818]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "hidden_dim_8192_num_layers_3\n",
      "[0.01085573 0.08362478 0.22526818]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "idden_dim_16384_num_layers_3\n",
      "[0.01085573 0.08362478 0.22526818]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "====================================================================================================\n",
      "kgw-k1-gamma0.25-delta2\n",
      "watermarked_model\n",
      "[0.22526818 0.4562407  0.71152611]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "idden_dim_16384_num_layers_3\n",
      "[0.22526818 0.4562407  0.71152611]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "hidden_dim_8192_num_layers_1\n",
      "[0.22158998 0.4562407  0.71152611]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "idden_dim_16384_num_layers_1\n",
      "[0.22526818 0.4562407  0.71152611]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "hidden_dim_8192_num_layers_3\n",
      "[0.22526818 0.4562407  0.71152611]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "hidden_dim_8192_num_layers_2\n",
      "[0.22526818 0.4562407  0.71152611]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "idden_dim_16384_num_layers_2\n",
      "[0.22526818 0.47262726 0.81661242]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "====================================================================================================\n",
      "kgw-k1-gamma0.25-delta1\n",
      "watermarked_model\n",
      "[0.22526818 0.4562407  0.71152611]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "hidden_dim_8192_num_layers_1\n",
      "[0.16308327 0.36296334 0.58733321]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "idden_dim_16384_num_layers_3\n",
      "[0.16308327 0.4562407  0.71152611]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "hidden_dim_8192_num_layers_2\n",
      "[0.14235497 0.36296334 0.71152611]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "idden_dim_16384_num_layers_1\n",
      "[0.22526818 0.4562407  0.71152611]\n",
      "----------------------------------------------------------------------------------------------------\n",
      "idden_dim_16384_num_layers_2\n",
      "[0.22526818 0.4562407  0.71152611]\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[69], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m model_name, generations \u001b[38;5;129;01min\u001b[39;00m watermark_sd\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 13\u001b[0m         output_scores \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_p_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgenerations\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtruncate_prompt_output\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkgw\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m         output_scores \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(output_scores)\n\u001b[1;32m     15\u001b[0m         \u001b[38;5;28mprint\u001b[39m(model_name)\n",
      "Cell \u001b[0;32mIn[52], line 8\u001b[0m, in \u001b[0;36mcompute_p_value\u001b[0;34m(samples, detector, type)\u001b[0m\n\u001b[1;32m      6\u001b[0m score_list \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s \u001b[38;5;129;01min\u001b[39;00m samples:\n\u001b[0;32m----> 8\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[43mdetector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     score_list\u001b[38;5;241m.\u001b[39mappend(score[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp_value\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkgw\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m score_list\u001b[38;5;241m.\u001b[39mappend(score) \n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m score_list\n",
      "File \u001b[0;32m~/watermark-learnability/experiments/../watermarks/kgw/watermark_processor.py:641\u001b[0m, in \u001b[0;36mWatermarkDetector.detect\u001b[0;34m(self, text, tokenized_text, window_size, window_stride, return_prediction, return_scores, z_threshold, convert_to_float, **kwargs)\u001b[0m\n\u001b[1;32m    639\u001b[0m     output_dict\u001b[38;5;241m.\u001b[39mupdate(score_dict)\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 641\u001b[0m     score_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_score_sequence\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenized_text\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_scores:\n\u001b[1;32m    643\u001b[0m     output_dict\u001b[38;5;241m.\u001b[39mupdate(score_dict)\n",
      "File \u001b[0;32m~/watermark-learnability/experiments/../watermarks/kgw/watermark_processor.py:394\u001b[0m, in \u001b[0;36mWatermarkDetector._score_sequence\u001b[0;34m(self, input_ids, return_num_tokens_scored, return_num_green_tokens, return_green_fraction, return_green_token_mask, return_z_score, return_z_at_T, return_p_value)\u001b[0m\n\u001b[1;32m    383\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_score_sequence\u001b[39m(\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    385\u001b[0m     input_ids: torch\u001b[38;5;241m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    392\u001b[0m     return_p_value: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    393\u001b[0m ):\n\u001b[0;32m--> 394\u001b[0m     ngram_to_watermark_lookup, frequencies_table \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_score_ngrams_in_passage\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m     green_token_mask, green_unique, offsets \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_green_at_T_booleans(\n\u001b[1;32m    396\u001b[0m         input_ids, ngram_to_watermark_lookup\n\u001b[1;32m    397\u001b[0m     )\n\u001b[1;32m    399\u001b[0m     \u001b[38;5;66;03m# Count up scores over all ngrams\u001b[39;00m\n",
      "File \u001b[0;32m~/watermark-learnability/experiments/../watermarks/kgw/watermark_processor.py:350\u001b[0m, in \u001b[0;36mWatermarkDetector._score_ngrams_in_passage\u001b[0;34m(self, input_ids)\u001b[0m\n\u001b[1;32m    348\u001b[0m     prefix \u001b[38;5;241m=\u001b[39m ngram_example \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mself_salt \u001b[38;5;28;01melse\u001b[39;00m ngram_example[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    349\u001b[0m     target \u001b[38;5;241m=\u001b[39m ngram_example[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m--> 350\u001b[0m     ngram_to_watermark_lookup[ngram_example] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_ngram_score_cached\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ngram_to_watermark_lookup, frequencies_table\n",
      "File \u001b[0;32m~/watermark-learnability/experiments/../watermarks/kgw/watermark_processor.py:330\u001b[0m, in \u001b[0;36mWatermarkDetector._get_ngram_score_cached\u001b[0;34m(self, prefix, target)\u001b[0m\n\u001b[1;32m    328\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Expensive re-seeding and sampling is cached.\"\"\"\u001b[39;00m\n\u001b[1;32m    329\u001b[0m \u001b[38;5;66;03m# Handle with care, should ideally reset on __getattribute__ access to self.prf_type, self.context_width, self.self_salt, self.hash_key\u001b[39;00m\n\u001b[0;32m--> 330\u001b[0m greenlist_ids \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_greenlist_ids\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprefix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m target \u001b[38;5;129;01min\u001b[39;00m greenlist_ids \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/watermark-learnability/experiments/../watermarks/kgw/watermark_processor.py:86\u001b[0m, in \u001b[0;36mWatermarkBase._get_greenlist_ids\u001b[0;34m(self, input_ids)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_seed_rng(input_ids)\n\u001b[1;32m     85\u001b[0m greenlist_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvocab_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgamma)\n\u001b[0;32m---> 86\u001b[0m vocab_permutation \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandperm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     87\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minput_ids\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrng\u001b[49m\n\u001b[1;32m     88\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mselect_green_tokens:  \u001b[38;5;66;03m# directly\u001b[39;00m\n\u001b[1;32m     90\u001b[0m     greenlist_ids \u001b[38;5;241m=\u001b[39m vocab_permutation[:greenlist_size]  \u001b[38;5;66;03m# new\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "quantiles = [0.25, 0.5, 0.75] \n",
    "print(f\"Quantile values: {quantiles}\")\n",
    "for watermark_type, watermark_sd in simulation_generation.items():\n",
    "    print(watermark_type)\n",
    "    detector = WatermarkDetector(\n",
    "                    device='cpu',\n",
    "                    tokenizer=tokenizer,\n",
    "                    vocab=tokenizer.get_vocab().values(),\n",
    "                    gamma=0.25,\n",
    "                    seeding_scheme='simple_0',\n",
    "                    normalizers=[],\n",
    "                )\n",
    "    # print(watermark_sd.keys())\n",
    "    if watermark_type == 'vanilla_model':\n",
    "        output_scores = compute_p_value(watermark_sd['truncate_prompt_output'], detector, type='kgw')\n",
    "        quantile_values = np.quantile(output_scores, quantiles)\n",
    "        print(quantile_values)\n",
    "        print('-'*100)\n",
    "    else:\n",
    "        for model_name, generations in watermark_sd.items():\n",
    "            output_scores = compute_p_value(generations['truncate_prompt_output'], detector, type='kgw')\n",
    "            output_scores = np.array(output_scores)\n",
    "            print(model_name)\n",
    "            quantile_values = np.quantile(output_scores, quantiles)\n",
    "            print(quantile_values)\n",
    "            print('-'*100)\n",
    "            # print(np.median(output_scores))\n",
    "            # break\n",
    "    print('='*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-05 13:24:10.309529: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-08-05 13:24:13.999439: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-08-05 13:24:14.974190: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-08-05 13:24:15.439064: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-08-05 13:24:17.528727: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-08-05 13:24:26.472951: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::_shape_as_tensor' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::_reshape_from_tensor' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::reshape' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::reshape_as' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::add' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::sub' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::rsub' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::mul' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::div' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::addcmul' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::floor_divide' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::floordiv' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::true_divide' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::reciprocal' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::cat' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::stack' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::list' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::mm' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::bmm' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::matmul' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::addmm' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::neg' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::sqrt' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::rsqrt' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::tanh' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::sin' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::cos' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::tan' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::asin' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::acos' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::atan' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::atan2' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::sigmoid' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::sign' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::prod' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::mean' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::sum' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::cumsum' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::_sample_dirichlet' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::_standard_gamma' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::t' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::numpy_T' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::expand' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::broadcast_to' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::expand_as' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::embedding' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::embedding_bag' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::size' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::transpose' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::permute' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::view' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::view_as' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::unsafe_chunk' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::split' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::unsafe_split' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::split_with_sizes' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::unsafe_split_with_sizes' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::unbind' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::select' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::square' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::squeeze' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::prelu' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::silu' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::mish' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::relu' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::relu6' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::ceil' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::floor' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::len' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::threshold' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::leaky_relu' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::glu' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::softmax' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::softplus' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::get_pool_ceil_padding' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::max_pool3d' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::max_pool2d' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::max_pool1d' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::max_pool1d_with_indices' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::max_pool2d_with_indices' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::max_pool3d_with_indices' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::avg_pool3d' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::avg_pool2d' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::avg_pool1d' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::adaptive_max_pool3d' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::adaptive_max_pool2d' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::adaptive_max_pool1d' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::adaptive_avg_pool3d' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::adaptive_avg_pool2d' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::adaptive_avg_pool1d' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::constant_pad_nd' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::reflection_pad3d' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::reflection_pad2d' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::reflection_pad1d' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::replication_pad3d' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::replication_pad2d' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::replication_pad1d' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::pad' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::upsample_trilinear3d' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::upsample_bilinear2d' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::upsample_linear1d' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::upsample_nearest3d' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::upsample_nearest2d' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::upsample_nearest1d' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::__interpolate' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::bitwise_not' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::bitwise_or' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::__not_' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::eq' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::ne' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::gt' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::lt' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::ge' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::le' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::__and_' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::__or_' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::__xor_' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::logical_and' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::logical_or' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::logical_xor' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::logical_not' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::__rshift_' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::__lshift_' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::where' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::log_softmax' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::_log_softmax' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::_convolution' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::_convolution_mode' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::convolution' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::conv1d' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::conv2d' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::conv3d' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::conv_transpose1d' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::conv_transpose2d' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::conv_transpose3d' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::batch_norm' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::native_layer_norm' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::layer_norm' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::instance_norm' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::unfold' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::elu' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::selu' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::index_select' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::index_put' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::index_fill' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::index_copy' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::bucketize' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::type_as' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::cosine_similarity' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::pairwise_distance' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::clone' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::abs' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::log' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::log1p' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::log10' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::pow' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::clamp' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::clamp_min' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::clamp_max' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::max' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::maximum' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::min' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::minimum' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::amax' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::amin' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::aminmax' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::exp' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::dropout' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::dropout_' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::feature_dropout' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::alpha_dropout' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::feature_alpha_dropout' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::feature_dropout_' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::feature_alpha_dropout_' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::alpha_dropout_' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::norm' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::conv_tbc' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::_unique' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::_unique2' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::_cast_Byte' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::_cast_Char' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::_cast_Short' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::_cast_Int' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::_cast_Long' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::_cast_Half' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::_cast_Float' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::_cast_Double' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::_cast_Bool' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::empty' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::empty_like' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::new_empty' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::scalar_tensor' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::tensor' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::as_tensor' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::zeros' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::zeros_like' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::new_zeros' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::zero' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::ones' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::ones_like' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::new_ones' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::full' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::full_like' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::new_full' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::eye' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::slice' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::hardtanh' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::hardswish' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::hardsigmoid' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::tanhshrink' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::hardshrink' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::softshrink' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::alias' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::unsqueeze' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::sort' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::numel' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::topk' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'prim::convert_element_type' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::to' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::repeat' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::repeat_interleave' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::pixel_shuffle' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::pixel_unshuffle' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::lstm' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::lstm_cell' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::rnn_relu' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::rnn_tanh' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::gru' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::_dim_arange' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::detach' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::contiguous' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::_pack_padded_sequence' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::_pad_packed_sequence' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::randint' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::randint_like' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::randn' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::rand' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::randn_like' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::rand_like' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::rrelu' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::bernoulli' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::log_sigmoid' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::erf' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::flatten' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::nonzero' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::nonzero_numpy' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::isnan' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::any' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::all' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::narrow' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::argmax' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::argmin' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::scatter' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::scatter_add' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::log2' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::is_floating_point' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::__is_' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::__isnot_' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::one_hot' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::gather' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::std' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::var' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::var_mean' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::std_mean' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::logsumexp' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::arange' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::linspace' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::lift' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::masked_fill' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::masked_fill_' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::index' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::linalg_norm' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::linalg_vector_norm' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::linalg_matrix_norm' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::linalg_cross' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::frobenius_norm' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::multinomial' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::baddbmm' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::meshgrid' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n",
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/onnx/_internal/registration.py:167: OnnxExporterWarning: Symbolic function 'aten::remainder' already registered for opset 9. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00410008430480957,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1767738904f45da9e5630f0bf05cd79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s>[INST] Reward Modelling Phase: In the second phase the SFT model is prompted with prompts x to\n",
      "produce pairs of answers (y1, y2 ) ∼ π SFT (y | x). These are then presented to human labelers\n",
      "who express preferences for one answer, denoted as yw ≻ yl | x where yw and yl denotes the\n",
      "preferred and dispreferred completion amongst (y1 , y2 ) respectively. The preferences are assumed\n",
      "to be generated by some latent reward model r∗ (y, x), which we do not have access to. There are a\n",
      "number of approaches used to model preferences, the Bradley-Terry (BT) [5] model being a popular\n",
      "choice (although more general Plackett-Luce ranking models [30, 21] are also compatible with the\n",
      "framework if we have access to several ranked answers). The BT model stipulates that the human\n",
      "preference distribution p∗ can be written as:\n",
      "p∗ (y1 ≻ y2 | x) =\n",
      "exp (r∗ (x, y1 ))\n",
      ".\n",
      "exp (r∗\n",
      "(x, y1 )) + exp (r ∗ (x, y2))\n",
      "(1)\n",
      "Assuming access to a static dataset of comparisons D = \n",
      "x(i), yw (i) , yl (i) N\n",
      "i=1 sampled from p∗ , we\n",
      "can parametrize a reward model rφ (x, y) and estimate the parameters via maximum likelihood.\n",
      "Framing the problem as a binary classification we have the negative log-likelihood loss:\n",
      "LR(rφ , D) = −E(x,yw ,yl )∼D \n",
      "log σ(rφ (x, yw ) − rφ (x, yl )).\n",
      "\n",
      "Please explain the math behind this paper by explaining all the variables [/INST] This paper is focused on modeling the preferences of human evaluators when ranking the quality of generated answers. The model is based on the Bradley-Terry (BT) model, which is a popular choice for modeling binary preferences. Here's a breakdown of the variables used in the paper:\n",
      "\n",
      "1. $x$: The context vector, which represents the input to the SFT model.\n",
      "2. $y$: The generated answer.\n",
      "3. $y_w$ and $y_l$: The preferred and dispreferred answer, respectively, when the SFT model is prompted with $x$.\n",
      "4. $r\\star (x, y_w)$ and $r\\star (x, y_l)$: The predicted rewards for the preferred and dispreferred answer, respectively, based on the reward model $r\\star$.\n",
      "5. $p\\star (y_w \\mid x)$ and $p\\star (y_l \\mid x)$: The predicted probability of the preferred and dispreferred answer, respectively, given the context $x$.\n",
      "6. $p\\star (y_w \\mid x)$ and $p\\star (y_l \\mid x)$: The predicted probability of the preferred and dispreferred answer, respectively, given the context $x$.\n",
      "7. $\\sigma$: The sigmoid function used to convert the predicted probabilities to binary labels.\n",
      "8. $D$: The dataset of comparisons, which consists of the context $x$, the preferred and dispreferred answer, and the corresponding label.\n",
      "9. $N$: The number of samples in the dataset $D$.\n",
      "10. $E$: The expected value of the log-likelihood loss.\n",
      "11. $LR(r\\star, D)$: The negative log-likelihood loss function, which is the difference between the predicted probabilities and the true labels.\n",
      "\n",
      "The goal of the paper is to estimate the parameters of the reward model $r\\star$ that maximizes the expected log-likelihood loss $E(LR(r\\star, D))$. The authors use a binary classification approach, where the predicted probabilities are converted to binary labels using the sigmoid function $\\sigma$. The log-likelihood loss is then computed as the difference between the predicted probabilities and the true labels. The parameters of the reward model $r\\star$ are estimated using maximum likelihood estimation.\n",
      "\n",
      "The authors use a Bradley-Terry model to model the human preference distribution $p\\star(y_w\\mid x)$ and $p\\star(y_l\\mid x)$. The Bradley-Terry model is a popular choice for modeling binary preferences, and it assumes that the probability of the preferred and dispreferred answer can be expressed as a function of the difference between the predicted rewards and the baseline reward. The baseline reward is a constant value that represents the expected reward for choosing the dispreferred answer. The probability of the preferred answer can be expressed as a function of the difference between the predicted reward and the baseline reward, which is called the Bradley-Terry function.\n",
      "\n",
      "The authors also use the Plackett-Luce ranking model, which is a more general model that allows for multiple preferred answers. The Plackett-Luce model is based on the idea that the ranking of the answers can be represented as a probability distribution over the possible rankings. The authors show that the Plackett-Luce model can be used to model the human preference distribution $p\\star(y_w\\mid x)$ and $p\\star(y_l\\mid x)$ in a more flexible way than the Bradley-Terry model.\n",
      "\n",
      "Overall, the paper presents a novel approach to modeling the preferences of human evaluators when ranking the quality of generated answers. The proposed approach is based on the Bradley-Terry and Plackett-Luce models, and it uses a binary classification approach to estimate the parameters of the reward model $r\\star$. The authors demonstrate the effectiveness of their approach using a real-world dataset of comparisons.\n",
      "447.13639307022095\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "\n",
    "model_name = \"RohitSahoo/llama-2-7b-chat-hf-math-ft-V1\"\n",
    "import os\n",
    "import copy\n",
    "os.environ['http_proxy'] = \"http://10.176.52.116:7890\"\n",
    "os.environ['https_proxy'] = \"http://10.176.52.116:7890\"\n",
    "os.environ['all_proxy'] = \"socks5://10.176.52.116:7891\"\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    # use the gpu\n",
    "    device_map=\"cpu\"\n",
    ")\n",
    "\n",
    "# don't use the cache\n",
    "model.config.use_cache = False\n",
    "\n",
    "# Load the tokenizer from the model (llama2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "\n",
    "text = '''Reward Modelling Phase: In the second phase the SFT model is prompted with prompts x to\n",
    "produce pairs of answers (y1, y2 ) ∼ π SFT (y | x). These are then presented to human labelers\n",
    "who express preferences for one answer, denoted as yw ≻ yl | x where yw and yl denotes the\n",
    "preferred and dispreferred completion amongst (y1 , y2 ) respectively. The preferences are assumed\n",
    "to be generated by some latent reward model r∗ (y, x), which we do not have access to. There are a\n",
    "number of approaches used to model preferences, the Bradley-Terry (BT) [5] model being a popular\n",
    "choice (although more general Plackett-Luce ranking models [30, 21] are also compatible with the\n",
    "framework if we have access to several ranked answers). The BT model stipulates that the human\n",
    "preference distribution p∗ can be written as:\n",
    "p∗ (y1 ≻ y2 | x) =\n",
    "exp (r∗ (x, y1 ))\n",
    ".\n",
    "exp (r∗\n",
    "(x, y1 )) + exp (r ∗ (x, y2))\n",
    "(1)\n",
    "Assuming access to a static dataset of comparisons D = \n",
    "x(i), yw (i) , yl (i) N\n",
    "i=1 sampled from p∗ , we\n",
    "can parametrize a reward model rφ (x, y) and estimate the parameters via maximum likelihood.\n",
    "Framing the problem as a binary classification we have the negative log-likelihood loss:\n",
    "LR(rφ , D) = −E(x,yw ,yl )∼D \n",
    "log σ(rφ (x, yw ) − rφ (x, yl )).\n",
    "\n",
    "Please explain the math behind this paper by explaining all the variables'''\n",
    "\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=1500)\n",
    "result = pipe(f\"<s>[INST] {text} [/INST]\")\n",
    "print(result[0]['generated_text'])\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "class LogitsDataset(Dataset):\n",
    "    def __init__(self, input_logits, target_logits):\n",
    "        self.input_logits = input_logits\n",
    "        self.target_logits = target_logits\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_logits)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.input_logits[idx], self.target_logits[idx]\n",
    "\n",
    "final_result_folder = \"/remote-home/miintern1/watermark-learnability/data/c4/\"\n",
    "watermark_residuals = torch.load(final_result_folder + \"watermark_residuals.pt\", map_location=torch.device('cpu'))\n",
    "vanilla_residuals = torch.load(final_result_folder + \"vanilla_residuals.pt\", map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['model.layers.28', 'model.layers.29', 'model.layers.30', 'model.layers.31']\n"
     ]
    }
   ],
   "source": [
    "watermark_configs = {\n",
    "     \"cygu/llama-2-7b-logit-watermark-distill-kgw-k0-gamma0.25-delta1\":{\"type\": \"kgw\", \"k\": 0, \"gamma\": 0.25, \"delta\": 1.0, \"seeding_scheme\": \"simple_0\", \"kgw_device\": \"cpu\"},\n",
    "     \"cygu/llama-2-7b-logit-watermark-distill-kgw-k0-gamma0.25-delta2\":{\"type\": \"kgw\", \"k\": 0, \"gamma\": 0.25, \"delta\": 2.0, \"seeding_scheme\": \"simple_0\", \"kgw_device\": \"cpu\"},\n",
    "     \"cygu/llama-2-7b-logit-watermark-distill-kgw-k1-gamma0.25-delta1\":{\"type\": \"kgw\", \"k\": 1, \"gamma\": 0.25, \"delta\": 1.0, \"seeding_scheme\": \"simple_1\", \"kgw_device\": \"cpu\"},\n",
    "     \"cygu/llama-2-7b-logit-watermark-distill-kgw-k1-gamma0.25-delta2\":{\"type\": \"kgw\", \"k\": 1, \"gamma\": 0.25, \"delta\": 2.0, \"seeding_scheme\": \"simple_1\", \"kgw_device\": \"cpu\"},\n",
    "}\n",
    "batch_size = 64\n",
    "model_dimension = 4096\n",
    "hidden_dimension = model_dimension * 4\n",
    "num_layers = 4\n",
    "analyze_layers = list(vanilla_residuals.keys())\n",
    "print(analyze_layers)\n",
    "target_layer = 'model.layers.31'\n",
    "target_watermark =  \"cygu/llama-2-7b-logit-watermark-distill-kgw-k1-gamma0.25-delta2\"\n",
    "\n",
    "dataset_dict = dict()\n",
    "dataloader_dict = dict()\n",
    "for layer in analyze_layers:\n",
    "    dataset_dict[layer] = dict()\n",
    "    dataloader_dict[layer] = dict()\n",
    "    for watermark_name, _ in watermark_configs.items():\n",
    "        dataset_dict[layer][watermark_name] = LogitsDataset(vanilla_residuals[layer], watermark_residuals[watermark_name][layer])\n",
    "        dataloader_dict[layer][watermark_name] = DataLoader(dataset_dict[layer][watermark_name], batch_size=batch_size, shuffle=True)\n",
    "\n",
    "dataloaders = dataloader_dict[target_layer][target_watermark]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10:   0%|          | 0/217 [00:03<?, ?it/s, loss=0.0164]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.0164\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.fc = nn.Linear(dim, dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc(x)\n",
    "        out = self.relu(out)\n",
    "        out = out + x \n",
    "        return out\n",
    "\n",
    "class TransformModel(nn.Module):\n",
    "    def __init__(self, num_layers=4, input_dim=1024, hidden_dim=500, output_dim=300):\n",
    "        super(TransformModel, self).__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        self.layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "        \n",
    "        for _ in range(num_layers - 2):\n",
    "            self.layers.append(ResidualBlock(hidden_dim))\n",
    "\n",
    "        self.layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.layers)):\n",
    "            x = self.layers[i](x)\n",
    "        return x\n",
    "\n",
    "# Training function\n",
    "def train_model(model, dataloader, num_epochs=10, learning_rate=0.001):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        for inputs, targets in progress_bar:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=running_loss/len(dataloader))\n",
    "            break\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(dataloader):.4f}\")\n",
    "        break\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TransformModel(num_layers=4, input_dim=4096, hidden_dim=4096*4, output_dim=4096)\n",
    "train_model(model, dataloader, num_epochs=10, learning_rate=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22\n",
      "Evaluation loss: 626.3929\n"
     ]
    }
   ],
   "source": [
    "train_ratio = 0.9\n",
    "dataset_dict = dict()\n",
    "train_dataloader_dict = dict()\n",
    "eval_dataloader_dict = dict()\n",
    "for layer in analyze_layers:\n",
    "    dataset_dict[layer] = dict()\n",
    "    train_dataloader_dict[layer] = dict()\n",
    "    eval_dataloader_dict[layer] = dict()\n",
    "    for watermark_name, _ in watermark_configs.items():\n",
    "        dataset_dict[layer][watermark_name] = LogitsDataset(vanilla_residuals[layer], watermark_residuals[watermark_name][layer])\n",
    "        train_dataset, test_dataset = torch.utils.data.random_split(dataset_dict[layer][watermark_name], [int(train_ratio*len(dataset_dict[layer][watermark_name])), len(dataset_dict[layer][watermark_name]) - int(train_ratio*len(dataset_dict[layer][watermark_name]))])\n",
    "        train_dataloader_dict[layer][watermark_name] = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "        eval_dataloader_dict[layer][watermark_name] = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "dataloader = train_dataloader_dict[target_layer][target_watermark]\n",
    "eval_dataloader = eval_dataloader_dict[target_layer][target_watermark]\n",
    "print(len(eval_dataloader))\n",
    "len(dataloader)\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model(model, dataloader):\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    num_layers = len(model.layers)\n",
    "    hidden_dimension = model.layers[0].out_features\n",
    "    hyperparameter_name = f\"num_layers_{num_layers}_hidden_dim_{hidden_dimension}\"\n",
    "    # writer = SummaryWriter(log_dir=f'runs/{hyperparameter_name}/evaluation')\n",
    "\n",
    "    running_loss = 0.0\n",
    "    # progress_bar = tqdm(dataloader, desc=\"Evaluating\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, targets) in enumerate(dataloader):\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            # progress_bar.set_postfix(loss=running_loss/(i+1))\n",
    "    \n",
    "    eval_loss = running_loss / len(dataloader)\n",
    "    # writer.add_scalar('Evaluation Loss', eval_loss, 0)\n",
    "    # writer.close()\n",
    "    return eval_loss\n",
    "\n",
    "eval_loss = evaluate_model(model, eval_dataloader)\n",
    "print(f\"Evaluation loss: {eval_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, num_epochs=10, learning_rate=0.001):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        for inputs, targets in progress_bar:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=running_loss/len(dataloader))\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(dataloader):.4f}\")\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TransformModel(num_layers=4, input_dim=4096, hidden_dim=4096*4, output_dim=4096)\n",
    "train_model(model, dataloader, num_epochs=10, learning_rate=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "def extract_files(folder_path):\n",
    "    files = glob.glob(os.path.join(folder_path, '*'))\n",
    "    files = [f for f in files if os.path.isfile(f)]\n",
    "    return files\n",
    "\n",
    "from collections import defaultdict\n",
    "folder_path = '/remote-home/miintern1/watermark-learnability/data/model_weights_2'\n",
    "MLP_model_list = extract_files(folder_path)\n",
    "MLP_model_path_dict = defaultdict(list)\n",
    "for path in MLP_model_list:\n",
    "    MLP_model_path_dict[path[83:106]].append(path)\n",
    "watermark_list = list(MLP_model_path_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['vanilla_model'])\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'kgw-k0-gamma0.25-delta1'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[53], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m     simulation_generation \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(simulation_generation\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m----> 5\u001b[0m \u001b[43msimulation_generation\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkgw-k0-gamma0.25-delta1\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mkeys()\n",
      "\u001b[0;31mKeyError\u001b[0m: 'kgw-k0-gamma0.25-delta1'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open('/remote-home/miintern1/watermark-learnability/data/c4/simulation_generation.json') as f:\n",
    "    simulation_generation = json.load(f)\n",
    "print(simulation_generation.keys())\n",
    "simulation_generation['kgw-k0-gamma0.25-delta1'].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['kgw-k0-gamma0.25-delta1', 'kgw-k0-gamma0.25-delta2', 'kgw-k1-gamma0.25-delta2'])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "dict_keys(['watermarked_model', 'num_layers_1_hidden_dim_16384', 'num_layers_1_hidden_dim_8192', 'num_layers_2_hidden_dim_16384'])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00397181510925293,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "pytorch_model.bin.index.json",
       "rate": null,
       "total": 26788,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d98c5c1710b24fcbbd01c89c447691f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin.index.json:   0%|          | 0.00/26.8k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0035653114318847656,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Downloading shards",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc669081c57f4b108ce2676e53fefba8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0037529468536376953,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "pytorch_model-00001-of-00002.bin",
       "rate": null,
       "total": 9976638373,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbf9af119a714103b0df57697865e124",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00001-of-00002.bin:   0%|          | 0.00/9.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.004242658615112305,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "pytorch_model-00002-of-00002.bin",
       "rate": null,
       "total": 3500317102,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5513f95417b24c8cbee8711a708778d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00002-of-00002.bin:   0%|          | 0.00/3.50G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.00838017463684082,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60d9ff8f01814492bc0eac158ae20fe9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.003658294677734375,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "generation_config.json",
       "rate": null,
       "total": 183,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a590ca0e07a34e6bad412737fe95c89d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/183 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0035042762756347656,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "tokenizer_config.json",
       "rate": null,
       "total": 1567,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3ac93350a574d5e8d384f801d3989e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.57k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.003547191619873047,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "tokenizer.model",
       "rate": null,
       "total": 499723,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c40e6c9dd6348efa0d6bffc3998221e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.003555774688720703,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "special_tokens_map.json",
       "rate": null,
       "total": 437,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc332623eb0f4407aa3e3ad544b1f08f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/437 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0038063526153564453,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "tokenizer.json",
       "rate": null,
       "total": 1842767,
       "unit": "B",
       "unit_divisor": 1000,
       "unit_scale": true
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "efed17e5c35e4a8494955bf895d4471f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    BitsAndBytesConfig,\n",
    "    pipeline,\n",
    "    logging,\n",
    ")\n",
    "import os\n",
    "os.environ['http_proxy'] = \"http://10.176.52.116:7890\"\n",
    "os.environ['https_proxy'] = \"http://10.176.52.116:7890\"\n",
    "os.environ['all_proxy'] = \"socks5://10.176.52.116:7891\"\n",
    "import sys\n",
    "\n",
    "model_name = \"RohitSahoo/llama-2-7b-chat-hf-math-ft-V1\"\n",
    "\n",
    "\n",
    "# load the quantized settings, we're doing 4 bit quantization\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.float16,\n",
    "#     bnb_4bit_use_double_quant=False,\n",
    "# )\n",
    "\n",
    "# Load base model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    # quantization_config=bnb_config,\n",
    "    # use the gpu\n",
    "    device_map='cpu'\n",
    ")\n",
    "\n",
    "# don't use the cache\n",
    "model.config.use_cache = False\n",
    "\n",
    "# Load the tokenizer from the model (llama2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, use_fast=False)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"right\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "text = '''Reward Modelling Phase: In the second phase the SFT model is prompted with prompts x to\n",
    "produce pairs of answers (y1, y2 ) ∼ π SFT (y | x). These are then presented to human labelers\n",
    "who express preferences for one answer, denoted as yw ≻ yl | x where yw and yl denotes the\n",
    "preferred and dispreferred completion amongst (y1 , y2 ) respectively. The preferences are assumed\n",
    "to be generated by some latent reward model r∗ (y, x), which we do not have access to. There are a\n",
    "number of approaches used to model preferences, the Bradley-Terry (BT) [5] model being a popular\n",
    "choice (although more general Plackett-Luce ranking models [30, 21] are also compatible with the\n",
    "framework if we have access to several ranked answers). The BT model stipulates that the human\n",
    "preference distribution p∗ can be written as:\n",
    "p∗ (y1 ≻ y2 | x) =\n",
    "exp (r∗ (x, y1 ))\n",
    ".\n",
    "exp (r∗\n",
    "(x, y1 )) + exp (r ∗ (x, y2))\n",
    "(1)\n",
    "Assuming access to a static dataset of comparisons D = x(i), yw (i) , yl (i) N\n",
    "i=1 sampled from p∗ , we\n",
    "can parametrize a reward model rφ (x, y) and estimate the parameters via maximum likelihood.\n",
    "Framing the problem as a binary classification we have the negative log-likelihood loss:\n",
    "LR(rφ , D) = −E(x,yw ,yl )∼D \n",
    "log σ(rφ (x, yw ) − rφ (x, yl )).\n",
    "\n",
    "Please explain the math behind this paper by explaining all the variables'''\n",
    "\n",
    "\n",
    "import time\n",
    "start = time.time()\n",
    "\n",
    "logging.set_verbosity(logging.CRITICAL)\n",
    "\n",
    "pipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=1500)\n",
    "result = pipe(f\"<s>[INST] {text} [/INST]\")\n",
    "print(result[0]['generated_text'])\n",
    "\n",
    "end = time.time()\n",
    "print(end - start)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tqdm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m all_attention_masks \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      5\u001b[0m hook_layer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m28\u001b[39m,\u001b[38;5;241m32\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m example \u001b[38;5;129;01min\u001b[39;00m \u001b[43mtqdm\u001b[49m(dataset):\n\u001b[1;32m      7\u001b[0m     text \u001b[38;5;241m=\u001b[39m example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# Tokenize the text\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tqdm' is not defined"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "block_size = 512\n",
    "batch_size = 16\n",
    "all_input_ids = []\n",
    "all_attention_masks = []\n",
    "hook_layer = range(28,32)\n",
    "for example in tqdm(dataset):\n",
    "    text = example[\"text\"]\n",
    "    # Tokenize the text\n",
    "    tokenized_text = tokenizer(text, return_tensors='pt', truncation=True, padding='max_length', max_length=block_size)\n",
    "    \n",
    "    # Add the tokenized text to the lists\n",
    "    all_input_ids.append(tokenized_text['input_ids'].squeeze().tolist())\n",
    "    all_attention_masks.append(tokenized_text['attention_mask'].squeeze().tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_input_ids = list(chain(*all_input_ids))\n",
    "all_attention_masks = list(chain(*all_attention_masks))\n",
    "\n",
    "total_length = (len(all_input_ids) // block_size) * block_size\n",
    "all_input_ids = all_input_ids[:total_length]\n",
    "all_attention_masks = all_attention_masks[:total_length]\n",
    "print(len(all_input_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the tokenized texts into chunks of block_size\n",
    "grouped_input_ids = [all_input_ids[i:i + block_size] for i in range(0, total_length, block_size)]\n",
    "grouped_attention_masks = [all_attention_masks[i:i + block_size] for i in range(0, total_length, block_size)]\n",
    "print(f\"{len(grouped_input_ids)=}, {len(grouped_attention_masks)=}\")\n",
    "\n",
    "# Create batches of size batch_size\n",
    "batched_input_ids = [grouped_input_ids[i:i + batch_size] for i in range(0, len(grouped_input_ids), batch_size)]\n",
    "batched_attention_masks = [grouped_attention_masks[i:i + batch_size] for i in range(0, len(grouped_attention_masks), batch_size)]\n",
    "print(f\"{len(batched_input_ids)=}, {len(batched_attention_masks)=}\")\n",
    "\n",
    "batched_input_ids = [torch.tensor(batch) for batch in batched_input_ids]\n",
    "batched_attention_masks = [torch.tensor(batch) for batch in batched_attention_masks]\n",
    "logging.info(\"Data tokenized and grouped successfully.\")\n",
    "\n",
    "print(f\"{batched_input_ids[-1].shape=}, {batched_attention_masks[-1].shape=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.003917694091796875,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "521ccf4b1263440cbcf5388417091ef4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "del vanilla_model\n",
    "vanilla_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", device_map = 'auto')\n",
    "logging.info(\"Vanilla model and tokenizer loaded successfully.\")\n",
    "\n",
    "vanilla_outputs = dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/remote-home/miintern1/anaconda3/envs/mech/lib/python3.12/site-packages/torch/cuda/__init__.py:141: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformer_lens\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m HookedTransformer\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoModelForCausalLM\n\u001b[0;32m----> 3\u001b[0m tl_model \u001b[38;5;241m=\u001b[39m HookedTransformer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLlama-2-7b\u001b[39m\u001b[38;5;124m'\u001b[39m, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m\"\u001b[39m, hf_model\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m)\n\u001b[1;32m      4\u001b[0m tl_model\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from transformer_lens import HookedTransformer\n",
    "from transformers import AutoModelForCausalLM\n",
    "tl_model = HookedTransformer.from_pretrained('Llama-2-7b', device=\"cpu\", hf_model=model)\n",
    "tl_model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_outputs = dict()\n",
    "def get_layer_output_hook(layer_name):\n",
    "    def hook_fn(module, input, output):\n",
    "        # print(f\"{output[0].shape=}\")\n",
    "        # print(f\"{output[1]=}\")\n",
    "        if layer_name not in vanilla_outputs:\n",
    "            vanilla_outputs[layer_name] = []\n",
    "        vanilla_outputs[layer_name].append(output[0].cpu())\n",
    "    return hook_fn\n",
    "\n",
    "for i in hook_layer:  # Change the range to match the number of layers you want to capture\n",
    "    layer_name = f\"model.layers.{i}\"  # Adjust the layer name/path as needed\n",
    "    layer = dict([*vanilla_model.named_modules()])[layer_name]\n",
    "    layer.register_forward_hook(get_layer_output_hook(layer_name))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, cache_value in cache.items():\n",
    "    if(isinstance(cache_value, torch.Tensor)):\n",
    "        if cache_value.shape == vanilla_outputs['model.layers.31'][0].shape:\n",
    "            if (torch.allclose(cache_value, vanilla_outputs['model.layers.30'][0], atol=1e-05)):\n",
    "                print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0041120052337646484,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fd70759ea234d73be7513c4a404b7f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# note me that truth to the102=?\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "# Load the model and tokenizer\n",
    "model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "vanilla_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Input text\n",
    "text = \"Please tell me the answer of 1+1=\"\n",
    "\n",
    "# Tokenize the input text\n",
    "inputs = tokenizer(text, return_tensors='pt', truncation=True)\n",
    "input_ids, attention_mask = inputs['input_ids'], inputs['attention_mask']\n",
    "\n",
    "# Get the logits from the model\n",
    "output = vanilla_model(input_ids, attention_mask=attention_mask)\n",
    "logits = output.logits\n",
    "\n",
    "# Get the predicted token IDs (taking the argmax to get the most likely token at each position)\n",
    "predicted_token_ids = torch.argmax(logits, dim=-1)\n",
    "\n",
    "# Decode the predicted token IDs to get the generated text\n",
    "generated_text = tokenizer.decode(predicted_token_ids[0], skip_special_tokens=True)\n",
    "\n",
    "print(generated_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.8467,  1.1656,  1.9924,  3.0213,  5.5284, -0.5593,  4.9252,  8.7389,\n",
      "         5.5826,  2.1625,  5.5826,  3.0333], grad_fn=<IndexBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for i, index in enumerate(input_ids):\n",
    "    print(tl_logits[0][i][index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# note me that truth to the102=?\n"
     ]
    }
   ],
   "source": [
    "predicted_token_ids = torch.argmax(output_logit, dim=-1)\n",
    "generated_text = tokenizer.decode(predicted_token_ids[0], skip_special_tokens=True)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[ help me the difference to this002. ['"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(tl_logits[0].argmax(dim=-1).tolist(), skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease tell me the answer of 1+1=\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m inputs \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      3\u001b[0m input_ids, attention_mask \u001b[38;5;241m=\u001b[39m inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m], inputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(input_ids\u001b[38;5;241m.\u001b[39mshape, attention_mask\u001b[38;5;241m.\u001b[39mshape)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "text = \"Please tell me the answer of 1+1=\"\n",
    "inputs = tokenizer(text, return_tensors='pt', truncation=True)\n",
    "input_ids, attention_mask = inputs['input_ids'], inputs['attention_mask']\n",
    "print(input_ids.shape, attention_mask.shape)\n",
    "output_logit = vanilla_model(input_ids, attention_mask=attention_mask, return_dict=True).logits\n",
    "tl_logits, cache = tl_model.run_with_cache(input_ids, return_type='logits')\n",
    "# assert torch.allclose(output_logit, tl_logits[0], atol=1e-05)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tl_logits' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtl_logits\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tl_logits' is not defined"
     ]
    }
   ],
   "source": [
    "tl_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_layer_normalized_result = tl_model.ln_final(cache['blocks.31.hook_resid_post'])\n",
    "assert (final_layer_normalized_result == cache['ln_final.hook_normalized']).all()\n",
    "assert (tl_model.unembed(final_layer_normalized_result) == tl_logits).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 32000])"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_logit.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 12, 4096])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vanilla_outputs['model.layers.31'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.3590e-10, 1.1438e-07, 1.2302e-04,  ..., 2.0504e-07, 5.8261e-08,\n",
       "         9.6211e-08],\n",
       "        [1.1671e-11, 4.1575e-09, 1.3147e-06,  ..., 1.7510e-09, 7.7993e-10,\n",
       "         6.6821e-09],\n",
       "        [7.2423e-11, 1.5085e-10, 1.0639e-06,  ..., 3.6436e-09, 2.6215e-09,\n",
       "         1.9634e-09],\n",
       "        ...,\n",
       "        [7.3471e-11, 6.4338e-12, 6.4296e-05,  ..., 9.2317e-10, 3.2505e-09,\n",
       "         2.3883e-08],\n",
       "        [1.8955e-10, 4.8383e-10, 1.8349e-02,  ..., 1.2916e-09, 1.0092e-08,\n",
       "         5.2991e-09],\n",
       "        [8.9344e-11, 2.0537e-09, 6.7708e-03,  ..., 1.4346e-08, 1.6418e-07,\n",
       "         1.8087e-08]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.matmul(vanilla_outputs['model.layers.31'][0], model.lm_head.weight.T)[0], dim=-1) - softmax_output_logit\n",
    "softmax_output_logit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([12, 32000])\n",
      "torch.Size([12, 32000])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([5.7971e-05, 2.3674e-05, 1.9974e-05, 3.2353e-05, 3.3633e-05, 3.8182e-05,\n",
       "        3.0333e-05, 1.9412e-05, 1.7680e-05, 1.5447e-05, 4.0036e-05, 4.4373e-05],\n",
       "       grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax_tl_logits = torch.softmax(tl_logits[0], dim=-1)\n",
    "print(softmax_tl_logits.shape)\n",
    "softmax_output_logit = torch.softmax(output_logit[0], dim=-1)\n",
    "print(softmax_output_logit.shape)\n",
    "torch.mean(torch.abs(softmax_tl_logits - softmax_output_logit), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False, False,  ..., False, False, False]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cache['blocks.11.hook_resid_post'][:, -1, :] == vanilla_outputs['model.layers.31'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "vanilla_outputs = dict()\n",
    "progress_bar = tqdm(total=len(batched_input_ids), desc=\"Processing batches\", leave=True)\n",
    "i=0\n",
    "for input_ids, attention_mask in zip(batched_input_ids, batched_attention_masks):\n",
    "    input_ids = input_ids.to(device)\n",
    "    print(f\"{input_ids.shape=}\")\n",
    "    attention_mask = attention_mask.to(device)\n",
    "    with torch.no_grad():\n",
    "        vanilla_output_logit = vanilla_model(input_ids=input_ids, attention_mask=attention_mask).logits\n",
    "    # del vanilla_output_logit\n",
    "    torch.cuda.empty_cache()\n",
    "    progress_bar.update(1)\n",
    "    break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanilla_output_logit[:, -1, :].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "watermarked_logits['cygu/llama-2-7b-logit-watermark-distill-kgw-k0-gamma0.25-delta1']['model.layers.28'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.fc = nn.Linear(dim, dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc(x)\n",
    "        out = self.relu(out)\n",
    "        out = out + x \n",
    "        return out\n",
    "\n",
    "class TransformModel(nn.Module):\n",
    "    def __init__(self, num_layers=4, input_dim=1024, hidden_dim=500, output_dim=300):\n",
    "        super(TransformModel, self).__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        self.layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "        \n",
    "        for _ in range(num_layers - 2):\n",
    "            self.layers.append(ResidualBlock(hidden_dim))\n",
    "\n",
    "        self.layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.layers)):\n",
    "            x = self.layers[i](x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Training function\n",
    "def train_model(model, dataloader, num_epochs=10, learning_rate=0.001):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "        \n",
    "        for inputs, targets in progress_bar:\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            progress_bar.set_postfix(loss=running_loss/len(dataloader))\n",
    "        \n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(dataloader):.4f}\")\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TransformModel(num_layers=4, input_dim=1024, hidden_dim=500, output_dim=300)\n",
    "train_model(model, dataloader, num_epochs=10, learning_rate=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'cygu/llama-2-7b-logit-watermark-distill-aar-k2'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "# dataset = load_dataset(\"allenai/c4\", \"realnewslike\", split=\"validation\", streaming=False)\n",
    "os.environ['HF_DATASETS_CACHE'] = '/remote-home/miintern1/watermark-learnability/experiments/.cache/huggingface/dataset/'\n",
    "dataset = load_dataset('allenai/c4', 'realnewslike', split='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "vanilla_residuals = torch.load(\"/remote-home/miintern1/watermark-learnability/data/c4/vanilla_residuals.pt\", map_location=torch.device('cpu'))\n",
    "watermarked_residuals = torch.load(\"/remote-home/miintern1/watermark-learnability/data/c4/watermark_residuals.pt\", map_location=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize task vector (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"cygu/llama-2-7b-logit-watermark-distill-kgw-k0-gamma0.25-delta1\")\n",
    "watermarked_model = AutoModelForCausalLM.from_pretrained(\"cygu/llama-2-7b-logit-watermark-distill-kgw-k0-gamma0.25-delta1\")\n",
    "vanilla_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "task_vector = TaskVector(vanilla_model, watermarked_model)\n",
    "\n",
    "coefficient = 0.1\n",
    "coefficient_watermarked_model = task_vector.apply_to(vanilla_model, scaling_coef = coefficient)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Watermarked Samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 250\n",
    "min_length = 250\n",
    "num_samples = 512\n",
    "batch_size = 1\n",
    "device = 'cpu'\n",
    "def filter_length(example):\n",
    "        return len(tokenizer(example['text'], truncation=True, max_length=max_length)[\"input_ids\"]) >= min_length\n",
    "\n",
    "def encode(examples):\n",
    "    trunc_tokens = tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    # Examples are truncated to max_length, which comprises the possible generation prompt and the text to be generated\n",
    "    examples[\"text\"] = tokenizer.batch_decode(trunc_tokens[\"input_ids\"], skip_special_tokens=True)\n",
    "    prompt = tokenizer(\n",
    "        examples[\"text\"], truncation=True, padding=True, max_length=50, return_tensors=\"pt\",\n",
    "    ).to(device)\n",
    "    examples[\"prompt_text\"] = tokenizer.batch_decode(prompt[\"input_ids\"], skip_special_tokens=True)\n",
    "    examples[\"input_ids\"] = prompt[\"input_ids\"]\n",
    "    examples[\"attention_mask\"] = prompt[\"attention_mask\"]\n",
    "    examples[\"text_completion\"] = tokenizer.batch_decode(\n",
    "        trunc_tokens[\"input_ids\"][:, 50:], skip_special_tokens=True\n",
    "    )\n",
    "    return examples\n",
    "\n",
    "dataset = dataset.filter(filter_length)\n",
    "# Set how many samples will be skipped\n",
    "dataset = dataset.map(encode, batched=True)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size)\n",
    "\n",
    "prompts = []\n",
    "human_text = []\n",
    "prompt_text = []\n",
    "full_human_text = []\n",
    "for batch in dataloader:\n",
    "    if len(human_text) >= num_samples:\n",
    "        break\n",
    "    if (type(batch[\"input_ids\"]) == list):\n",
    "        batch[\"input_ids\"] = torch.stack(batch[\"input_ids\"], dim=1).to(device)\n",
    "    if (type(batch[\"attention_mask\"]) == list):\n",
    "        batch[\"attention_mask\"] = torch.stack(batch[\"attention_mask\"], dim=1).to(device)\n",
    "    prompts.append(batch)\n",
    "    human_text.extend(batch[\"text_completion\"])\n",
    "    prompt_text.extend(batch[\"prompt_text\"])\n",
    "    full_human_text.extend(batch[\"text\"])\n",
    "human_text = human_text[:num_samples]\n",
    "prompt_text = prompt_text[:num_samples]\n",
    "full_human_text = full_human_text[:num_samples]\n",
    "raw_input = {\n",
    "    \"prompts\": prompts,\n",
    "    \"human_text\": human_text,\n",
    "    \"prompt_text\": prompt_text,\n",
    "    \"full_human_text\": full_human_text,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids, attention_mask = prompts[0][\"input_ids\"], prompts[0][\"attention_mask\"]\n",
    "print(input_ids.shape, attention_mask.shape)\n",
    "watermarked_model(input_ids=input_ids, attention_mask=attention_mask).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "DO_SAMPLE = True\n",
    "temperature=1.0\n",
    "top_p=0.9\n",
    "top_k=0\n",
    "\n",
    "\n",
    "watermarked_results = []\n",
    "for batch in tqdm(prompts[:10]):\n",
    "    with torch.no_grad():\n",
    "        watermarked_output = watermarked_model.generate(\n",
    "                            input_ids=batch[\"input_ids\"],\n",
    "                            attention_mask=batch[\"attention_mask\"],\n",
    "                            do_sample=DO_SAMPLE,\n",
    "                            min_new_tokens=200,\n",
    "                            max_new_tokens=200,\n",
    "                            temperature=temperature,\n",
    "                            top_p=top_p,\n",
    "                            top_k=top_k,\n",
    "                            pad_token_id=tokenizer.eos_token_id,\n",
    "                        )\n",
    "    watermarked_results.append(tokenizer.batch_decode(watermarked_output, skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(watermarked_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "pprint(tokenizer.decode(watermarked_output[0], skip_special_tokens=True))\n",
    "pprint(prompts[0]['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = AarWatermarkDetector(tokenizer=tokenizer, k=2, eps=1e-20)\n",
    "p_value = detector.detect(tokenizer.decode(watermarked_output[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(p_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "watermarked_model = AutoModelForCausalLM.from_pretrained(model_name, device_map = 'cpu')   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'cygu/llama-2-7b-logit-watermark-distill-aar-k3'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "watermarked_model = AutoModelForCausalLM.from_pretrained(model_name, device_map = 'cpu')   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Watermark Sample Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "watermark_config = data['samples']['llama-2-7b-logit-watermark-distill-kgw-k1-gamma0.25-delta1']['watermark_config']\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('meta-llama/Llama-2-7b-hf')\n",
    "watermark_type = WatermarkType.KGW\n",
    "detector = WatermarkDetector(\n",
    "    device=watermark_config.get(\"kgw_device\", 'cpu'),\n",
    "    tokenizer=tokenizer,\n",
    "    vocab=tokenizer.get_vocab().values(),\n",
    "    gamma=watermark_config[\"gamma\"],\n",
    "    seeding_scheme=watermark_config[\"seeding_scheme\"],\n",
    "    normalizers=[],\n",
    ")\n",
    "model_samples = data['samples']['llama-2-7b-logit-watermark-distill-kgw-k1-gamma0.25-delta1']['model_text']\n",
    "human_samples = data['samples']['llama-2-7b-logit-watermark-distill-kgw-k1-gamma0.25-delta1']['human_text']\n",
    "model_scores = []\n",
    "human_scores = []\n",
    "for sample in tqdm(model_samples):\n",
    "    score = detector.detect(sample)\n",
    "    model_scores.append(score['p_value'])\n",
    "for sample in tqdm(human_samples):\n",
    "    score = detector.detect(sample)\n",
    "    human_scores.append(score['p_value'])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "# Calculate basic statistics\n",
    "mean = statistics.mean(model_scores)\n",
    "median = statistics.median(model_scores)\n",
    "stdev = statistics.stdev(model_scores)\n",
    "minimum = min(model_scores)\n",
    "maximum = max(model_scores)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Mean: {mean}\")\n",
    "print(f\"Median: {median}\")\n",
    "print(f\"Standard Deviation: {stdev}\")\n",
    "print(f\"Minimum: {minimum}\")\n",
    "print(f\"Maximum: {maximum}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
