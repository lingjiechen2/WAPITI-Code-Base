{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /remote-home1/miintern1/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import copy\n",
    "os.environ['http_proxy'] = \"http://10.176.58.101:7890\"\n",
    "os.environ['https_proxy'] = \"http://10.176.58.101:7890\"\n",
    "os.environ['all_proxy'] = \"socks5://10.176.58.101:7891\"\n",
    "import sys\n",
    "sys.path.append(('../'))\n",
    "sys.path.append(('../../'))\n",
    "sys.path.append('/remote-home1/miintern1/watermark-learnability')\n",
    "# sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "import json\n",
    "import time\n",
    "from typing import Dict\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n",
    "from transformer_lens import HookedTransformer\n",
    "from task_vector import TaskVector\n",
    "import plotly.express as px\n",
    "from safetensors import safe_open\n",
    "\n",
    "from watermarks.kgw.watermark_processor import WatermarkDetector\n",
    "from watermarks.aar.aar_watermark import AarWatermarkDetector\n",
    "from watermarks.watermark_types import WatermarkType\n",
    "\n",
    "from huggingface_hub import login\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/remote-home1/miintern1/anaconda3/envs/watermark/lib/python3.12/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "\n",
    "\n",
    "watermarked_model_name = 'cygu/llama-2-7b-logit-watermark-distill-kgw-k0-gamma0.25-delta2'\n",
    "config = AutoConfig.from_pretrained(watermarked_model_name)\n",
    "model = AutoModelForCausalLM.from_config(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['model-00001-of-00006.safetensors', 'model-00002-of-00006.safetensors', 'model-00003-of-00006.safetensors', 'model-00004-of-00006.safetensors', 'model-00005-of-00006.safetensors', 'model-00006-of-00006.safetensors']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sharded_weights_dir = '/remote-home1/miintern1/watermark-learnability/data/refinetuning/llama-2-7b-sampling-watermark-distill-kgw-k0-gamma0.25-delta2/checkpoint-8500'\n",
    "\n",
    "# List all shard filenames\n",
    "shard_filenames = [f for f in os.listdir(sharded_weights_dir) if f.startswith('model-') and f.endswith('.safetensors')]\n",
    "\n",
    "# Sort the shards to ensure they are in the correct order\n",
    "shard_filenames.sort()\n",
    "print(shard_filenames)\n",
    "# Initialize an empty state dict\n",
    "full_state_dict = {}\n",
    "\n",
    "# Load each shard and combine them into the full state dict\n",
    "for shard_filename in shard_filenames:\n",
    "    shard_state_dict = {}\n",
    "    shard_path = os.path.join(sharded_weights_dir, shard_filename)\n",
    "    with safe_open(shard_path, framework='pt', device = 'cpu') as f:\n",
    "        for k in f.keys():\n",
    "            shard_state_dict[k] = f.get_tensor(k)\n",
    "    \n",
    "    # Merge the shard into the full state dict\n",
    "    full_state_dict.update(shard_state_dict)\n",
    "\n",
    "# Now full_state_dict contains the complete model state\n",
    "# Load this into your model\n",
    "model.load_state_dict(full_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# tensors = {}\n",
    "# with safe_open('/remote-home1/miintern1/watermark-learnability/data/refinetuning/model-00001-of-00006.safetensors', framework=\"pt\", device='cpu') as f:\n",
    "#     for k in f.keys():\n",
    "#         tensors[k] = f.get_tensor(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "068bbaa96e4d4b3cbf902802a643f076",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/1024 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3268fe564f814c3d8486d216c8606ad9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/512 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/remote-home1/miintern1/anaconda3/envs/watermark/lib/python3.12/site-packages/torch/cuda/__init__.py:141: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at /opt/conda/conda-bld/pytorch_1708025845206/work/c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfd76b6863284bb19347af5e520ca1db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10498 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "# dataset = load_dataset(\"allenai/c4\", \"realnewslike\", \"validation\")\n",
    "dataset = load_dataset(\"allenai/c4\", \"realnewslike\", split=\"validation\")\n",
    "\n",
    "max_length = 250\n",
    "min_length = 250\n",
    "num_samples = 512\n",
    "batch_size = 16\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def filter_length(example):\n",
    "        return len(tokenizer(example['text'], truncation=True, max_length=max_length)[\"input_ids\"]) >= min_length\n",
    "\n",
    "def encode(examples):\n",
    "    trunc_tokens = tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    # Examples are truncated to max_length, which comprises the possible generation prompt and the text to be generated\n",
    "    examples[\"text\"] = tokenizer.batch_decode(trunc_tokens[\"input_ids\"], skip_special_tokens=True)\n",
    "    prompt = tokenizer(\n",
    "        examples[\"text\"], truncation=True, padding=True, max_length=50, return_tensors=\"pt\",\n",
    "    ).to(device)\n",
    "    examples[\"prompt_text\"] = tokenizer.batch_decode(prompt[\"input_ids\"], skip_special_tokens=True)\n",
    "    examples[\"input_ids\"] = prompt[\"input_ids\"]\n",
    "    examples[\"attention_mask\"] = prompt[\"attention_mask\"]\n",
    "    examples[\"text_completion\"] = tokenizer.batch_decode(\n",
    "        trunc_tokens[\"input_ids\"][:, 50:], skip_special_tokens=True\n",
    "    )\n",
    "    return examples\n",
    "\n",
    "dataset = dataset.filter(filter_length)\n",
    "# Set how many samples will be skipped\n",
    "dataset = dataset.map(encode, batched=True)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size)\n",
    "\n",
    "\n",
    "\n",
    "prompts = []\n",
    "human_text = []\n",
    "prompt_text = []\n",
    "full_human_text = []\n",
    "for batch in dataloader:\n",
    "    if len(human_text) >= num_samples:\n",
    "        break\n",
    "    if (type(batch[\"input_ids\"]) == list):\n",
    "        batch[\"input_ids\"] = torch.stack(batch[\"input_ids\"], dim=1).to(device)\n",
    "    if (type(batch[\"attention_mask\"]) == list):\n",
    "        batch[\"attention_mask\"] = torch.stack(batch[\"attention_mask\"], dim=1).to(device)\n",
    "    prompts.append(batch)\n",
    "    human_text.extend(batch[\"text_completion\"])\n",
    "    prompt_text.extend(batch[\"prompt_text\"])\n",
    "    full_human_text.extend(batch[\"text\"])\n",
    "human_text = human_text[:num_samples]\n",
    "prompt_text = prompt_text[:num_samples]\n",
    "full_human_text = full_human_text[:num_samples]\n",
    "raw_input = {\n",
    "    \"prompts\": prompts,\n",
    "    \"human_text\": human_text,\n",
    "    \"prompt_text\": prompt_text,\n",
    "    \"full_human_text\": full_human_text,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DO_SAMPLE = True\n",
    "temperature=1.0\n",
    "top_p=0.9\n",
    "top_k=0\n",
    "\n",
    "\n",
    "for batch in prompts:\n",
    "    watermarked_output = model.generate(\n",
    "        input_ids=batch[\"input_ids\"],\n",
    "                            attention_mask=batch[\"attention_mask\"],\n",
    "                            do_sample=DO_SAMPLE,\n",
    "                            min_new_tokens=200,\n",
    "                            max_new_tokens=200,\n",
    "                            temperature=temperature,\n",
    "                            top_p=top_p,\n",
    "                            top_k=top_k,\n",
    "                            pad_token_id=tokenizer.eos_token_id,\n",
    "                        )\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Cluster comprises IBM\\'s Opteron-based eServer 325 server and systems management software and storage devices that can run Linux and Windows operating systems.\\nIBM on Tuesday announced a prepackaged and pre-integrated set of servers and storage aimed at making it easier to build Linux-powered data centers. The offering provides a managed and grid-ready, dual-operating-system offering that comes with built-in storage and security options.\\nThe offering comes as IBM works to boost its entry in the Linux server market, as Hewlett-Packard takes top spots and Dell and HCL Systems promise to ramp up Linux servers and services.\\nThe LinuxONE offering is backed by IBM Business Services\\' software services group and will be sold by IBM and by Business Services directly to clients in its open-cloud marketplaces.\\nThe Cluster is comprised of IBM\\'s Opteron-based eServer 325 server and storage devices that can run both Windows and Linux operating systems. That offering will be targeted to mid-size companies and start-ups and provide \"grid-ready\" capabilities to access and pool both local', 'Belying expectations, Prasar Bharti has earned only Rs 58.19 crore (Rs 581.9 million) as revenue during the Commonwealth Games last month.\\nThe gross revenue of Rs 333.35 crore and expense of Rs 338.34 crore earned and incurred by DD - India for producing and broadcasting the Commonweath Games (CWG) with staff support and working expense of Rs 333.35 crore and grant-in-aid of Rs 52.33 crore.\\nA sum of Rs 58.19 crore has also been included in Other Services – one of the non-air revenue sources – for offering community radio, television and audio services apart from multi-media and mobile radio respectively.\\n“This is an amount of money lesser than what was spent by the broadcaster for the 2006 world cup in Ghana and South Africa,” DG-Prereplaces M S Sodhi told PTI.\\nAt the time of the', 'Marketers are employed in the public sector as well as the private sector.\\nMost people think of marketing as a strictly private sector activity, but the reality is people with sales and marketing backgrounds are hired by government agencies to make citizens aware of public programs and services. Within the public sector, marketers often provide research and analysis to guide decision-making by agency leaders, and they actively support campaigns to promote agency programs and services.\\nMarketing professionals often build their careers by switching agencies and locations – sometimes every couple of years – rather than by working their way up to increasing responsibility and pay at a single agency. Government jobs in marketing and sales are often advertised with temporary time periods and generally start at the entry-level pay grade of GS-5.\\nJob opportunities in the public sector will generally be found by looking at agency job boards and sometimes researching current events. Some agencies will also allow you to pursue the standard route to federal employment and will post openings to USAJOBS.gov and open the positions to a national competition.\\nMarketing careers may also be found in legislative agencies and advoc', 'MIDDLETOWN, NJ - The Middletown Township Public Library and the Township of Middletown are pleased to announce the appointment of Kim Rinaldi as Provisional Library Director, beginning on February 10. All appointments are subject to county and state approval.\\n\\nRinaldi currently serves as Business Manager and Head of Business and Economic Services at Middletown Township. Prior to Middletown, she served as Business Manager and Head of Business and Economic Services at Freehold Township and Business Manager and Head of Business and Economic Services at Bordentown Township and Atlantic City. Ms. Rinaldi also served as a Business Manager at Atlantic City and Ewing Township, and Business and Marketing Manager for Philmont Scout Ranch and Chief Financial Officer for the Ewing Township Business Improvement District. Ms. Rinaldi holds a Master of Business Administration and a Master of Business Administration in Executive Management and an MBA in Strategic Marketing from Drexel University and a Master of Business Administration from Rutgers University-Camden.\\n\\n“As we set out to build our library of the future', 'An eagerly awaited report from the World Health Organization (WHO) states that processed meats such as bacon and sausages cause cancer, and red meat likely does so too.\\nThe France-based International Agency for Research on Cancer (IARC), which drew on decades of research by scientists worldwide to make its assessment, classified processed meat in the group with “agents that are probably carcinogenic to humans” – along with tobacco, alcohol, and asbestos – and classified red meat to be “probably carcinogenic to humans” – along with alcohol and solar radiation.\\n“This is great news,” Meagan Hirsch, senior staff attorney with the Natural Resources Defense Council, tells FDAnews. “At long last, scientific evidence backs what consumers and advocates have been telling them – that eating meat and dairy puts consumers at risk of developing cancer, and so should be a food to be consumed sparingly or in moderation.”\\nRed meat consumed by humans includes both beef and pork. Meat also may refer to poultry, wild game meat,', 'Was it a surprise to you that you were given the arts and culture position?\\nNo, there is no surprise when you are a cadre of the ANC because you are deployed anywhere. You are given a five-year contract to do a job. So, there is no shock that they had put you there.\\nWas there resistance to you taking that post and how did you cope with that?\\nNo, I had no choice. I got an indication of what was to happen with comrades serving in institutions of higher education. Afterwards, some comrades served in institutions of higher education and had to be dumped. Even I had to be dumped later.\\nYou are 83 years old and served as ANC chief whip of the National Council of Provinces (NCOP). What did you say to yourself, seeing that they don’t mind using elders of the ANC to serve in the NCOP?\\nThey have no choice to put somebody to serve in Parliament. As the ANC has got no Members of Parliament in Parliament, it has to fill that gap and put elders to serve there. It means to try and engage them to be', 'WASHINGTON (Reuters) - U.S. Treasury Secretary Jack Lew on Sunday warned Congress against manufacturing a crisis over federal spending in the months ahead, as looming deadlines set the stage for a renewed budget clash.\\nLew said Sunday he had spoken to congressional leaders and \"gotten nothing but reassurance and promise\" that there will be no showdown with President Barack Obama before the end of the year.\\nLew predicted there will be no default on Sept. 30 when funding for the government runs out and no \"technical default\" of the U.S. government when the statutory borrowing limit is reached that same day.\\nStill, Lew warned of the risk of a showdown with Congress before Dec. 31, when money runs out for agency operations and raising the statutory debt limit to avoid breaching it again.\\nOn Sunday, he cited the fact that much of Congress will be working with no hearings and no time to develop real legislation. \"The last thing we should do is to manufacture a crisis. I think a great deal of damage', \"Im definetely with some of the above posts when saying that they left the serious open ended. I knew up until then that this was supposed to be the final season but after seeing the last episode, I could see another see another season. I just think that they won't do it for some money reasons and possibly having to give up the current show runner's agent that got them to NY to show them how great tv is.\\nBut Im with some of the posts and didnt see some of the real jokes before seeing them again during the credits. Im referring to the ones before and after Season 3 and how it was almost an ending to see those at the end of the series with some comment and that having the credits and music be very melodramatic. I thought it was intended to be some laugh-in-my-face-funny but to some of the jokes and the show (and probably also to be intended to be some pathos and melodramatic nature) did to me come across as too melo-dramatic and almost to be too self aware and that's where some of the humor became quite humorless and forced and hard to see jokes in\", \"In case you missed it, the initials ICYMI stand for the first five words of this sentence. In the event you have, indeed, missed it (ITEYH, I, MI has not taken off yet for some reason), here's the actual thing.\\n\\nIn fact, to be frank, it's surprising how much of what happens with humans online is brought to you by the Cliff Notes. Google and Facebook are nothing without them. And yet, both are considered to be the mnemonic orthopedic consortium of knowledge by both mnemonic orthopedic consumers and orthopedic mnemonic consortium.\\n\\nWhile everyone likes to accuse Google of helping to develop a cliff-biter complex among students, what irks the orthodontists the most is how much of our time is spent trying to get them to grasp the concept of mnemonic orthopedic consortium of knowledge before we've had a chance to see them and how much time we don't spend on them during and after treatments and to educate them on mnemonic orthodontic consortium of knowledge and mnemonic\", \"Widow Of Pulse Nightclub Gunman AcquittedNoor Salman was found not guilty of charges of obstruction and providing material support to a terrorist organization.\\n'It Still Hurts, It's Still Very Raw': OSU Terror Victim's Mom Meets With President-Elect Donald TrumpKim Parsons says she prayed with Trump on Tuesday and thanked him for promising to donate $1 million to the families of the massacre.\\n'There's A Mess And It's Gotta Get Better': Trump Meets With Attack Victims, FamiliesTuesday morning Donald Trump met with the victims of the massacre and with the families of the slain officers.\\nFBI Debunks Terror Connection To Ohio State AttackStanfield and Gendron did not have any ties to terrorist organizations.\\nVeterans Day Weekend: Best Way To Say Thank You\\nGOP House To Move To Vote To Repeal Obama-Backed Regulations\\nWWII Hero Who Survived 'Bataan Death March' Dies\\nThe Papers: Trump Threatens\", 'NEW YORK — Television retailer QVC has made aggressive plans to keep shoppers watching — instead of mall-hopping — on Black Friday, an event it has traditionally ignored.\\nThe leading network for live-streamed video-on-demand, QVC will put an emphasis on both hosting live events and offering flexible delivery time frames to try to lure holiday shoppers away from traditional stores and give them more control over how and when to shop.\\nQVC will be offering free shipping and pay-later financing for some of its top-sellers and will be hosting nine \"live events\" featuring high-profile guests such as talk-show star Dr. Mehmet Oz and YouTube personality and actress Lele Pons.\\nThey will be taking place between November 20 and December 23 and will be hosted by QVC\\'s top on-air talent to try to mimic the atmosphere of in-store shopping.\\nAccording to Deborah M. Fackler, chief customer officer and executive vice president of customer care and marketing at QVC Inc., much of QVC\\'s top', 'She was academically brilliant, the shining light of a community that has seen many potentially intelligent girls given no sniff at education and many dropping out along the way.\\nThen she got a lifeline. UCC had only ten people with whom to start the MBA, and picked her for her academic credentials and business experience, and there were no women in her peer group, and on and on – and there are now 16 in her peer group, and with much to be done for business and industry, she may be of huge benefit to the republic.\\nShe has had to give up some of her normal pursuits – no more yoga, no socialising – and will be working hard to retain her crown.\\nWe will keep telling you of some of our county MBA students and what they intend to do – some of them will be of great benefit to Ireland. Some will be successful and others will be fine CEOs and some will be not so successful and fine and also some may be really very successful.\\nLots of drama and fun to be had here and we will be following and telling you how they cope and coped and how they fare. These ten have started something and will', 'Occasional diarrhea is a common occurrence. Most people will experience an episode of diarrhea at least once or twice a year that will disappear in a couple of days.\\nLuckily, there are many foods and natural remedies that can stop diarrhea and keep you comfortable and hydrated. Eating certain foods may also provide nutrition to help your body cope with an illness.\\nThis article will guide you to how to prevent and cure your diarrhea with some foods that will keep you hydrated and nutritioned and will also be of great help to you and keep away from some nasty side-effects of taking heavy and expensive drugs to cure them. And be assured to be back to normal state of health in no time using the techniques and tips described here.\\nThis article will also guide you to how to prevent and cure your diarrhea with some foods that will keep you hydrated and nutritioned and will also be of great help to you and keep away from some nasty side-effects of taking heavy and expensive drugs to cure them. And be', 'NEW YORK, Sept. 29, 2015 /PRNewswire/ -- As previously announced, WebMD Health Corp. (NASDAQ: WBMD) will be holding its Annual Meeting of Stockholders on Thursday, October 1, 2015. The Company wishes to notify stockholders of the location of the annual meeting and to make some statements of fact to be considered by stockholders.\\nAs previously announced, the annual meeting of stockholders will be held at 3333 Steeles Avenue West, Markham, Ontario L3R 4L9 on Thursday, October 1, 2015 at 10:00 AM ET (8:00 AM PT). The Company also wishes to notify stockholders of the location of the annual meeting and to make some statements of fact to be considered by stockholders.\\nAs noted in the proxy statement and the Letter to Stockholders, the annual meeting of stockholders will be held at 3333 Steeles Avenue West, Markham, Ontario L3R 4L9', 'SocialDevCamp Chicago is a weekend long conference and hackathon about the social web. It’s the event for social app developers, mobile developers and designers. SocialDevCamp is a weekend of discussions on social platforms and how to build successful applications on them. We will be spending three hours of each day creating and demonstrating social apps with support from subject matter experts and industry leaders. We will also be hacking together to improve and innovate some of the social web’s current short comings.\\n\\nThis will be the fourth annual conferences and the first time Social Dev Camp has been hosted in Chicago. To accommodate the jump in attendees, the format will be broken into three parts. The first portion of each day will be about design and choosing the right social platform to build an app. In the afternoon, there will be multiple hands-on mobile and social app building workshops using the chosen platform. The final portion of each day will be presentations and talks of the morning and presentations and open-forum conversations with industry leaders and subject matter experts.\\n\\n## Event Overview\\n\\n* Registration and Q&A with industry leaders', 'Record low and negative interest rates in developed economies are fueling a ravenous search for yield that is forcing pension funds and other conservative institutional investors out of their comfort zones and into risky emerging markets, according to a survey of institutions by consultancy firm Willis Towers Watson. Institutions are looking to expand their investment portfolios to include equities and hedge funds to get the higher returns investors demand and to hedge against future falls in bond yields and low interest rates.\\nThree quarters of institutions surveyed by Willis Towers Watson believe there will be no let-up in the low-interest rate environment and almost three quarters will pursue alternatives to traditional investments.\\nFederal Reserve Bank of New York President William Dudley warned that the US economy faces bigger risks of weakness due to low growth and low inflation. Dudley warned of deflationary dangers and said there was no \"immediate need to act\". But Dudley\\'s concerns echoed those of his US counterpart, Janet Yellen, who expressed fears of a \"dangerous cycle of weak growth and low inflation']\n",
      "[0.998490172015865, 0.8295082903480915, 0.3007857466297952, 0.9783372996072881, 0.7430777072073017, 0.6390284549131275, 0.25247519770310706, 0.20873131758043478, 0.012923370216803726, 0.10740233874027198, 0.9201349965393348, 0.5243614102901001, 0.0011021026595585362, 0.00893872321330345, 0.25247519770310706, 0.9689673907368748]\n"
     ]
    }
   ],
   "source": [
    "watermarked_text = tokenizer.batch_decode(watermarked_output, skip_special_tokens=True)\n",
    "print(watermarked_text)\n",
    "watermark_configs = {\n",
    "     \"cygu/llama-2-7b-logit-watermark-distill-kgw-k0-gamma0.25-delta1\":{\"type\": \"kgw\", \"k\": 0, \"gamma\": 0.25, \"delta\": 1.0, \"seeding_scheme\": \"simple_0\", \"kgw_device\": \"cpu\"},\n",
    "     \"cygu/llama-2-7b-logit-watermark-distill-kgw-k0-gamma0.25-delta2\":{\"type\": \"kgw\", \"k\": 0, \"gamma\": 0.25, \"delta\": 2.0, \"seeding_scheme\": \"simple_0\", \"kgw_device\": \"cpu\"},\n",
    "     \"cygu/llama-2-7b-logit-watermark-distill-kgw-k1-gamma0.25-delta1\":{\"type\": \"kgw\", \"k\": 1, \"gamma\": 0.25, \"delta\": 1.0, \"seeding_scheme\": \"simple_1\", \"kgw_device\": \"cpu\"},\n",
    "     \"cygu/llama-2-7b-logit-watermark-distill-kgw-k1-gamma0.25-delta2\":{\"type\": \"kgw\", \"k\": 1, \"gamma\": 0.25, \"delta\": 2.0, \"seeding_scheme\": \"simple_1\", \"kgw_device\": \"cpu\"},\n",
    "     \"cygu/llama-2-7b-logit-watermark-distill-kgw-k2-gamma0.25-delta2\":{\"type\": \"kgw\", \"k\": 2, \"gamma\": 0.25, \"delta\": 2.0, \"seeding_scheme\": \"simple_2\", \"kgw_device\": \"cpu\"},\n",
    "}\n",
    "watermarked_model_name = 'cygu/llama-2-7b-logit-watermark-distill-kgw-k0-gamma0.25-delta2'\n",
    "watermark_config = watermark_configs[watermarked_model_name]\n",
    "\n",
    "p_values = []\n",
    "for sample in watermarked_text:\n",
    "     detector = WatermarkDetector(\n",
    "                              device=watermark_config.get(\"kgw_device\", 'cpu'),\n",
    "                              tokenizer=tokenizer,\n",
    "                              vocab=tokenizer.get_vocab().values(),\n",
    "                              gamma=watermark_config[\"gamma\"],\n",
    "                              seeding_scheme=watermark_config[\"seeding_scheme\"],\n",
    "                              normalizers=[]\n",
    "                         )\n",
    "     p_values.append(detector.detect(sample)['p_value'])\n",
    "print(p_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/remote-home1/miintern1/anaconda3/envs/watermark/lib/python3.12/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cda75db77b864e3286b20418162fd471",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.3\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"mistralai/Mistral-7B-v0.3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/remote-home1/miintern1/anaconda3/envs/watermark/lib/python3.12/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4dcc34dd91514538bb138eabb096d461",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "llama_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32000, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m在当前单元格或上一个单元格中执行代码时 Kernel 崩溃。\n",
      "\u001b[1;31m请查看单元格中的代码，以确定故障的可能原因。\n",
      "\u001b[1;31m单击<a href='https://aka.ms/vscodeJupyterKernelCrash'>此处</a>了解详细信息。\n",
      "\u001b[1;31m有关更多详细信息，请查看 Jupyter <a href='command:jupyter.viewOutput'>log</a>。"
     ]
    }
   ],
   "source": [
    "llama_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32768, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x MistralDecoderLayer(\n",
       "        (self_attn): MistralSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): MistralRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm()\n",
       "        (post_attention_layernorm): MistralRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32768, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/remote-home1/miintern1/watermark-learnability/data/refinetuning/watermark_ability_decay_step_200.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/remote-home1/miintern1/watermark-learnability/data/refinetuning/watermark_ability_decay_step_200.json\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m      2\u001b[0m     data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(data)\n",
      "File \u001b[0;32m~/anaconda3/envs/watermark/lib/python3.12/site-packages/IPython/core/interactiveshell.py:324\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m {\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m}:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIPython won\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt let you open fd=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m by default \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    320\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    321\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou can use builtins\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m open.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    322\u001b[0m     )\n\u001b[0;32m--> 324\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mio_open\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/remote-home1/miintern1/watermark-learnability/data/refinetuning/watermark_ability_decay_step_200.json'"
     ]
    }
   ],
   "source": [
    "with open('/remote-home1/miintern1/watermark-learnability/data/refinetuning/watermark_ability_decay_step_200.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'strip'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdetector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwatermarked_text\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/watermark-learnability/experiments/../watermarks/kgw/watermark_processor.py:601\u001b[0m, in \u001b[0;36mWatermarkDetector.detect\u001b[0;34m(self, text, tokenized_text, window_size, window_stride, return_prediction, return_scores, z_threshold, convert_to_float, **kwargs)\u001b[0m\n\u001b[1;32m    599\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalizers) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    600\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mText after normalization:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 601\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m text \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrip\u001b[49m() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    602\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSkipping text because it is empty or only whitespace: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {}\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'strip'"
     ]
    }
   ],
   "source": [
    "detector.detect(watermarked_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "watermark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
