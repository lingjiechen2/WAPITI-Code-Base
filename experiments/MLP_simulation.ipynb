{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import os\n",
    "import copy\n",
    "os.environ['http_proxy'] = \"http://10.176.52.116:7890\"\n",
    "os.environ['https_proxy'] = \"http://10.176.52.116:7890\"\n",
    "os.environ['all_proxy'] = \"socks5://10.176.52.116:7891\"\n",
    "import sys\n",
    "sys.path.append(('../'))\n",
    "sys.path.append(('../../'))\n",
    "# sys.path.append(os.path.abspath(os.path.join(os.getcwd(), '..')))\n",
    "import json\n",
    "from typing import Dict\n",
    "import torch\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, set_seed\n",
    "from task_vector import TaskVector\n",
    "\n",
    "from watermarks.kgw.watermark_processor import WatermarkDetector\n",
    "from watermarks.aar.aar_watermark import AarWatermarkDetector\n",
    "from watermarks.watermark_types import WatermarkType\n",
    "import logging\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.fc = nn.Linear(dim, dim)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc(x)\n",
    "        out = self.relu(out)\n",
    "        out = out + x \n",
    "        return out\n",
    "\n",
    "class TransformModel(nn.Module):\n",
    "    def __init__(self, num_layers=4, input_dim=1024, hidden_dim=500, output_dim=300):\n",
    "        super(TransformModel, self).__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        self.layers.append(nn.Linear(input_dim, hidden_dim))\n",
    "        \n",
    "        for _ in range(num_layers - 2):\n",
    "            self.layers.append(ResidualBlock(hidden_dim))\n",
    "\n",
    "        self.layers.append(nn.Linear(hidden_dim, output_dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        for i in range(len(self.layers)):\n",
    "            x = self.layers[i](x)\n",
    "        return x\n",
    "    \n",
    "save_path = '/remote-home/miintern1/watermark-learnability/data/model_weights_2/model.layers.31_kgw-k0-gamma0.25-delta1_hidden_dim_8192_num_layers_1_best_model.pt'\n",
    "hyperparameter_dict = {\n",
    "    'num_layers': 4,\n",
    "    'input_dim': 4096,\n",
    "    'hidden_dim': 500,\n",
    "    'output_dim': 300\n",
    "}\n",
    "loaded_dict = torch.load(save_path, map_location='cpu')\n",
    "model_state_dict = loaded_dict[\"model_state_dict\"]\n",
    "loaded_hyperparameters = loaded_dict[\"hyperparameters\"]\n",
    "\n",
    "loaded_num_layers = loaded_hyperparameters[\"num_layers\"]\n",
    "loaded_hidden_dim = loaded_hyperparameters[\"hidden_dim\"]\n",
    "loaded_learning_rate = loaded_hyperparameters[\"learning_rate\"]\n",
    "loaded_num_epochs = loaded_hyperparameters[\"num_epochs\"]\n",
    "\n",
    "loaded_model = TransformModel(\n",
    "    num_layers=loaded_num_layers,\n",
    "    input_dim=4096,  # Change this as needed\n",
    "    hidden_dim=loaded_hidden_dim,\n",
    "    output_dim=4096  # Change this as needed\n",
    ")\n",
    "\n",
    "# Load the model state dictionary into the initialized model\n",
    "loaded_model.load_state_dict(model_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generation imitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TransformModel(\n",
       "  (layers): ModuleList(\n",
       "    (0): Linear(in_features=4096, out_features=8192, bias=True)\n",
       "    (1): Linear(in_features=8192, out_features=4096, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loaded_model.half()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.003902912139892578,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c064db68a4ea467ab966caad11f4ace8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Function to get layer output and substitute with transformed activation\n",
    "def substitute_layer_hook(layer_name, mlp_model=None):\n",
    "    def hook_fn(module, input, output):\n",
    "        try:\n",
    "            # Store the original output\n",
    "            if layer_name not in vanilla_outputs:\n",
    "                vanilla_outputs[layer_name] = []\n",
    "            activation = output[0][0]\n",
    "            # Debug: Print activation dtype\n",
    "            print(f\"Inside hook: {layer_name}, {activation.dtype=}\")\n",
    "            vanilla_outputs[layer_name].append(output)\n",
    "\n",
    "            # If mlp_model is provided, use it to transform the output\n",
    "            if mlp_model is not None:\n",
    "                print(f\"Original: {activation.dtype=}\")\n",
    "                print(f\"Transforming activation with MLP model\")\n",
    "                # with torch.no_grad():\n",
    "                # Ensure the output is on the same device as the mlp_model\n",
    "                activation = output[0]\n",
    "                transformed_output = mlp_model(activation.cpu()).to(torch.float16).to(output[0].device)\n",
    "                print(f\"Transformed: {transformed_output.dtype=}\")\n",
    "                modified_output = (transformed_output, *output[1:])\n",
    "                if layer_name not in modified_outputs:\n",
    "                    modified_outputs[layer_name] = []\n",
    "                modified_outputs[layer_name].append(modified_output[0].cpu())\n",
    "                return modified_output\n",
    "            else:\n",
    "                print(f\"Original: {activation.dtype=} without transformation\")\n",
    "                return output\n",
    "        except Exception as e:\n",
    "            print(f\"Error in hook {layer_name}: {e}\")\n",
    "            return output\n",
    "    return hook_fn\n",
    "\n",
    "\n",
    "# Example of usage\n",
    "vanilla_outputs = {}\n",
    "modified_outputs = {}\n",
    "watermark_outputs = {}\n",
    "hook_layer = [31]  # Specify the layers to hook\n",
    "\n",
    "# Load your vanilla model\n",
    "hooked_model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "hooked_model = AutoModelForCausalLM.from_pretrained(hooked_model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(hooked_model_name)\n",
    "hooked_model.half()\n",
    " \n",
    "# Dictionary to store hook handles\n",
    "hook_handles = {}\n",
    "\n",
    "# Register hooks to the specified layers\n",
    "for i in hook_layer:\n",
    "    layer_name = f\"model.layers.{i}\"\n",
    "    layer = dict([*hooked_model.named_modules()])[layer_name]\n",
    "    hook_handle = layer.register_forward_hook(substitute_layer_hook(layer_name, loaded_model))\n",
    "    hook_handles[layer_name] = hook_handle\n",
    "\n",
    "\n",
    "# Function to remove hooks\n",
    "def remove_hooks(hook_handles):\n",
    "    for handle in hook_handles.values():\n",
    "        handle.remove()\n",
    "\n",
    "# At some point in your code, when you want to remove the hooks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inside hook: model.layers.31, activation.dtype=torch.float16\n",
      "Original: activation.dtype=torch.float16\n",
      "Transforming activation with MLP model\n",
      "Transformed: transformed_output.dtype=torch.float16\n"
     ]
    }
   ],
   "source": [
    "# Input text\n",
    "# remove_hooks(hook_handles)\n",
    "text = \"Please introduce yourself to me:\"\n",
    "inputs = tokenizer(text, return_tensors='pt', truncation=True)\n",
    "input_ids, attention_mask = inputs['input_ids'], inputs['attention_mask']\n",
    "\n",
    "\n",
    "# Run the model to activate hooks\n",
    "with torch.no_grad():\n",
    "    hooked_logit = hooked_model(input_ids, attention_mask=attention_mask, return_dict=True).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vanilla_outputs['model.layers.31'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_layer_output_hook(layer_name):\n",
    "    def hook_fn(module, input, output):\n",
    "        # print(f\"{output[0].shape=}\")\n",
    "        # print(f\"{output[1]=}\")\n",
    "        if layer_name not in watermark_outputs:\n",
    "            watermark_outputs[layer_name] = []\n",
    "        watermark_outputs[layer_name].append(output[0][:, -1, :].cpu())\n",
    "    return hook_fn\n",
    "\n",
    "watermark_model_name = \"cygu/llama-2-7b-logit-watermark-distill-kgw-k0-gamma0.25-delta1\"\n",
    "watermarked_model = AutoModelForCausalLM.from_pretrained(watermark_model_name)\n",
    "\n",
    "for i in hook_layer:\n",
    "    layer_name = f\"model.layers.{i}\"\n",
    "    layer = dict([*watermarked_model.named_modules()])[layer_name]\n",
    "    layer.register_forward_hook(get_layer_output_hook(layer_name))\n",
    "\n",
    "\n",
    "vanilla_model_name = \"meta-llama/Llama-2-7b-hf\"\n",
    "vanilla_model = AutoModelForCausalLM.from_pretrained(vanilla_model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(vanilla_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/json": {
       "ascii": false,
       "bar_format": null,
       "colour": null,
       "elapsed": 0.0035924911499023438,
       "initial": 0,
       "n": 0,
       "ncols": null,
       "nrows": null,
       "postfix": null,
       "prefix": "Loading checkpoint shards",
       "rate": null,
       "total": 2,
       "unit": "it",
       "unit_divisor": 1000,
       "unit_scale": false
      },
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "984baf02edf541c98d0f2179e256a2bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    }
   ],
   "source": [
    "# Input text\n",
    "text = \"Please introduce yourself to me:\"\n",
    "inputs = tokenizer(text, return_tensors='pt', truncation=True)\n",
    "input_ids, attention_mask = inputs['input_ids'], inputs['attention_mask']\n",
    "\n",
    "\n",
    "# Run the model to activate hooks\n",
    "with torch.no_grad():\n",
    "    hooked_logit = hooked_model(input_ids, attention_mask=attention_mask, return_dict=True).logits\n",
    "\n",
    "# Run the model to activate hooks\n",
    "with torch.no_grad():\n",
    "    output_logit = vanilla_model(input_ids, attention_mask=attention_mask, return_dict=True).logits\n",
    "\n",
    "# Run the model to activate hooks\n",
    "with torch.no_grad():\n",
    "    watermark_logit = watermarked_model(input_ids, attention_mask=attention_mask, return_dict=True).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please introduce yourself to me:\n",
      "[name], a person who is not a student, a teacher, an employee or a parent\n",
      "A good-looking woman\n",
      "Please explain how you would answer the questions differently if you were an employer:\n",
      "A better-looking woman with bigger breasts\n",
      "The answers are the same. If you think an answer is too personal for a parent to ask you, the answer is probably too personal for a teacher or student to ask you. And the same\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Please introduce yourself to me:\n",
      "Hey, I’m Siobhan! I’m a 17-year-old photographer who recently graduated high school, and I’m currently studying photography at the University of Lincoln.\n",
      "Who or what inspires you and why?\n",
      "There’s a lot of things that inspire me. A lot of photographers inspire me, artists like Diane Arbus and Nan Goldin really inspired me as a younger\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Please introduce yourself to me:\n",
      "My name is Gina Kozar. I am the proud mom of a beautiful 20 year old daughter that I love with all my heart. I have a BFA degree in Sculpture and have a strong passion for all art and crafts. I have been making jewelry for well over 15 years and in the last year have had the opportunity to work with the most gorgeous metals.\n",
      "I use\n",
      "----------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "substututed_output = hooked_model.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    do_sample=True,  # or False, depending on your requirement\n",
    "    min_length=50,\n",
    "    max_length=100,\n",
    "    temperature=1.0,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "substututed_text = tokenizer.decode(substututed_output[0], skip_special_tokens=True)\n",
    "print(substututed_text)\n",
    "print('-'*100)\n",
    "\n",
    "generated_output = vanilla_model.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    do_sample=True,  # or False, depending on your requirement\n",
    "    min_length=50,\n",
    "    max_length=100,\n",
    "    temperature=1.0,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "generated_text = tokenizer.decode(generated_output[0], skip_special_tokens=True)\n",
    "print(generated_text)\n",
    "print('-'*100)\n",
    "\n",
    "watermarked_output = watermarked_model.generate(\n",
    "    input_ids=input_ids,\n",
    "    attention_mask=attention_mask,\n",
    "    do_sample=True,  # or False, depending on your requirement\n",
    "    min_length=50,\n",
    "    max_length=100,\n",
    "    temperature=1.0,\n",
    "    top_p=0.95,\n",
    "    top_k=50,\n",
    "    pad_token_id=tokenizer.eos_token_id,\n",
    ")\n",
    "watermarked_text = tokenizer.decode(watermarked_output[0], skip_special_tokens=True)\n",
    "print(watermarked_text)\n",
    "print('-'*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Watermarked score: 0.011818979440471876\n",
      "Substituted score: 0.08319032287015378\n",
      "Generated score: 0.49219720463171884\n"
     ]
    }
   ],
   "source": [
    "detector = WatermarkDetector(\n",
    "                            device='cpu',\n",
    "                            tokenizer=tokenizer,\n",
    "                            vocab=tokenizer.get_vocab().values(),\n",
    "                            gamma=0.25,\n",
    "                            seeding_scheme='simple_0',\n",
    "                            normalizers=[],\n",
    "                        )\n",
    "waterk_score = detector.detect(watermarked_text)\n",
    "substitution_score = detector.detect(substututed_text)\n",
    "generated_score = detector.detect(generated_text)\n",
    "\n",
    "print(f\"Watermarked score: {waterk_score['p_value']}\")\n",
    "print(f\"Substituted score: {substitution_score['p_value']}\")\n",
    "print(f\"Generated score: {generated_score['p_value']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "watermarked_model = AutoModelForCausalLM.from_pretrained(\n",
    "    local_model_path,\n",
    "    cache_dir=\"/path/to/cache/dir\",  # Optional: specify a cache directory\n",
    "    force_download=False,            # Optional: force a fresh download\n",
    "    local_files_only=True            # Ensure it only looks locally\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 250\n",
    "min_length = 250\n",
    "num_samples = 512\n",
    "batch_size = 1\n",
    "save_path = ''\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "set_seed(42)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def filter_length(example):\n",
    "    return len(tokenizer(example['text'], truncation=True, max_length=max_length)[\"input_ids\"]) >= min_length\n",
    "\n",
    "def encode(examples):\n",
    "    trunc_tokens = tokenizer(\n",
    "        examples['text'],\n",
    "        truncation=True,\n",
    "        padding=True,\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\"\n",
    "    ).to(device)\n",
    "    # Examples are truncated to max_length, which comprises the possible generation prompt and the text to be generated\n",
    "    examples[\"text\"] = tokenizer.batch_decode(trunc_tokens[\"input_ids\"], skip_special_tokens=True)\n",
    "    prompt = tokenizer(\n",
    "        examples[\"text\"], truncation=True, padding=True, max_length=50, return_tensors=\"pt\",\n",
    "    ).to(device)\n",
    "    examples[\"prompt_text\"] = tokenizer.batch_decode(prompt[\"input_ids\"], skip_special_tokens=True)\n",
    "    examples[\"input_ids\"] = prompt[\"input_ids\"]\n",
    "    examples[\"attention_mask\"] = prompt[\"attention_mask\"]\n",
    "    examples[\"text_completion\"] = tokenizer.batch_decode(\n",
    "        trunc_tokens[\"input_ids\"][:, 50:], skip_special_tokens=True\n",
    "    )\n",
    "    return examples\n",
    "\n",
    "dataset = dataset.filter(filter_length)\n",
    "# Set how many samples will be skipped\n",
    "dataset = dataset.map(encode, batched=True)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size)\n",
    "\n",
    "prompts = []\n",
    "human_text = []\n",
    "prompt_text = []\n",
    "full_human_text = []\n",
    "for batch in dataloader:\n",
    "    if len(human_text) >= num_samples:\n",
    "        break\n",
    "    if (type(batch[\"input_ids\"]) == list):\n",
    "        batch[\"input_ids\"] = torch.stack(batch[\"input_ids\"], dim=1).to(device)\n",
    "    if (type(batch[\"attention_mask\"]) == list):\n",
    "        batch[\"attention_mask\"] = torch.stack(batch[\"attention_mask\"], dim=1).to(device)\n",
    "    prompts.append(batch)\n",
    "    human_text.extend(batch[\"text_completion\"])\n",
    "    prompt_text.extend(batch[\"prompt_text\"])\n",
    "    full_human_text.extend(batch[\"text\"])\n",
    "human_text = human_text[:num_samples]\n",
    "prompt_text = prompt_text[:num_samples]\n",
    "full_human_text = full_human_text[:num_samples]\n",
    "raw_input = {\n",
    "    \"prompts\": prompts,\n",
    "    \"human_text\": human_text,\n",
    "    \"prompt_text\": prompt_text,\n",
    "    \"full_human_text\": full_human_text,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_p_value(samples, detector, type='kgw'):\n",
    "    score_list = []\n",
    "    for s in tqdm(samples):\n",
    "        score = detector.detect(s)\n",
    "        score_list.append(score['p_value']) if type=='kgw' else score_list.append(score)\n",
    "    return score_list\n",
    "\n",
    "\n",
    "def compute_seq_rep_n(samples, tokenizer, n=3):\n",
    "    \"\"\"compute seq-rep-n metric\"\"\"\n",
    "    n_gram_reps = []\n",
    "    \n",
    "    for s in samples:\n",
    "        n_grams = []\n",
    "        tokens = tokenizer(s, add_special_tokens=False).input_ids\n",
    "        for i in range(len(tokens)):\n",
    "            if i <= len(tokens) - n:\n",
    "                n_grams.append(tuple(tokens[i:i + n]))\n",
    "                    \n",
    "        rep = 1 - len(set(n_grams)) / len(n_grams)\n",
    "        n_gram_reps.append(rep)\n",
    "            \n",
    "    median_rep = np.median(n_gram_reps)\n",
    "    mean_rep = np.mean(n_gram_reps)\n",
    "    return {\n",
    "        f\"median_seq_rep_{n}\": median_rep,\n",
    "        f\"mean_seq_rep_{n}\": mean_rep,\n",
    "        f\"list_seq_rep_{n}\": n_gram_reps,\n",
    "    }\n",
    "\n",
    "\n",
    "def compute_total_rep_n(samples, tokenizer, n=3):\n",
    "    \"\"\"compute total-rep-n metric\"\"\"\n",
    "    n_grams = []\n",
    "    \n",
    "    for s in samples:\n",
    "        tokens = tokenizer(s, add_special_tokens=False).input_ids\n",
    "        for i in range(len(tokens)):\n",
    "            if i <= len(tokens) - n:\n",
    "                n_grams.append(tuple(tokens[i:i + n]))\n",
    "\n",
    "    total_rep = 1 - len(set(n_grams)) / len(n_grams)        \n",
    "\n",
    "    return {f\"total_rep_{n}\": total_rep}\n",
    "\n",
    "\n",
    "def compute_repetition(samples_dict, tokenizer):\n",
    "    \"\"\"Compute repetition metrics.\"\"\"\n",
    "    samples = samples_dict['truncate_prompt_output']\n",
    "    samples_dict.update(compute_seq_rep_n(samples, tokenizer, n=3))\n",
    "    samples_dict.update(compute_total_rep_n(samples, tokenizer, n=3))\n",
    "    # print(f\"Model name: {model_name}\\nMedian seq rep 3: {samples['median_seq_rep_3']}\\nTotal rep 3: {samples['total_rep_3']}\")\n",
    "    return f\"Median seq rep 3: {samples_dict['median_seq_rep_3']}\\nTotal rep 3: {samples_dict['total_rep_3']}\"\n",
    "\n",
    "def compute_ppl(samples_dict, prompts,  tokenizer, model, batch_size, fp16=True):\n",
    "    \"\"\"Compute perplexities under `ppl_model_name`.\"\"\"\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    if model.device.type != device:\n",
    "        original_device = model.device\n",
    "        model.to(device)\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "    ppls = []\n",
    "    loss_fct = CrossEntropyLoss(reduction=\"none\")\n",
    "\n",
    "    samples = samples_dict[\"full_output\"]\n",
    "\n",
    "    for i in tqdm(range(0, len(samples), batch_size)):\n",
    "        s = samples[i:i + batch_size]\n",
    "        encodings = tokenizer(\n",
    "            s,\n",
    "            add_special_tokens=True,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "            return_attention_mask=True,\n",
    "        ).to(device)\n",
    "\n",
    "        encoded_batch = encodings[\"input_ids\"]\n",
    "        attn_mask = encodings[\"attention_mask\"]\n",
    "\n",
    "        labels = encoded_batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out_logits = model(encoded_batch, attention_mask=attn_mask).logits\n",
    "\n",
    "        prompt_text = prompts[i:i + batch_size]\n",
    "        # print(prompt_text)\n",
    "        # print(type(prompt_text))\n",
    "        # print(len(prompt_text))\n",
    "        \n",
    "        prompt_encodings = tokenizer(\n",
    "            prompt_text,\n",
    "            add_special_tokens=True,\n",
    "            padding=True,\n",
    "            return_tensors=\"pt\",\n",
    "            return_attention_mask=True,\n",
    "        ).to(device)\n",
    "        prompt_attn_mask = prompt_encodings[\"attention_mask\"]\n",
    "\n",
    "        # match shape of prompt_attn_mask and attn_mask by padding with 0\n",
    "        padding = torch.zeros(\n",
    "            (attn_mask.shape[0], attn_mask.shape[1] - prompt_attn_mask.shape[1]),\n",
    "        ).to(device)\n",
    "        padded_prompt_attn_mask = torch.cat([prompt_attn_mask, padding], dim=1)\n",
    "        prompt_mask = (padded_prompt_attn_mask == 1)\n",
    "        \n",
    "        # don't score prompt tokens\n",
    "        attn_mask[prompt_mask] = 0\n",
    "\n",
    "        shift_logits = out_logits[..., :-1, :].contiguous()\n",
    "        shift_labels = labels[..., 1:].contiguous()\n",
    "        shift_attention_mask_batch = attn_mask[..., 1:].contiguous()\n",
    "\n",
    "        perplexity_batch = torch.exp(\n",
    "            (loss_fct(shift_logits.transpose(1, 2), shift_labels) * shift_attention_mask_batch).sum(1)\n",
    "            / shift_attention_mask_batch.sum(1)\n",
    "        )\n",
    "\n",
    "        ppls += perplexity_batch.tolist()\n",
    "\n",
    "    mean_perplexity = np.mean(ppls)\n",
    "    median_perplexity = np.median(ppls)\n",
    "    samples_dict[\"mean_perplexity\"] = mean_perplexity\n",
    "    samples_dict[\"median_perplexity\"] = median_perplexity\n",
    "    samples_dict[\"perplexities\"] = ppls\n",
    "    # if original_device!=device:\n",
    "    #     model.to(original_device)\n",
    "    return f\"mean perplexity: {mean_perplexity}, median perplexity: {median_perplexity}\"\n",
    "\n",
    "\n",
    "def move_to_device(batch, device):\n",
    "    \"\"\"Move batch to the specified device.\"\"\"\n",
    "    new_batch = {}\n",
    "    for key, value in batch.items():\n",
    "        if isinstance(value, torch.Tensor):\n",
    "            new_batch[key] = value.to(device)\n",
    "        elif isinstance(value, list):\n",
    "            # Assuming lists are lists of tensors, move each tensor to the device\n",
    "            new_batch[key] = [v.to(device) if isinstance(v, torch.Tensor) else v for v in value]\n",
    "        else:\n",
    "            new_batch[key] = value\n",
    "    return new_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 29.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median seq rep 3: 0.0\n",
      "Total rep 3: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:01<00:00,  1.97s/it]\n",
      "  0%|          | 1/512 [00:21<3:03:09, 21.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean perplexity: 3.885118246078491, median perplexity: 3.885118246078491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 30.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Median seq rep 3: 0.0\n",
      "Total rep 3: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:02<00:00,  1.34s/it]\n",
      "  0%|          | 2/512 [00:44<3:08:06, 22.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean perplexity: 4.476223111152649, median perplexity: 4.476223111152649\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def generate_and_evaluate(prompts, num_samples, vanilla_model, tokenizer, generation_result_dict, watermark_config):\n",
    "    output_results = []\n",
    "    full_output_results = []\n",
    "\n",
    "    for batch in tqdm(prompts):\n",
    "        if len(output_results) >= num_samples:\n",
    "            break\n",
    "        with torch.no_grad():\n",
    "            batch = move_to_device(batch, \"cpu\")\n",
    "            vanilla_output = vanilla_model.generate(\n",
    "                input_ids=batch[\"input_ids\"],\n",
    "                attention_mask=batch[\"attention_mask\"],\n",
    "                do_sample=True,\n",
    "                min_length=50,\n",
    "                max_length=100,\n",
    "                temperature=1.0,\n",
    "                top_p=0.95,\n",
    "                top_k=50,\n",
    "                pad_token_id=tokenizer.eos_token_id,\n",
    "            )\n",
    "        \n",
    "            n_input_tokens = batch[\"input_ids\"].shape[1]\n",
    "            truncated_prompt_output = vanilla_output[:, n_input_tokens:]\n",
    "\n",
    "            output_results.extend(tokenizer.batch_decode(truncated_prompt_output, skip_special_tokens=True))\n",
    "            full_output_results.extend(tokenizer.batch_decode(vanilla_output, skip_special_tokens=True))\n",
    "\n",
    "        generation_result_dict['full_output'] = full_output_results[:num_samples]\n",
    "        generation_result_dict[\"truncate_prompt_output\"] = output_results[:num_samples]\n",
    "     \n",
    "\n",
    "        output_results = output_results[:num_samples]\n",
    "        if watermark_config[\"type\"] == \"kgw\":\n",
    "            detector = WatermarkDetector(\n",
    "                device=watermark_config.get(\"kgw_device\", 'cpu'),\n",
    "                tokenizer=tokenizer,\n",
    "                vocab=tokenizer.get_vocab().values(),\n",
    "                gamma=watermark_config[\"gamma\"],\n",
    "                seeding_scheme=watermark_config[\"seeding_scheme\"],\n",
    "                normalizers=[],\n",
    "            )\n",
    "        elif watermark_config[\"type\"] == \"aar\":\n",
    "            detector = AarWatermarkDetector(tokenizer=tokenizer, k=watermark_config['k'], seed=watermark_config['seed'], eps=1e-20)\n",
    "        \n",
    "        output_scores = compute_p_value(output_results, detector, type=watermark_config[\"type\"])\n",
    "       \n",
    "        generation_result_dict['watermark_scores'] = output_scores\n",
    "        rep_output = compute_repetition(generation_result_dict, tokenizer)\n",
    "        print(f\"{rep_output}\")\n",
    "        ppl_output = compute_ppl(generation_result_dict, prompt_text, tokenizer, vanilla_model, batch_size)\n",
    "        print(f\"{ppl_output}\")\n",
    "    return generation_result_dict\n",
    "\n",
    "generation_dict = {}\n",
    "result = generate_and_evaluate(prompts, 2, vanilla_model, tokenizer, generation_dict, {'type': 'kgw', 'gamma': 0.25, 'seeding_scheme': 'simple_0'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'full_output': [\"Cluster comprises IBM's Opteron-based eServer 325 server and systems management software and storage devices that can run Linux and Windows operating systems.\\nIBM on Tuesday announced a prepackaged and pre-priced system of servers, software and storage devices that aims to meet the business computing needs of small and midsize companies, particularly those with Linux servers.\\nThe offering, called Cluster 100, includes IBM's Op\",\n",
       "  'Belying expectations, Prasar Bharti has earned only Rs 58.19 crore (Rs 581.9 million) as revenue during the Commonwealth Games last month.\\nThe gross revenue, which excludes revenue from sale of television rights and advertising slots, is far lower than its actual income during the Beijing Olympics (Rs 213.21 crore) and the previous Games in'],\n",
       " 'truncate_prompt_output': [\"-priced system of servers, software and storage devices that aims to meet the business computing needs of small and midsize companies, particularly those with Linux servers.\\nThe offering, called Cluster 100, includes IBM's Op\",\n",
       "  'revenue, which excludes revenue from sale of television rights and advertising slots, is far lower than its actual income during the Beijing Olympics (Rs 213.21 crore) and the previous Games in'],\n",
       " 'watermark_scores': [0.33187088109041213, 0.8166124177620249],\n",
       " 'median_seq_rep_3': 0.0,\n",
       " 'mean_seq_rep_3': 0.0,\n",
       " 'list_seq_rep_3': [0.0, 0.0],\n",
       " 'total_rep_3': 0.0,\n",
       " 'mean_perplexity': 4.476223111152649,\n",
       " 'median_perplexity': 4.476223111152649,\n",
       " 'perplexities': [3.885118246078491, 5.067327976226807]}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generation_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mech",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
